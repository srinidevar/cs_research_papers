# Computer Science arXiv Papers

Collection of top 10 Computer Science research papers pulled daily from arXiv.

---

Pulled on 2025-10-21 17:00:24.952403 PST.

### Artificial Intelligence

### 1. [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](http://arxiv.org/pdf/2510.17064v1)

Authors: Rongbin Li, Wenbo Chen, Zhao Li, Rodrigo Munoz-Castaneda, Jinbo Li, Neha S. Maurya, Arnav Solanki, Huan He, Hanwen Xing, Meaghan Ramlakhan, Zachary Wise, Zhuhao Wu, Hua Xu, Michael Hawrylycz, W. Jim Zheng

Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

### 2. [Structured Debate Improves Corporate Credit Reasoning in Financial AI](http://arxiv.org/pdf/2510.17108v1)

Authors: Yoonjin Lee, Munhee Kim, Hanbi Choi, Juhyeon Park, Seungho Lyoo, Woojin Park

Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

### 3. [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](http://arxiv.org/pdf/2510.17145v1)

Authors: Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan

Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

### 4. [Which LLM Multi-Agent Protocol to Choose?](http://arxiv.org/pdf/2510.17149v1)

Authors: Hongyi Du, Jiaqi Su, Jisen Li, Lijie Ding, Yingxuan Yang, Peixuan Han, Xiangru Tang, Kunlun Zhu, Jiaxuan You

As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

### 5. [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](http://arxiv.org/pdf/2510.17172v1)

Authors: Shun Huang, Wenlu Xing, Shijia Geng, Hailong Wang, Guangkun Nie, Gongzheng Tang, Chenyang He, Shenda Hong

Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

### 6. [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](http://arxiv.org/pdf/2510.17235v1)

Authors: Chong Chen, Ze Liu, Lingfeng Bao, Yanlin Wang, Ting Chen, Daoyuan Wu, Jiachi Chen

The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

### 7. [RubiSCoT: A Framework for AI-Supported Academic Assessment](http://arxiv.org/pdf/2510.17309v1)

Authors: Thorsten Fröhlich, Tim Schlippe

The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

### 8. [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](http://arxiv.org/pdf/2510.17450v1)

Authors: Johan Schubert, Farzad Kamrani, Tove Gustavi

We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

### 9. [Label Indeterminacy in AI & Law](http://arxiv.org/pdf/2510.17463v1)

Authors: Cor Steging, Tadeusz Zbiegień

Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

### 10. [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](http://arxiv.org/pdf/2510.17057v1)

Authors: Nikolaus Howe, Micah Carroll

The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

### Hardware Architecture

### 1. [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](http://arxiv.org/pdf/2510.17251v1)

Authors: Chengxi Li, Yang Sun, Lei Chen, Yiwen Wang, Mingxuan Yuan, Evangeline F. Y. Young

This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

### 2. [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](http://arxiv.org/pdf/2510.17189v1)

Authors: Wenxun Wang, Shuchang Zhou, Wenyu Sun, Peiqin Sun, Yongpan Liu

Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

### 3. [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](http://arxiv.org/pdf/2510.17392v1)

Authors: Sonu Kumar, Arjun S. Nair, Bhawna Chaudhary, Mukul Lokhande, Santosh Kumar Vishvakarma

We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.

### Computational Complexity

### 1. [Counterfactual Explanations for Integer Optimization Problems](http://arxiv.org/pdf/2510.17624v1)

Authors: Felix Engelhardt, Jannis Kurtz, Ş. İlker Birbil, Ted Ralphs

Counterfactual explanations (CEs) offer a human-understandable way to explain
decisions by identifying specific changes to the input parameters of a base or
present model that would lead to a desired change in the outcome. For
optimization models, CEs have primarily been studied in limited contexts and
little research has been done on CEs for general integer optimization problems.
In this work, we address this gap. We first show that the general problem of
constructing a CE is $\Sigma_2^p$-complete even for binary integer programs
with just a single mutable constraint. Second, we propose solution algorithms
for several of the most tractable special cases: (i) mutable objective
parameters, (ii) a single mutable constraint, (iii) mutable right-hand-side,
and (iv) all input parameters can be modified. We evaluate our approach using
classical knapsack problem instances, focusing on cases with mutable constraint
parameters. Our results show that our methods are capable of finding optimal
CEs for small instances involving up to 40 items within a few hours.

### 2. [Unifying the Landscape of Super-Logarithmic Dynamic Cell-Probe Lower Bounds](http://arxiv.org/pdf/2510.17717v1)

Authors: Young Kun Ko

We prove a general translation theorem for converting one-way communication
lower bounds over a product distribution to dynamic cell-probe lower bounds.
  Specifically, we consider a class of problems considered in [Pat10] where:
  1. $S_1, \ldots, S_m \in \{0, 1\}^n$ are given and publicly known.
  2. $T \in \{0, 1\}^n$ is a sequence of updates, each taking $t_u$ time.
  3. For a given $Q \in [m]$, we must output $f(S_Q, T)$ in $t_q$ time. Our
main result shows that for a "hard" function $f$, for which it is difficult to
obtain a non-trivial advantage over random guessing with one-way communication
under some product distribution over $S_Q$ and $T$ (for example, a uniform
distribution), then the above explicit dynamic cell-probe problem must have
$\max \{ t_u, t_q \} \geq \tilde{\Omega}(\log^{3/2}(n))$ if $m =
\Omega(n^{0.99})$. This result extends and unifies the super-logarithmic
dynamic data structure lower bounds from [LWY20] and [LY25] into a more general
framework.
  From a technical perspective, our approach merges the cell-sampling and
chronogram techniques developed in [LWY20] and [LY25] with the new static data
structure lower bound methods from [KW20] and [Ko25], thereby merging all known
state-of-the-art cell-probe lower-bound techniques into one.
  As a direct consequence of our method, we establish a super-logarithmic lower
bound against the Multiphase Problem [Pat10] for the case where the data
structure outputs the Inner Product (mod 2) of $S_Q$ and $T$. We suspect
further applications of this general method towards showing super-logarithmic
dynamic cell-probe lower bounds. We list some example applications of our
general method, including a novel technique for a one-way communication lower
bound against small-advantage protocols for a product distribution using
average min-entropy, which could be of independent interest.

### 3. [The Parameterized Complexity of Computing the VC-Dimension](http://arxiv.org/pdf/2510.17451v1)

Authors: Florent Foucaud, Harmender Gahlawat, Fionn Mc Inerney, Prafullkumar Tale

The VC-dimension is a fundamental and well-studied measure of the complexity
of a set system (or hypergraph) that is central to many areas of machine
learning. We establish several new results on the complexity of computing the
VC-dimension. In particular, given a hypergraph
$\mathcal{H}=(\mathcal{V},\mathcal{E})$, we prove that the naive
$2^{\mathcal{O}(|\mathcal{V}|)}$-time algorithm is asymptotically tight under
the Exponential Time Hypothesis (ETH). We then prove that the problem admits a
1-additive fixed-parameter approximation algorithm when parameterized by the
maximum degree of $\mathcal{H}$ and a fixed-parameter algorithm when
parameterized by its dimension, and that these are essentially the only such
exploitable structural parameters. Lastly, we consider a generalization of the
problem, formulated using graphs, which captures the VC-dimension of both set
systems and graphs. We show that it is fixed-parameter tractable parameterized
by the treewidth of the graph (which, in the case of set systems, applies to
the treewidth of its incidence graph). In contrast with closely related
problems whose dependency on the treewidth is necessarily double-exponential
(assuming the ETH), our algorithm has a relatively low dependency on the
treewidth.

### Computational Engineering

### 1. [StrengthLawExtractor: A Fiji plugin for 3D morphological feature extraction from X-ray micro-CT data](http://arxiv.org/pdf/2510.17279v1)

Authors: Qinyi Tian, Laura E. Dalton

Non-destructive methods are essential for linking the microstructural
geometry of porous materials to their mechanical behavior, as destructive
testing is often infeasible due to limited material availability or
irreproducible conditions. Micro-computed tomography (micro-CT) provides high
resolution three dimensional reconstructions of porous microstructures,
enabling direct quantification of geometric descriptors. Recent advances in
morphometric theory have demonstrated that four independent morphometric
measures (porosity, surface area, mean curvature, and Euler characteristic) are
required to capture the relationship between microstructure and strength,
thereby forming the basis of generalized strength laws. To facilitate practical
application of this framework, a Fiji plugin was developed to extract the four
morphometric measures (porosity, surface area, mean curvature, Euler
characteristic) from micro-CT datasets automatically. The plugin integrates
within the Fiji platform to provide reproducible, accessible, and user friendly
analysis. The application of the tool demonstrates that the extracted
descriptors can be readily incorporated into constitutive models and machine
learning workflows, enabling the forward prediction of stress-strain behavior
as well as the inverse design of microstructures. This approach supports
non-destructive evaluation, accelerates materials selection, and advances the
integration of imaging with predictive modeling in porous media research.

### 2. [Modelling complexity in system safety: generalizing the D2T2 methodology](http://arxiv.org/pdf/2510.17351v1)

Authors: Silvia Tolo, John Andrews

Although Fault Tree and Event Tree analysis are still today the standard
approach to system safety analysis for many engineering sectors, these
techniques lack the capabilities of fully capturing the realistic, dynamic
behaviour of complex systems, which results in a dense network of dependencies
at any level, i.e. between components, trains of components or subsystems.
While these limitations are well recognised across both industry and academia,
the shortage of alternative tools able to tackle such challenges while
retaining the computational feasibility of the analysis keeps fuelling the
long-lived success of Fault Tree and Event Tree modelling. Analysts and
regulators often rely on the use of conservative assumptions to mitigate the
effect of oversimplifications associated with the use of such techniques.
However, this results in the analysis output to be characterised by an unknown
level of conservatism, with potential consequences on market competitiveness
(i.e., over-conservatism) or safety (i.e., under-conservatism). This study
proposes a generalization of the Dynamic and Dependent Tree Theory, which
offers theoretical tools for the systematic integration of dependency modelling
within the traditional Fault and Event Tree analysis framework. This is
achieved by marrying the traditional combinatorial nature of failure analysis,
formalised by the Fault and Event Tree language, with more flexible modelling
solutions, which provide the flexibility required to capture complex system
features. The main advantage of the proposed approach in comparison to existent
solutions is the ability to take into account, under the same modelling
framework, any type of dependency regardless of its nature and location, while
retaining the familiarity and effectiveness of traditional safety modelling.

### 3. [Volumetric Non-Invasive Cardiac Mapping for Accessible Global Arrhythmia Characterization](http://arxiv.org/pdf/2510.17539v1)

Authors: Jorge Vicente-Puig, Judit Chamorro-Servent, Ernesto Zacur, Inés Llorente-Lipe, Marta Martínez, Jorge Sanchez, Jana Reventós, Ivo Roca-Luque, Lluis Mont, Felipe Atienza, Andreu M. Climent, Maria S. Guillem, Ismael Hernández-Romero

Cardiac arrhythmias are a major cause of morbidity and mortality increasing
the risk of stroke, heart failure, and sudden cardiac death. Imageless
electrocardiographic imaging (ECGI) provides a non invasive alternative to
electrical mapping from body surface potentials, but conventional ECGI is
confined to epicardial reconstructions and can miss arrhythmias originating in
deeper myocardium. We address this by reconstructing three dimensional cardiac
activity with a volumetric formulation that solves an inverse source problem
via Green's functions, enabling full volume activation mapping and improved
localization in anatomically complex regions. We evaluate the approach on
simulated premature ventricular beats and on four challenging patient cases, a
right ventricular outflow tract premature ventricular contraction, a left
bundle branch block, a ventricular tachycardia, and Wolff Parkinson White, and
additionally assess performance on an open source myocardial infarction
dataset. Results show that volumetric ECGI recovers 3D activation and sharpens
arrhythmia origin localization, achieving a 59.3% reduction in geodesic error
between estimated and simulated origins relative to surface only methods; in
patient cases, activation patterns align with clinical diagnoses. Overall,
imageless volumetric ECGI offers accessible, non invasive 3D activation mapping
that overcomes a core limitation of surface restricted techniques and may
improve preprocedural planning, ablation target guidance, and selection or
optimization of cardiac resynchronization therapy.

### 4. [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](http://arxiv.org/pdf/2510.17146v1)

Authors: Subin Lin, Chuanbo Hua

Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

### 5. [Trading with the Devil: Risk and Return in Foundation Model Strategies](http://arxiv.org/pdf/2510.17165v1)

Authors: Jinrui Zhang

Foundation models - already transformative in domains such as natural
language processing - are now starting to emerge for time-series tasks in
finance. While these pretrained architectures promise versatile predictive
signals, little is known about how they shape the risk profiles of the trading
strategies built atop them, leaving practitioners reluctant to commit serious
capital. In this paper, we propose an extension to the Capital Asset Pricing
Model (CAPM) that disentangles the systematic risk introduced by a shared
foundation model - potentially capable of generating alpha if the underlying
model is genuinely predictive - from the idiosyncratic risk attributable to
custom fine-tuning, which typically accrues no systematic premium. To enable a
practical estimation of these separate risks, we align this decomposition with
the concepts of uncertainty disentanglement, casting systematic risk as
epistemic uncertainty (rooted in the pretrained model) and idiosyncratic risk
as aleatory uncertainty (introduced during custom adaptations). Under the
Aleatory Collapse Assumption, we illustrate how Monte Carlo dropout - among
other methods in the uncertainty-quantization toolkit - can directly measure
the epistemic risk, thereby mapping trading strategies to a more transparent
risk-return plane. Our experiments show that isolating these distinct risk
factors yields deeper insights into the performance limits of
foundation-model-based strategies, their model degradation over time, and
potential avenues for targeted refinements. Taken together, our results
highlight both the promise and the pitfalls of deploying large pretrained
models in competitive financial markets.

### 6. [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](http://arxiv.org/pdf/2510.17088v1)

Authors: Zan Li, Rui Fan

Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

### Computational Geometry

### 1. [Flow-Aware Ellipsoidal Filtration for Persistent Homology of Recurrent Signals](http://arxiv.org/pdf/2510.17735v1)

Authors: Omer Bahadir Eryilmaz, Cihan Katar, Max A. Little

One common use of persistent homology is to explore the shape of point
clouds, where points are assumed to be sampled from a geometric object. We
propose a novel filtration, called ellipsoidal filtration, which assumes that
point clouds are sampled from a dynamic smooth flow. Instead of creating
topologies from point clouds at increasing scales using isotropic balls (for
example, Vietoris-Rips filtration), ellipsoidal filtration creates ellipsoids
around points based on local flow variances, approximating the flow's manifold
as the scale increases. We show that constructing ellipsoidal neighbourhoods
improves the denoising of recurrent signals and the estimation of recurrence
times, especially when the data contain bottlenecks. Choosing ellipsoids
according to the maximum persistence of the H1 class provides a data-driven
threshold for both denoising and recurrence-time estimation.

### 2. [Who Needs Crossings?: Noncrossing Linkages are Universal, and Deciding (Global) Rigidity is Hard](http://arxiv.org/pdf/2510.17737v1)

Authors: Zachary Abel, Erik D. Demaine, Martin L. Demaine, Sarah Eisenstat, Jayson Lynch, Tao B. Schardl

We exactly settle the complexity of graph realization, graph rigidity, and
graph global rigidity as applied to three types of graphs: "globally
noncrossing" graphs, which avoid crossings in all of their configurations;
matchstick graphs, with unit-length edges and where only noncrossing
configurations are considered; and unrestricted graphs (crossings allowed) with
unit edge lengths (or in the global rigidity case, edge lengths in $\{1,2\}$).
We show that all nine of these questions are complete for the class
$\exists\mathbb{R}$, defined by the Existential Theory of the Reals, or its
complement $\forall\mathbb{R}$; in particular, each problem is (co)NP-hard.
  One of these nine results--that realization of unit-distance graphs is
$\exists\mathbb{R}$-complete--was shown previously by Schaefer (2013), but the
other eight are new. We strengthen several prior results. Matchstick graph
realization was known to be NP-hard (Eades \& Wormald 1990, or Cabello et al.\
2007), but its membership in NP remained open; we show it is complete for the
(possibly) larger class $\exists\mathbb{R}$. Global rigidity of graphs with
edge lengths in $\{1,2\}$ was known to be coNP-hard (Saxe 1979); we show it is
$\forall\mathbb{R}$-complete.
  The majority of the paper is devoted to proving an analog of Kempe's
Universality Theorem--informally, "there is a linkage to sign your name"--for
globally noncrossing linkages. In particular, we show that any polynomial curve
$\phi(x,y)=0$ can be traced by a noncrossing linkage, settling an open problem
from 2004. More generally, we show that the regions in the plane that may be
traced by a noncrossing linkage are precisely the compact semialgebraic regions
(plus the trivial case of the entire plane). Thus, no drawing power is lost by
restricting to noncrossing linkages. We prove analogous results for matchstick
linkages and unit-distance linkages as well.

### Computation and Language

### 1. [When AI companions become witty: Can human brain recognize AI-generated irony?](http://arxiv.org/pdf/2510.17168v1)

Authors: Xiaohui Rao, Hanlin Wu, Zhenguang G. Cai

As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

### 2. [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](http://arxiv.org/pdf/2510.17210v1)

Authors: Chenchen Tan, Youyang Qu, Xinghao Li, Hui Zhang, Shujie Cui, Cunjian Chen, Longxiang Gao

The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

### 3. [StreamingThinker: Large Language Models Can Think While Reading](http://arxiv.org/pdf/2510.17238v1)

Authors: Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen

Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

### 4. [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](http://arxiv.org/pdf/2510.17256v1)

Authors: Shahin Atakishiyev, Housam K. B. Babiker, Jiayi Dai, Nawshad Farruque, Teruaki Hayashi, Nafisa Sadaf Hriti, Md Abed Rahman, Iain Smith, Mi-Young Kim, Osmar R. Zaïane, Randy Goebel

Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

### 5. [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](http://arxiv.org/pdf/2510.17263v1)

Authors: Avishek Lahiri, Yufang Hou, Debarshi Kumar Sanyal

Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

### 6. [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](http://arxiv.org/pdf/2510.17289v1)

Authors: Hajar Bakarou, Mohamed Sinane El Messoussi, Anaïs Ollagnier

Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

### 7. [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](http://arxiv.org/pdf/2510.17388v1)

Authors: Henry Lim, Kwan Hui Lim

Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

### 8. [Agentic Reinforcement Learning for Search is Unsafe](http://arxiv.org/pdf/2510.17431v1)

Authors: Yushi Yang, Shreyansh Padarha, Andrew Lee, Adam Mahdi

Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

### 9. [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](http://arxiv.org/pdf/2510.17437v1)

Authors: Manuela Daniela Danu, George Marica, Constantin Suciu, Lucian Mihai Itu, Oladimeji Farri

The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

### 10. [Evaluating Large Language Models on Urdu Idiom Translation](http://arxiv.org/pdf/2510.17460v1)

Authors: Muhammad Farmal Khan, Mousumi Akter

Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

### Cryptography and Security

### 1. [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](http://arxiv.org/pdf/2510.17277v1)

Authors: Xinkai Wang, Beibei Li, Zerui Shao, Ao Liu, Shouling Ji

Multimodal large language models (MLLMs) have demonstrated significant
utility across diverse real-world applications. But MLLMs remain vulnerable to
jailbreaks, where adversarial inputs can collapse their safety constraints and
trigger unethical responses. In this work, we investigate jailbreaks in the
text-vision multimodal setting and pioneer the observation that visual
alignment imposes uneven safety constraints across modalities in MLLMs, thereby
giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a
black-box jailbreak method grounded in reinforcement learning. Initially, we
probe the model's attention dynamics and latent representation space, assessing
how visual inputs reshape cross-modal information flow and diminish the model's
ability to separate harmful from benign inputs, thereby exposing exploitable
vulnerabilities. On this basis, we systematize them into generalizable and
reusable operational rules that constitute a structured library of Atomic
Strategy Primitives, which translate harmful intents into jailbreak inputs
through step-wise transformations. Guided by the primitives, PolyJailbreak
employs a multi-agent optimization process that automatically adapts inputs
against the target models. We conduct comprehensive evaluations on a variety of
open-source and closed-source MLLMs, demonstrating that PolyJailbreak
outperforms state-of-the-art baselines.

### 2. [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](http://arxiv.org/pdf/2510.17284v1)

Authors: Jiri Gavenda, Petr Svenda, Stanislav Bobon, Vladimir Sedlacek

A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

### 3. [Single-Shuffle Full-Open Card-Based Protocols for Any Function](http://arxiv.org/pdf/2510.17308v1)

Authors: Reo Eriguchi, Kazumasa Shinagawa

A card-based secure computation protocol is a method for $n$ parties to
compute a function $f$ on their private inputs $(x_1,\ldots,x_n)$ using
physical playing cards, in such a way that the suits of revealed cards leak no
information beyond the value of $f(x_1,\ldots,x_n)$. A \textit{single-shuffle
full-open} protocol is a minimal model of card-based secure computation in
which, after the parties place face-down cards representing their inputs, a
single shuffle operation is performed and then all cards are opened to derive
the output. Despite the simplicity of this model, the class of functions known
to admit single-shuffle full-open protocols has been limited to a few small
examples. In this work, we prove for the first time that every function admits
a single-shuffle full-open protocol. We present two constructions that offer a
trade-off between the number of cards and the complexity of the shuffle
operation. These feasibility results are derived from a novel connection
between single-shuffle full-open protocols and a cryptographic primitive known
as \textit{Private Simultaneous Messages} protocols, which has rarely been
studied in the context of card-based cryptography. We also present variants of
single-shuffle protocols in which only a subset of cards are revealed. These
protocols reduce the complexity of the shuffle operation compared to existing
protocols in the same setting.

### 4. [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](http://arxiv.org/pdf/2510.17311v1)

Authors: Eduard Marin, Jinwoo Kim, Alessio Pavoni, Mauro Conti, Roberto Di Pietro

Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

### 5. [Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs](http://arxiv.org/pdf/2510.17521v1)

Authors: Francesco Balassone, Víctor Mayoral-Vilches, Stefan Rass, Martin Pinzger, Gaetano Perrone, Simon Pietro Romano, Peter Schartner

We empirically evaluate whether AI systems are more effective at attacking or
defending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution
framework, we deployed autonomous agents in 23 Attack/Defense CTF
battlegrounds. Statistical analysis reveals defensive agents achieve 54.3%
unconstrained patching success versus 28.3% offensive initial access
(p=0.0193), but this advantage disappears under operational constraints: when
defense requires maintaining availability (23.9%) and preventing all intrusions
(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy
analysis suggests potential patterns in vulnerability exploitation, though
limited sample sizes preclude definitive conclusions. This study provides the
first controlled empirical evidence challenging claims of AI attacker
advantage, demonstrating that defensive effectiveness critically depends on
success criteria, a nuance absent from conceptual analyses but essential for
deployment. These findings underscore the urgency for defenders to adopt
open-source Cybersecurity AI frameworks to maintain security equilibrium
against accelerating offensive automation.

### 6. [Dynamic Switched Quantum Key Distribution Networkwith PUF-based authentication](http://arxiv.org/pdf/2510.17552v1)

Authors: Persefoni Konteli, Nikolaos Makris, Evgenia Niovi Sassalou, Stylianos A. Kazazis, Alkinoos Papageorgopoulos, Stefanos Vasileiadis, Konstantinos Tsimvrakidis, Symeon Tsintzos, Georgios M. Nikolopoulos, George T. Kanellos

We demonstrate a centrally controlled dynamic switched-QKD network,
withintegrated PUF-based dynamic authentication for each QKD link. The
performance of the dynamicswitched-QKD network with real-time PUF-based
authentication is analyzed.

### 7. [Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](http://arxiv.org/pdf/2510.17098v1)

Authors: Elias Hossain, Swayamjit Saha, Somshubhra Roy, Ravi Prasad

Even when prompts and parameters are secured, transformer language models
remain vulnerable because their key-value (KV) cache during inference
constitutes an overlooked attack surface. This paper introduces Malicious Token
Injection (MTI), a modular framework that systematically perturbs cached key
vectors at selected layers and timesteps through controlled magnitude and
frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A
theoretical analysis quantifies how these perturbations propagate through
attention, linking logit deviations to the Frobenius norm of corruption and
softmax Lipschitz dynamics. Empirical results show that MTI significantly
alters next-token distributions and downstream task performance across GPT-2
and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic
reasoning pipelines. These findings identify cache integrity as a critical yet
underexplored vulnerability in current LLM deployments, positioning cache
corruption as a reproducible and theoretically grounded threat model for future
robustness and security research.

### 8. [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](http://arxiv.org/pdf/2510.17175v1)

Authors: Muhammad Wahid Akram, Keshav Sood, Muneeb Ul Hassan

Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

### 9. [GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](http://arxiv.org/pdf/2510.17621v1)

Authors: Vincenzo Carletti, Pasquale Foggia, Carlo Mazzocca, Giuseppe Parrella, Mario Vento

Federated Learning (FL) enables collaborative training of Machine Learning
(ML) models across multiple clients while preserving their privacy. Rather than
sharing raw data, federated clients transmit locally computed updates to train
the global model. Although this paradigm should provide stronger privacy
guarantees than centralized ML, client updates remain vulnerable to privacy
leakage. Adversaries can exploit them to infer sensitive properties about the
training data or even to reconstruct the original inputs via Gradient Inversion
Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to
reconstruct training data by reversing intermediate updates using
optimizationbased techniques. We observe that these approaches usually
reconstruct noisy approximations of the original inputs, whose quality can be
enhanced with specialized denoising models. This paper presents Gradient Update
Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion
models as denoising tools to improve image reconstruction attacks in FL. GUIDE
can be integrated into any GIAs that exploits surrogate datasets, a widely
adopted assumption in GIAs literature. We comprehensively evaluate our approach
in two attack scenarios that use different FL algorithms, models, and datasets.
Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe-
art GIAs, substantially improving reconstruction quality across multiple
metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,
as measured by the DreamSim metric.

### 10. [SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering](http://arxiv.org/pdf/2510.17633v1)

Authors: Weilin Lin, Jianze Li, Hui Xiong, Li Liu

Large Audio-Language Models (LALMs) are becoming essential as a powerful
multimodal backbone for real-world applications. However, recent studies show
that audio inputs can more easily elicit harmful responses than text, exposing
new risks toward deployment. While safety alignment has made initial advances
in LLMs and Large Vision-Language Models (LVLMs), we find that vanilla
adaptation of these approaches to LALMs faces two key limitations: 1) LLM-based
steering fails under audio input due to the large distributional gap between
activations, and 2) prompt-based defenses induce over-refusals on benign-speech
queries. To address these challenges, we propose Safe-Ablated Refusal Steering
(SARSteer), the first inference-time defense framework for LALMs. Specifically,
SARSteer leverages text-derived refusal steering to enforce rejection without
manipulating audio inputs and introduces decomposed safe-space ablation to
mitigate over-refusal. Extensive experiments demonstrate that SARSteer
significantly improves harmful-query refusal while preserving benign responses,
establishing a principled step toward safety alignment in LALMs.

### Computer Vision and Pattern Recognition

### 1. [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](http://arxiv.org/pdf/2510.17068v1)

Authors: Zhe Luo, Wenjing Jia, Stuart Perry

Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

### 2. [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](http://arxiv.org/pdf/2510.17078v1)

Authors: Jad Berjawi, Yoann Dupas, Christophe C'erin

Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

### 3. [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](http://arxiv.org/pdf/2510.17095v1)

Authors: Ruitong Gan, Junran Peng, Yang Liu, Chuanchen Luo, Qing Li, Zhaoxiang Zhang

Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

### 4. [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](http://arxiv.org/pdf/2510.17105v1)

Authors: Xiaogang Xu, Jian Wang, Yunfan Lu, Ruihang Chu, Ruixing Wang, Jiafei Wu, Bei Yu, Liang Lin

Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

### 5. [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](http://arxiv.org/pdf/2510.17114v1)

Authors: Hodaka Kawachi, Tomoya Nakamura, Hiroaki Santo, SaiKiran Kumar Tedla, Trevor Dalton Canham, Yasushi Yagi, Michael S. Brown

This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

### 6. [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](http://arxiv.org/pdf/2510.17137v1)

Authors: WenBo Xu, Liu Liu, Li Zhang, Ran Zhang, Hao Wu, Dan Guo, Meng Wang

Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

### 7. [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](http://arxiv.org/pdf/2510.17169v1)

Authors: Roland Croft, Brian Du, Darcy Joseph, Sharath Kumar

Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

### 8. [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](http://arxiv.org/pdf/2510.17171v1)

Authors: Feihong Yan, Peiru Wang, Yao Zhu, Kaiyu Pang, Qingyan Wei, Huiqi Li, Linfeng Zhang

Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

### 9. [Capturing Head Avatar with Hand Contacts from a Monocular Video](http://arxiv.org/pdf/2510.17181v1)

Authors: Haonan He, Yufeng Zheng, Jie Song

Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

### 10. [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](http://arxiv.org/pdf/2510.17188v1)

Authors: Vaibhav Rathore, Divyam Gupta, Biplab Banerjee

Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

### Computers and Society

### 1. [Mensen aanwijzen maar niet bij naam noemen: behavioural targeting, persoonsgegevens, en de nieuwe Privacyverordening](http://arxiv.org/pdf/2510.17710v1)

Authors: Frederik Zuiderveen Borgesius

Information about millions of people is collected for behavioural targeting,
a type of marketing that involves tracking people's online behaviour for
targeted advertising. It is hotly debated whether data protection law applies
to behavioural targeting. Many behavioural targeting companies say that, as
long as they do not tie names to data they hold about individuals, they do not
process any personal data, and that, therefore, data protection law does not
apply to them. European Data Protection Authorities, however, take the view
that a company processes personal data if it uses data to single out a person,
even if it cannot tie a name to these data. This paper argues that data
protection law should indeed apply to behavioural targeting. Companies can
often tie a name to nameless data about individuals. Furthermore, behavioural
targeting relies on collecting information about individuals, singling out
individuals, and targeting ads to individuals. Many privacy risks remain,
regardless of whether companies tie a name to the information they hold about a
person. A name is merely one of the identifiers that can be tied to data about
a person, and it is not even the most practical identifier for behavioural
targeting. Seeing data used to single out a person as personal data fits the
rationale for data protection law: protecting fairness and privacy.

### 2. [Discrimination, intelligence artificielle et decisions algorithmiques](http://arxiv.org/pdf/2510.17711v1)

Authors: Frederik Zuiderveen Borgesius

Artificial intelligence (AI) has a huge impact on our personal lives and also
on our democratic society as a whole. While AI offers vast opportunities for
the benefit of people, its potential to embed and perpetuate bias and
discrimination remains one of the most pressing challenges deriving from its
increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen
Borgesius for the Anti-discrimination Department of the Council of Europe,
elaborates on the risks of discrimination caused by algorithmic decision-making
and other types of artificial intelligence (AI).

### 3. [Online Political Microtargeting: Promises and Threats for Democracy](http://arxiv.org/pdf/2510.17712v1)

Authors: Frederik J. Zuiderveen Borgesius, Judith Möller, Sanne Kruikemeier, Ronan Ó Fathaigh, Kristina Irion, Tom Dobber, Balazs Bodo, Claes de Vreese

Online political microtargeting involves monitoring people's online
behaviour, and using the collected data, sometimes enriched with other data, to
show people-targeted political advertisements. Online political microtargeting
is widely used in the US; Europe may not be far behind. This paper maps
microtargeting's promises and threats to democracy. For example, microtargeting
promises to optimise the match between the electorate's concerns and political
campaigns, and to boost campaign engagement and political participation. But
online microtargeting could also threaten democracy. For instance, a political
party could, misleadingly, present itself as a different one-issue party to
different individuals. And data collection for microtargeting raises privacy
concerns. We sketch possibilities for policymakers if they seek to regulate
online political microtargeting. We discuss which measures would be possible,
while complying with the right to freedom of expression under the European
Convention on Human Rights.

### 4. [Defining the urban "local" with low dimensional manifolds of human mobility networks](http://arxiv.org/pdf/2510.17174v1)

Authors: Hezhishi Jiang, Liyan Xu, Tianshu Li, Jintong Tang, Zekun Chen, Yuxuan Wang, Haoran Liu, Hongmou Zhang, Huanfa Chen, Yu Liu

Urban science has largely relied on universal models, rendering the
heterogeneous and locally specific nature of cities effectively invisible. Here
we introduce a topological framework that defines and detects localities in
human mobility networks. We empirically demonstrate that these human mobility
network localities are rigorous geometric entities that map directly to
geographic localities, revealing that human mobility networks lie on manifolds
of dimension <=5. This representation provides a compact theoretical foundation
for spatial embedding and enables efficient applications to facility location
and propagation modeling. Our approach reconciles local heterogeneity with
universal representation, offering a new pathway toward a more comprehensive
urban science.

### 5. [Visibility Allocation Systems: How Algorithmic Design Shapes Online Visibility and Societal Outcomes](http://arxiv.org/pdf/2510.17241v1)

Authors: Stefania Ionescu, Robin Forsberg, Elsa Lichtenegger, Salima Jaoua, Kshitijaa Jaglan, Florian Dorfler, Aniko Hannak

Throughout application domains, we now rely extensively on algorithmic
systems to engage with ever-expanding datasets of information. Despite their
benefits, these systems are often complex (comprising of many intricate tools,
e.g., moderation, recommender systems, prediction models), of unknown structure
(due to the lack of accompanying documentation), and having hard-to-predict yet
potentially severe downstream consequences (due to the extensive use,
systematic enactment of existing errors, and many comprising feedback loops).
As such, understanding and evaluating these systems as a whole remains a
challenge for both researchers and legislators. To aid ongoing efforts, we
introduce a formal framework for such visibility allocation systems (VASs)
which we define as (semi-)automated systems deciding which (processed) data to
present a human user with. We review typical tools comprising VASs and define
the associated computational problems they solve. By doing so, VASs can be
decomposed into sub-processes and illustrated via data flow diagrams. Moreover,
we survey metrics for evaluating VASs throughout the pipeline, thus aiding
system diagnostics. Using forecasting-based recommendations in school choice as
a case study, we demonstrate how our framework can support VAS evaluation. We
also discuss how our framework can support ongoing AI-legislative efforts to
locate obligations, quantify systemic risks, and enable adaptive compliance.

### 6. [Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis](http://arxiv.org/pdf/2510.17425v1)

Authors: Aditi Dutta

Addressing climate change effectively requires more than cataloguing the
number of policies in place; it calls for tools that can reveal their thematic
priorities and their tangible impacts on development outcomes. Existing
assessments often rely on qualitative descriptions or composite indices, which
can mask crucial differences between key domains such as mitigation,
adaptation, disaster risk management, and loss and damage. To bridge this gap,
we develop a quantitative indicator of climate policy orientation by applying a
multilingual transformer-based language model to official national policy
documents, achieving a classification accuracy of 0.90 (F1-score). Linking
these indicators with World Bank development data in panel regressions reveals
that mitigation policies are associated with higher GDP and GNI; disaster risk
management correlates with greater GNI and debt but reduced foreign direct
investment; adaptation and loss and damage show limited measurable effects.
This integrated NLP-econometric framework enables comparable, theme-specific
analysis of climate governance, offering a scalable method to monitor progress,
evaluate trade-offs, and align policy emphasis with development goals.

### 7. [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](http://arxiv.org/pdf/2510.17383v1)

Authors: Ludovica Schaerf

This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

### 8. [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](http://arxiv.org/pdf/2510.17516v1)

Authors: Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Dirk Hovy, Nigel Collier, Paul Röttger

Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

### 9. [Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts](http://arxiv.org/pdf/2510.17753v1)

Authors: Celeste Riley, Omar Al-Refai, Yadira Colunga Reyes, Eman Hammad

As stories of human-AI interactions continue to be highlighted in the news
and research platforms, the challenges are becoming more pronounced, including
potential risks of overreliance, cognitive offloading, social and emotional
manipulation, and the nuanced degradation of human agency and judgment. This
paper surveys recent research on these issues through the lens of the
psychological triad: cognition, behavior, and emotion. Observations seem to
suggest that while AI can substantially enhance memory, creativity, and
engagement, it also introduces risks such as diminished critical thinking,
skill erosion, and increased anxiety. Emotional outcomes are similarly mixed,
with AI systems showing promise for support and stress reduction, but raising
concerns about dependency, inappropriate attachments, and ethical oversight.
This paper aims to underscore the need for responsible and context-aware AI
design, highlighting gaps for longitudinal research and grounded evaluation
frameworks to balance benefits with emerging human-centric risks.

### 10. [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](http://arxiv.org/pdf/2510.17590v1)

Authors: Mir Nafis Sharear Shopnil, Sharad Duwal, Abhishek Tyagi, Adiba Mahbub Proma

Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

### Databases

### 1. [AVOCADO: The Streaming Process Mining Challenge](http://arxiv.org/pdf/2510.17089v1)

Authors: Christian Imenkamp, Andrea Maldonado, Hendrik Reiter, Martin Werner, Wilhelm Hasselbring, Agnes Koschmider, Andrea Burattin

Streaming process mining deals with the real-time analysis of streaming data.
Event streams require algorithms capable of processing data incrementally. To
systematically address the complexities of this domain, we propose AVOCADO, a
standardized challenge framework that provides clear structural divisions:
separating the concept and instantiation layers of challenges in streaming
process mining for algorithm evaluation. The AVOCADO evaluates algorithms on
streaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean
Square Error (RMSE), Processing Latency, and robustness. This initiative seeks
to foster innovation and community-driven discussions to advance the field of
streaming process mining. We present this framework as a foundation and invite
the community to contribute to its evolution by suggesting new challenges, such
as integrating metrics for system throughput and memory consumption, and
expanding the scope to address real-world stream complexities like out-of-order
event arrival.

### 2. [Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage](http://arxiv.org/pdf/2510.17326v1)

Authors: Kun Yu, Jiabao Jin, Xiaoyao Zhong, Peng Cheng, Lei Chen, Zhitao Shen, Jingkuan Song, Hengtao Shen, Xuemin Lin

Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an
essential operator in many online services, such as information retrieval and
recommendation. Indices constructed by the state-of-the-art ANNS algorithms
must be stored in single machine's memory or disk for high recall rate and
throughput, suffering from substantial storage cost, constraint of limited
scale and single point of failure. While distributed storage can provide a
cost-effective and robust solution, there is no efficient and effective
algorithms for indexing vectors in distributed storage scenarios. In this
paper, we present a new graph-cluster hybrid indexing and search system which
supports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.
DSANN can efficiently index, store, search billion-scale vector database in
distributed storage and guarantee the high availability of index service. DSANN
employs the concurrent index construction method to significantly reduces the
complexity of index building. Then, DSANN applies Point Aggregation Graph to
leverage the structural information of graph to aggregate similar vectors,
optimizing storage efficiency and improving query throughput via asynchronous
I/O in distributed storage. Through extensive experiments, we demonstrate DSANN
can efficiently and effectively index, store and search large-scale vector
datasets in distributed storage scenarios.

### 3. [DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework](http://arxiv.org/pdf/2510.17586v1)

Authors: Boyan Li, Chong Chen, Zhujun Xue, Yinan Mei, Yuyu Luo

Large language models (LLMs) have advanced Text-to-SQL, yet existing
solutions still fall short of system-level reliability. The limitation is not
merely in individual modules - e.g., schema linking, reasoning, and
verification - but more critically in the lack of structured orchestration that
enforces correctness across the entire workflow. This gap motivates a paradigm
shift: treating Text-to-SQL not as free-form language generation but as a
software-engineering problem that demands structured, verifiable orchestration.
We present DeepEye-SQL, a software-engineering-inspired framework that reframes
Text-to-SQL as the development of a small software program, executed through a
verifiable process guided by the Software Development Life Cycle (SDLC).
DeepEye-SQL integrates four synergistic stages: it grounds ambiguous user
intent through semantic value retrieval and robust schema linking; enhances
fault tolerance with N-version SQL generation using diverse reasoning
paradigms; ensures deterministic verification via a tool-chain of unit tests
and targeted LLM-guided revision; and introduces confidence-aware selection
that clusters execution results to estimate confidence and then takes a
high-confidence shortcut or runs unbalanced pairwise adjudication in
low-confidence cases, yielding a calibrated, quality-gated output. This
SDLC-aligned workflow transforms ad hoc query generation into a disciplined
engineering process. Using ~30B open-source LLMs without any fine-tuning,
DeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on
Spider-Test, outperforming state-of-the-art solutions. This highlights that
principled orchestration, rather than LLM scaling alone, is key to achieving
system-level reliability in Text-to-SQL.

### 4. [This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!](http://arxiv.org/pdf/2510.17748v1)

Authors: William Zhang, Wan Shen Lim, Andrew Pavlo

Tuning database management systems (DBMSs) is challenging due to trillions of
possible configurations and evolving workloads. Recent advances in tuning have
led to breakthroughs in optimizing over the possible configurations. However,
due to their design and inability to leverage query-level historical insights,
existing automated tuners struggle to adapt and re-optimize the DBMS when the
environment changes (e.g., workload drift, schema transfer).
  This paper presents the Booster framework that assists existing tuners in
adapting to environment changes (e.g., drift, cross-schema transfer). Booster
structures historical artifacts into query-configuration contexts, prompts
large language models (LLMs) to suggest configurations for each query based on
relevant contexts, and then composes the query-level suggestions into a
holistic configuration with beam search. With multiple OLAP workloads, we
evaluate Booster's ability to assist different state-of-the-art tuners (e.g.,
cost-/machine learning-/LLM-based) in adapting to environment changes. By
composing recommendations derived from query-level insights, Booster assists
tuners in discovering configurations that are up to 74% better and in up to
4.7x less time than the alternative approach of continuing to tune from
historical configurations.

### 5. [Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models](http://arxiv.org/pdf/2510.17301v1)

Authors: Panos Kalnis. Shuo Shang, Christian S. Jensen

Spatio-temporal data captures complex dynamics across both space and time,
yet traditional visualizations are complex, require domain expertise and often
fail to resonate with broader audiences. Here, we propose MapMuse, a
storytelling-based framework for interpreting spatio-temporal datasets,
transforming them into compelling, narrative-driven experiences. We utilize
large language models and employ retrieval augmented generation (RAG) and
agent-based techniques to generate comprehensive stories. Drawing on principles
common in cinematic storytelling, we emphasize clarity, emotional connection,
and audience-centric design. As a case study, we analyze a dataset of taxi
trajectories. Two perspectives are presented: a captivating story based on a
heat map that visualizes millions of taxi trip endpoints to uncover urban
mobility patterns; and a detailed narrative following a single long taxi
journey, enriched with city landmarks and temporal shifts. By portraying
locations as characters and movement as plot, we argue that data storytelling
drives insight, engagement, and action from spatio-temporal information. The
case study illustrates how MapMuse can bridge the gap between data complexity
and human understanding. The aim of this short paper is to provide a glimpse to
the potential of the cinematic storytelling technique as an effective
communication tool for spatio-temporal data, as well as to describe open
problems and opportunities for future research.

### Distributed, Parallel, and Cluster Computing

### 1. [On the Universality of Round Elimination Fixed Points](http://arxiv.org/pdf/2510.17639v1)

Authors: Alkida Balliu, Sebastian Brandt, Ole Gabsdil, Dennis Olivetti, Jukka Suomela

Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

### 2. [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](http://arxiv.org/pdf/2510.17158v1)

Authors: Daniel Nichols, Konstantinos Parasyris, Charles Jekel, Abhinav Bhatele, Harshitha Menon

Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

### 3. [Quantum Federated Learning: Architectural Elements and Future Directions](http://arxiv.org/pdf/2510.17642v1)

Authors: Siva Sai, Abhishek Sawaika, Prabhjot Singh, Rajkumar Buyya

Federated learning (FL) focuses on collaborative model training without the
need to move the private data silos to a central server. Despite its several
benefits, the classical FL is plagued with several limitations, such as high
computational power required for model training(which is critical for
low-resource clients), privacy risks, large update traffic, and non-IID
heterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated
Learning (QFL), which introduces quantum computation, that addresses multiple
challenges of classical FL and offers rapid computing capability while keeping
the classical orchestration intact. Firstly, we motivate QFL with a concrete
presentation on pain points of classical FL, followed by a discussion on a
general architecture of QFL frameworks specifying the roles of client and
server, communication primitives and the quantum model placement. We classify
the existing QFL systems based on four criteria - quantum architecture (pure
QFL, hybrid QFL), data processing method (quantum data encoding, quantum
feature mapping, and quantum feature selection & dimensionality reduction),
network topology (centralized, hierarchial, decentralized), and quantum
security mechanisms (quantum key distribution, quantum homomorphic encryption,
quantum differential privacy, blind quantum computing). We then describe
applications of QFL in healthcare, vehicular networks, wireless networks, and
network security, clearly highlighting where QFL improves communication
efficiency, security, and performance compared to classical FL. We close with
multiple challenges and future works in QFL, including extension of QFL beyond
classification tasks, adversarial attacks, realistic hardware deployment,
quantum communication protocols deployment, aggregation of different quantum
models, and quantum split learning as an alternative to QFL.

### Discrete Mathematics

### 1. [Efficient recognition algorithms for $(1,2)$-, $(2,1)$- and $(2,2)$-graphs](http://arxiv.org/pdf/2510.17665v1)

Authors: Flavia Bonomo-Braberman, Min Chih Lin, Ignacio Maqueda

A graph $G$ is said to be a $(k,\ell)$-graph if its vertex set can be
partitioned into $k$ independent sets and $\ell$ cliques. It is well
established that the recognition problem for $(k,\ell)$-graphs is NP-complete
whenever $k \geq 3$ or $\ell \geq 3$, while it is solvable in polynomial time
otherwise. In particular, for the case $k+\ell \leq 2$, recognition can be
carried out in linear time, since split graphs coincide with the class of
$(1,1)$-graphs, bipartite graphs correspond precisely to $(2,0)$-graphs, and
$(\ell,k)$-graphs are the complements of $(k,\ell)$-graphs. Recognition
algorithms for $(2,1)$- and $(1,2)$-graphs were provided by Brandst\"adt, Le
and Szymczak in 1998, while the case of $(2,2)$-graphs was addressed by Feder,
Hell, Klein, and Motwani in 1999. In this work, we refine these results by
presenting improved recognition algorithms with lower time complexity.
Specifically, we reduce the running time from $O((n+m)^2)$ to $O(n^2+nm)$ for
$(2,1)$-graphs, from $O((n+\overline{m})^2)$ to $O(n^2+n\overline{m})$ for
$(1,2)$-graphs, and from $O(n^{10}(n+m))$ to $O(n^4
(n+\min\{m,\overline{m}\})^3)$ for $(2,2)$-graphs. Here, $n$ and $m$ denote the
number of vertices and edges of the input graph $G$, respectively, and
$\overline{m}$ denotes the number of edges in the complement of $G$.

### 2. [Hitting all longest paths in $H$-free graphs and $H$-graphs](http://arxiv.org/pdf/2510.17312v1)

Authors: Paloma T. de Lima, Amir Nikabadi, Paweł Rzążewski

The \textit{longest path transversal number} of a connected graph $G$,
denoted by $lpt(G)$, is the minimum size of a set of vertices of $G$ that
intersects all longest paths in $G$. We present constant upper bounds for the
longest path transversal number of \textit{hereditary classes of graphs}, that
is, classes of graphs closed under taking induced subgraphs. Our first main
result is a structural theorem that allows us to \textit{refine} a given
longest path transversal in a graph using domination properties. This has
several consequences: First, it implies that for every $t \in \{5,6\}$, every
connected $P_t$-free graph $G$ satisfies $lpt(G) \leq t-2$. Second, it shows
that every $(\textit{bull}, \textit{chair})$-free graph $G$ satisfies $lpt(G)
\leq 5$. Third, it implies that for every $t \in \mathbb{N}$, every connected
chordal graph $G$ with no induced subgraph isomorphic to $K_t \mat
\overline{K_t}$ satisfies $lpt(G) \leq t-1$, where $K_t \mat \overline{K_t}$ is
the graph obtained from a $t$-clique and an independent set of size $t$ by
adding a perfect matching between them. Our second main result provides an
upper bound for the longest path transversal number in \textit{$H$-intersection
graphs}. For a given graph $H$, a graph $G$ is called an \textit{$H$-graph} if
there exists a subdivision $H'$ of $H$ such that $G$ is the intersection graph
of a family of vertex subsets of $H'$ that each induce connected subgraphs. The
concept of $H$-graphs, introduced by Bir\'o, Hujter, and Tuza, naturally
captures interval graphs, circular-arc graphs, and chordal graphs, among
others. Our result shows that for every connected graph $H$ with at least two
vertices, there exists an integer $k = k(H)$ such that every connected
$H$-graph $G$ satisfies $lpt(G) \leq k$.

### 3. [The Parameterized Complexity of Computing the VC-Dimension](http://arxiv.org/pdf/2510.17451v1)

Authors: Florent Foucaud, Harmender Gahlawat, Fionn Mc Inerney, Prafullkumar Tale

The VC-dimension is a fundamental and well-studied measure of the complexity
of a set system (or hypergraph) that is central to many areas of machine
learning. We establish several new results on the complexity of computing the
VC-dimension. In particular, given a hypergraph
$\mathcal{H}=(\mathcal{V},\mathcal{E})$, we prove that the naive
$2^{\mathcal{O}(|\mathcal{V}|)}$-time algorithm is asymptotically tight under
the Exponential Time Hypothesis (ETH). We then prove that the problem admits a
1-additive fixed-parameter approximation algorithm when parameterized by the
maximum degree of $\mathcal{H}$ and a fixed-parameter algorithm when
parameterized by its dimension, and that these are essentially the only such
exploitable structural parameters. Lastly, we consider a generalization of the
problem, formulated using graphs, which captures the VC-dimension of both set
systems and graphs. We show that it is fixed-parameter tractable parameterized
by the treewidth of the graph (which, in the case of set systems, applies to
the treewidth of its incidence graph). In contrast with closely related
problems whose dependency on the treewidth is necessarily double-exponential
(assuming the ETH), our algorithm has a relatively low dependency on the
treewidth.

### Data Structures and Algorithms

### 1. [Combinatorial Maximum Flow via Weighted Push-Relabel on Shortcut Graphs](http://arxiv.org/pdf/2510.17182v1)

Authors: Aaron Bernstein, Joakim Blikstad, Jason Li, Thatchaphol Saranurak, Ta-Wei Tu

We give a combinatorial algorithm for computing exact maximum flows in
directed graphs with $n$ vertices and edge capacities from $\{1,\dots,U\}$ in
$\tilde{O}(n^{2}\log U)$ time, which is near-optimal on dense graphs. This
shaves an $n^{o(1)}$ factor from the recent result of
[Bernstein-Blikstad-Saranurak-Tu FOCS'24] and, more importantly, greatly
simplifies their algorithm. We believe that ours is by a significant margin the
simplest of all algorithms that go beyond $\tilde{O}(m\sqrt{n})$ time in
general graphs. To highlight this relative simplicity, we provide a full
implementation of the algorithm in C++.
  The only randomized component of our work is the cut-matching game. Via
existing tools, we show how to derandomize it for vertex-capacitated max flow
and obtain a deterministic $\tilde{O}(n^2)$ time algorithm. This marks the
first deterministic near-linear time algorithm for this problem (or even for
the special case of bipartite matching) in any density regime.

### 2. [Finding 4-Additive Spanners: Faster, Stronger, and Simpler](http://arxiv.org/pdf/2510.17262v1)

Authors: Chuhan Qi

Additive spanners are fundamental graph structures with wide applications in
network design, graph sparsification, and distance approximation. In
particular, a $4$-additive spanner is a subgraph that preserves all pairwise
distances up to an additive error of $4$. In this paper, we present a new
deterministic algorithm for constructing $4$-additive spanners that matches the
best known edge bound of $\tilde{O}(n^{7/5})$ (up to polylogarithmic factors),
while improving the running time to $\tilde{O}(\min\{mn^{3/5}, n^{11/5}\})$,
compared to the previous $\tilde{O}(mn^{3/5})$ randomized construction. Our
algorithm is not only faster in the dense regime but also fully deterministic,
conceptually simpler, and easier to implement and analyze.

### 3. [On Algorithmic Meta-Theorems for Solution Discovery: Tractability and Barriers](http://arxiv.org/pdf/2510.17344v1)

Authors: Nicolas Bousquet, Amer E. Mouawad, Stephanie Maaz, Naomi Nishimura, Sebastian Siebertz

Solution discovery asks whether a given (infeasible) starting configuration
to a problem can be transformed into a feasible solution using a limited number
of transformation steps. This paper investigates meta-theorems for solution
discovery for graph problems definable in monadic second-order logic (MSO$_1$
and MSO$_2$) and first-order logic (FO) where the transformation step is to
slide a token to an adjacent vertex, focusing on parameterized complexity and
structural graph parameters that do not involve the transformation budget $b$.
We present both positive and negative results. On the algorithmic side, we
prove that MSO$_2$-Discovery is in XP when parameterized by treewidth and that
MSO$_1$-Discovery is fixed-parameter tractable when parameterized by
neighborhood diversity. On the hardness side, we establish that FO-Discovery is
W[1]-hard when parameterized by modulator to stars, modulator to paths, as well
as twin cover, numbers. Additionally, we prove that MSO$_1$-Discovery is
W[1]-hard when parameterized by bandwidth. These results complement the
straightforward observation that solution discovery for the studied problems is
fixed-parameter tractable when the budget $b$ is included in the parameter (in
particular, parameterized by cliquewidth$+b$, where the cliquewidth of a graph
is at most any of the studied parameters), and provide a near-complete
(fixed-parameter tractability) meta-theorems investigation for solution
discovery problems for MSO- and FO-definable graph problems and structural
parameters larger than cliquewidth.

### 4. [Approximating Asymmetric A Priori TSP beyond the Adaptivity Gap](http://arxiv.org/pdf/2510.17595v1)

Authors: Manuel Christalla, Luise Puhlmann, Vera Traub

In Asymmetric A Priori TSP (with independent activation probabilities) we are
given an instance of the Asymmetric Traveling Salesman Problem together with an
activation probability for each vertex. The task is to compute a tour that
minimizes the expected length after short-cutting to the randomly sampled set
of active vertices.
  We prove a polynomial lower bound on the adaptivity gap for Asymmetric A
Priori TSP. Moreover, we show that a poly-logarithmic approximation ratio, and
hence an approximation ratio below the adaptivity gap, can be achieved by a
randomized algorithm with quasi-polynomial running time.
  To achieve this, we provide a series of polynomial-time reductions. First we
reduce to a novel generalization of the Asymmetric Traveling Salesman Problem,
called Hop-ATSP. Next, we use directed low-diameter decompositions to obtain
structured instances, for which we then provide a reduction to a covering
problem. Eventually, we obtain a polynomial-time reduction of Asymmetric A
Priori TSP to a problem of finding a path in an acyclic digraph minimizing a
particular objective function, for which we give an O(log n)-approximation
algorithm in quasi-polynomial time.

### 5. [Near-Optimal Property Testers for Pattern Matching](http://arxiv.org/pdf/2510.17645v1)

Authors: Ce Jin, Tomasz Kociumaka

The classic exact pattern matching problem, given two strings -- a pattern
$P$ of length $m$ and a text $T$ of length $n$ -- asks whether $P$ occurs as a
substring of $T$. A property tester for the problem needs to distinguish (with
high probability) the following two cases for some threshold $k$: the YES case,
where $P$ occurs as a substring of $T$, and the NO case, where $P$ has Hamming
distance greater than $k$ from every substring of $T$, that is, $P$ has no
$k$-mismatch occurrence in $T$.
  In this work, we provide adaptive and non-adaptive property testers for the
exact pattern matching problem, jointly covering the whole spectrum of
parameters. We further establish unconditional lower bounds demonstrating that
the time and query complexities of our algorithms are optimal, up to
$\mathrm{polylog}\, n$ factors hidden within the $\tilde O(\cdot)$ notation
below.
  In the most studied regime of $n=m+\Theta(m)$, our non-adaptive property
tester has the time complexity of $\tilde O(n/\sqrt{k})$, and a matching lower
bound remains valid for the query complexity of adaptive algorithms. This
improves both upon a folklore solution that attains the optimal query
complexity but requires $\Omega(n)$ time, and upon the only previously known
sublinear-time property tester, by Chan, Golan, Kociumaka, Kopelowitz, and
Porat [STOC 2020], with time complexity $\tilde O(n/\sqrt[3]{k})$. The
aforementioned results remain valid for $n=m+\Omega(m)$, where our optimal
running time $\tilde O(\sqrt{nm/k}+n/k)$ improves upon the previously best time
complexity of $\tilde O(\sqrt[3]{n^2m/k}+n/k)$. In the regime of $n=m+o(m)$,
which has not been targeted in any previous work, we establish a surprising
separation between adaptive and non-adaptive algorithms, whose optimal time and
query complexities are $\tilde O(\sqrt{(n-m+1)m/k}+n/k)$ and $\tilde
O(\min(n\sqrt{n-m+1}/k,\sqrt{nm/k}+n/k))$, respectively.

### 6. [Generalized Flow in Nearly-linear Time on Moderately Dense Graphs](http://arxiv.org/pdf/2510.17740v1)

Authors: Shunhua Jiang, Michael Kapralov, Lawrence Li, Aaron Sidford

In this paper we consider generalized flow problems where there is an
$m$-edge $n$-node directed graph $G = (V,E)$ and each edge $e \in E$ has a loss
factor $\gamma_e >0$ governing whether the flow is increased or decreased as it
crosses edge $e$. We provide a randomized $\tilde{O}( (m + n^{1.5}) \cdot
\mathrm{polylog}(\frac{W}{\delta}))$ time algorithm for solving the generalized
maximum flow and generalized minimum cost flow problems in this setting where
$\delta$ is the target accuracy and $W$ is the maximum of all costs,
capacities, and loss factors and their inverses. This improves upon the
previous state-of-the-art $\tilde{O}(m \sqrt{n} \cdot \log^2(\frac{W}{\delta})
)$ time algorithm, obtained by combining the algorithm of [Daitch-Spielman,
2008] with techniques from [Lee-Sidford, 2014]. To obtain this result we
provide new dynamic data structures and spectral results regarding the matrices
associated to generalized flows and apply them through the interior point
method framework of [Brand-Lee-Liu-Saranurak-Sidford-Song-Wang, 2021].

### 7. [Pattern Matching under Weighted Edit Distance](http://arxiv.org/pdf/2510.17752v1)

Authors: Panagiotis Charalampopoulos, Tomasz Kociumaka, Philip Wellnitz

In Pattern Matching with Weighted Edits (PMWED), we are given a pattern $P$
of length $m$, a text $T$ of length $n$, a positive threshold $k$, and oracle
access to a weight function that specifies the costs of edits (depending on the
involved characters, and normalized so that the cost of each edit is at least
$1$). The goal is to compute the starting positions of all fragments of $T$
that can be obtained from $P$ with edits of total cost at most $k$. PMWED
captures typical real-world applications more accurately than its unweighted
variant (PMED), where all edits have unit costs.
  We obtain three main results:
  (a) a conceptually simple $\tilde{O}(nk)$-time algorithm for PMWED, very
different from that of Landau and Vishkin for PMED;
  (b) a significantly more complicated $\tilde{O}(n+k^{3.5} \cdot W^4\cdot
n/m)$-time algorithm for PMWED under the assumption that the weight function is
a metric with integer values between $0$ and $W$; and
  (c) an $\tilde{O}(n+k^4 \cdot n/m)$-time algorithm for PMWED for the case of
arbitrary weights.
  In the setting of metrics with small integer values, we nearly match the
state of the art for PMED where $W=1$.

### 8. [Dynamic Dyck and Tree Edit Distance: Decompositions and Reductions to String Edit Distance](http://arxiv.org/pdf/2510.17799v1)

Authors: Debarati Das, Jacob Gilbert, MohammadTaghi Hajiaghayi, Tomasz Kociumaka, Barna Saha

We present the first dynamic algorithms for Dyck and tree edit distances with
subpolynomial update times. Dyck edit distance measures how far a parenthesis
string is from a well-parenthesized expression, while tree edit distance
quantifies the minimum number of node insertions, deletions, and substitutions
required to transform one rooted, ordered, labeled tree into another. Despite
extensive study, no prior work has addressed efficient dynamic algorithms for
these problems, which naturally arise in evolving structured data such as LaTeX
documents, JSON or XML files, and RNA secondary structures.
  Our main contribution is a set of reductions and decompositions that
transform Dyck and tree edit distance instances into efficiently maintainable
string edit distance instances, which can be approximated within a $n^{o(1)}$
factor in $n^{o(1)}$ update time. For Dyck edit distance, our reduction incurs
only polylogarithmic overheads in approximation and update time, yielding an
$n^{o(1)}$-approximation with $n^{o(1)}$ updates. For tree edit distance, we
introduce a new static reduction that improves the best-known approximation
ratio from $n^{3/4}$ to $\tilde{O}(\sqrt{n})$ and removes the restriction to
constant-degree trees. Extending this reduction dynamically achieves
$n^{1/2+o(1)}$ approximation with $n^{o(1)}$ update time.
  A key component is a dynamic maintenance algorithm for history-independent
heavy-light decompositions, of independent interest. We also provide a novel
static and dynamic decomposition achieving an $O(k \log n)$-approximation when
the tree edit distance is at most $k$. Combined with the trivial bound $k \le
n$, this yields a dynamic deterministic $O(\sqrt{n \log n})$-approximation. In
the static setting, our algorithm runs in near-linear time; dynamically, it
requires only polylogarithmic updates, improving on prior linear-time static
$O(\sqrt{n})$-approximation.

### 9. [Opinion Maximization in Social Networks by Modifying Internal Opinions](http://arxiv.org/pdf/2510.17226v1)

Authors: Gengyu Wang, Runze Zhang, Zhongzhi Zhang

Public opinion governance in social networks is critical for public health
campaigns, political elections, and commercial marketing. In this paper, we
addresse the problem of maximizing overall opinion in social networks by
strategically modifying the internal opinions of key nodes. Traditional matrix
inversion methods suffer from prohibitively high computational costs, prompting
us to propose two efficient sampling-based algorithms. Furthermore, we develop
a deterministic asynchronous algorithm that exactly identifies the optimal set
of nodes through asynchronous update operations and progressive refinement,
ensuring both efficiency and precision. Extensive experiments on real-world
datasets demonstrate that our methods outperform baseline approaches. Notably,
our asynchronous algorithm delivers exceptional efficiency and accuracy across
all scenarios, even in networks with tens of millions of nodes.

### 10. [Unifying the Landscape of Super-Logarithmic Dynamic Cell-Probe Lower Bounds](http://arxiv.org/pdf/2510.17717v1)

Authors: Young Kun Ko

We prove a general translation theorem for converting one-way communication
lower bounds over a product distribution to dynamic cell-probe lower bounds.
  Specifically, we consider a class of problems considered in [Pat10] where:
  1. $S_1, \ldots, S_m \in \{0, 1\}^n$ are given and publicly known.
  2. $T \in \{0, 1\}^n$ is a sequence of updates, each taking $t_u$ time.
  3. For a given $Q \in [m]$, we must output $f(S_Q, T)$ in $t_q$ time. Our
main result shows that for a "hard" function $f$, for which it is difficult to
obtain a non-trivial advantage over random guessing with one-way communication
under some product distribution over $S_Q$ and $T$ (for example, a uniform
distribution), then the above explicit dynamic cell-probe problem must have
$\max \{ t_u, t_q \} \geq \tilde{\Omega}(\log^{3/2}(n))$ if $m =
\Omega(n^{0.99})$. This result extends and unifies the super-logarithmic
dynamic data structure lower bounds from [LWY20] and [LY25] into a more general
framework.
  From a technical perspective, our approach merges the cell-sampling and
chronogram techniques developed in [LWY20] and [LY25] with the new static data
structure lower bound methods from [KW20] and [Ko25], thereby merging all known
state-of-the-art cell-probe lower-bound techniques into one.
  As a direct consequence of our method, we establish a super-logarithmic lower
bound against the Multiphase Problem [Pat10] for the case where the data
structure outputs the Inner Product (mod 2) of $S_Q$ and $T$. We suspect
further applications of this general method towards showing super-logarithmic
dynamic cell-probe lower bounds. We list some example applications of our
general method, including a novel technique for a one-way communication lower
bound against small-advantage protocols for a product distribution using
average min-entropy, which could be of independent interest.

### Emerging Technologies

### 1. [Navigate in Demanding Missions: Integrating Human Intelligence and Brain-Inspired Intelligence](http://arxiv.org/pdf/2510.17530v1)

Authors: Xu He, Xiaolin Meng, Youdong Zhang, Lingfei Mo, Wenxuan Yin

This perspective analyzes the intricate interplay among neuroscience,
Brain-Inspired Intelligence (BII), and Brain-Inspired Navigation (BIN),
revealing a current lack of cooperative relationship between Brain-Computer
Interfaces (BCIs) and BIN fields. We advocate for the integration of
neuromorphic-empowered BCI into BIN, thereby bolstering the unmanned systems'
reliable navigation in demanding missions, such as deep space exploration, etc.
We highlight that machine intelligence, reinforced by brain-inspired artificial
consciousness, can extend human intelligence, with human intelligence mediated
by neuromorphic-enabled BCI acting as a safeguard in case machine intelligence
failures. This study also discusses the potentials of the proposed approach to
enhance unmanned systems' capabilities and facilitate the diagnostics of
spatial cognition disorders, while considering associated ethical and security
concerns.

### 2. [Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring](http://arxiv.org/pdf/2510.17688v1)

Authors: Shawn M. Gibford, Mohammad Reza Boskabadi, Christopher J. Savoie, Seyed Soheil Mansouri

Data scarcity and sparsity in bio-manufacturing poses challenges for accurate
model
  development, process monitoring, and optimization. We aim to replicate and
capture
  the complex dynamics of industrial bioprocesses by proposing the use of a
Quantum
  Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP)
to
  generate synthetic time series data for industrially relevant processes. The
  generator within our GAN is comprised of a Parameterized Quantum Circuit
(PQC). This
  methodology offers potential advantages in process monitoring, modeling,
  forecasting, and optimization, enabling more efficient bioprocess management
by
  reducing the dependence on scarce experimental data. Our results demonstrate
  acceptable performance in capturing the temporal dynamics of real bioprocess
data.
  We focus on Optical Density, a key measurement for Dry Biomass estimation.
The data
  generated showed high fidelity to the actual historical experimental data.
This
  intersection of quantum computing and machine learning has opened new
frontiers in
  data analysis and generation, particularly in computationally intensive
fields, for
  use cases such as increasing prediction accuracy for soft sensor design or
for use
  in predictive control.

### Formal Languages and Automata Theory

### 1. [Castor Ministerialis](http://arxiv.org/pdf/2510.17438v1)

Authors: Christian Hercher

The famous problem of Busy Beavers can be stated as the question on how long
a $n$-state Turing machine (using a 2-symbol alphabet or -- in a generalization
-- a $m$-symbol alphabet) can run if it is started on the blank tape before it
holds. Thus, not halting Turing machines are excluded. For up to four states
the answer to this question is well-known. Recently, it could be verified that
the widely assumed candidate for five states is in fact the champion. And there
is progress in searching for good candidates with six or more states.
  We investigate a variant of this problem: Additionally to the requirement
that the Turing machines have to start from the blank tape we only consider
such Turing machines that hold on the blank tape, too. For this variant we give
definitive answers on how long such a Turing machine with up to five states can
run, analyze the behavior of a six-states candidate and give some findings on
the generalization of Turing-machines with $m$-symbol alphabet.

### 2. [Non-interference analysis of bounded labeled Petri nets](http://arxiv.org/pdf/2510.17582v1)

Authors: Ning Ran, Zhengguang Wu, Shaokang Zhang, Zhou He, Carla Seatzu

This paper focuses on a fundamental problem on information security of
bounded labeled Petri nets: non-interference analysis. As in hierarchical
control, we assume that a system is observed by users at different levels,
namely high-level users and low-level users. The output events produced by the
firing of transitions are also partitioned into high-level output events and
low-level output events. In general, high-level users can observe the
occurrence of all the output events, while low-level users can only observe the
occurrence of low-level output events. A system is said to be non-interferent
if low-level users cannot infer the firing of transitions labeled with
high-level output events by looking at low-level outputs. In this paper, we
study a particular non-interference property, namely strong non-deterministic
non-interference (SNNI), using a special automaton called SNNI Verifier, and
propose a necessary and sufficient condition for SNNI.

### 3. [Inference of Deterministic Finite Automata via Q-Learning](http://arxiv.org/pdf/2510.17386v1)

Authors: Elaheh Hosseinkhani, Martin Leucker

Traditional approaches to inference of deterministic finite-state automata
(DFA) stem from symbolic AI, including both active learning methods (e.g.,
Angluin's L* algorithm and its variants) and passive techniques (e.g., Biermann
and Feldman's method, RPNI). Meanwhile, sub-symbolic AI, particularly machine
learning, offers alternative paradigms for learning from data, such as
supervised, unsupervised, and reinforcement learning (RL). This paper
investigates the use of Q-learning, a well-known reinforcement learning
algorithm, for the passive inference of deterministic finite automata. It
builds on the core insight that the learned Q-function, which maps state-action
pairs to rewards, can be reinterpreted as the transition function of a DFA over
a finite domain. This provides a novel bridge between sub-symbolic learning and
symbolic representations. The paper demonstrates how Q-learning can be adapted
for automaton inference and provides an evaluation on several examples.

### 4. [Multihead Finite-State Compression](http://arxiv.org/pdf/2510.17544v1)

Authors: Neil Lutz

This paper develops multihead finite-state compression, a generalization of
finite-state compression, complementary to the multihead finite-state
dimensions of Huang, Li, Lutz, and Lutz (2025). In this model, an infinite
sequence of symbols is compressed by a compressor that produces outputs
according to finite-state rules, based on the symbols read by a constant number
of finite-state read heads moving forward obliviously through the sequence. The
main theorem of this work establishes that for every sequence and every
positive integer $h$, the infimum of the compression ratios achieved by
$h$-head finite-state information-lossless compressors equals the $h$-head
finite-state predimension of the sequence. As an immediate corollary, the
infimum of these ratios over all $h$ is the multihead finite-state dimension of
the sequence.

### Graphics

### 1. [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](http://arxiv.org/pdf/2510.17101v1)

Authors: Lu Yin, Ziying Shi, Yinghao Wu, Xinyu Yi, Feng Xu, Shihui Guo

Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

### Computer Science and Game Theory

### 1. [Eliciting Truthful Feedback for Preference-Based Learning via the VCG Mechanism](http://arxiv.org/pdf/2510.17285v1)

Authors: Leo Landolt, Anna Maddux, Andreas Schlaginhaufen, Saurabh Vaishampayan, Maryam Kamgarpour

We study resource allocation problems in which a central planner allocates
resources among strategic agents with private cost functions in order to
minimize a social cost, defined as an aggregate of the agents' costs. This
setting poses two main challenges: (i) the agents' cost functions may be
unknown to them or difficult to specify explicitly, and (ii) agents may
misreport their costs strategically. To address these challenges, we propose an
algorithm that combines preference-based learning with Vickrey-Clarke-Groves
(VCG) payments to incentivize truthful reporting. Our algorithm selects
informative preference queries via D-optimal design, estimates cost parameters
through maximum likelihood, and computes VCG allocations and payments based on
these estimates. In a one-shot setting, we prove that the mechanism is
approximately truthful, individually rational, and efficient up to an error of
$\tilde{\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an
online setting, these guarantees hold asymptotically with sublinear regret at a
rate of $\tilde{\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate
our approach through a numerical case study on demand response in local
electricity markets.

### 2. [On the Universal Near Optimality of Hedge in Combinatorial Settings](http://arxiv.org/pdf/2510.17099v1)

Authors: Zhiyuan Fan, Arnab Maiti, Kevin Jamieson, Lillian J. Ratliff, Gabriele Farina

In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

### 3. [Convergence of Regret Matching in Potential Games and Constrained Optimization](http://arxiv.org/pdf/2510.17067v1)

Authors: Ioannis Anagnostides, Emanuel Tewolde, Brian Hu Zhang, Ioannis Panageas, Vincent Conitzer, Tuomas Sandholm

Regret matching (RM} -- and its modern variants -- is a foundational online
algorithm that has been at the heart of many AI breakthrough results in solving
benchmark zero-sum games, such as poker. Yet, surprisingly little is known so
far in theory about its convergence beyond two-player zero-sum games. For
example, whether regret matching converges to Nash equilibria in potential
games has been an open problem for two decades. Even beyond games, one could
try to use RM variants for general constrained optimization problems. Recent
empirical evidence suggests that they -- particularly regret matching$^+$
(RM$^+$) -- attain strong performance on benchmark constrained optimization
problems, outperforming traditional gradient descent-type algorithms.
  We show that alternating RM$^+$ converges to an $\epsilon$-KKT point after
$O_\epsilon(1/\epsilon^4)$ iterations, establishing for the first time that it
is a sound and fast first-order optimizer. Our argument relates the KKT gap to
the accumulated regret, two quantities that are entirely disparate in general
but interact in an intriguing way in our setting, so much so that when regrets
are bounded, our complexity bound improves all the way to
$O_\epsilon(1/\epsilon^2)$. From a technical standpoint, while RM$^+$ does not
have the usual one-step improvement property in general, we show that it does
in a certain region that the algorithm will quickly reach and remain in
thereafter. In sharp contrast, our second main result establishes a lower
bound: RM, with or without alternation, can take an exponential number of
iterations to reach a crude approximate solution even in two-player potential
games. This represents the first worst-case separation between RM and RM$^+$.
Our lower bound shows that convergence to coarse correlated equilibria in
potential games is exponentially faster than convergence to Nash equilibria.

### 4. [Data Reliability Scoring](http://arxiv.org/pdf/2510.17085v1)

Authors: Yiling Chen, Shi Feng, Paul Kattuman, Fang-Yi Yu

How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

### Human-Computer Interaction

### 1. [Planar or Spatial: Exploring Design Aspects and Challenges for Presentations in Virtual Reality with No-coding Interface](http://arxiv.org/pdf/2510.17073v1)

Authors: Liwei Wu, Yilin Zhang, Justin Leung, Jingyi Gao, April Li, Jian Zhao

The proliferation of virtual reality (VR) has led to its increasing adoption
as an immersive medium for delivering presentations, distinct from other VR
experiences like games and 360-degree videos by sharing information in richly
interactive environments. However, creating engaging VR presentations remains a
challenging and time-consuming task for users, hindering the full realization
of VR presentation's capabilities. This research aims to explore the potential
of VR presentation, analyze users' opinions, and investigate these via
providing a user-friendly no-coding authoring tool. Through an examination of
popular presentation software and interviews with seven professionals, we
identified five design aspects and four design challenges for VR presentations.
Based on the findings, we developed VRStory, a prototype for presentation
authoring without coding to explore the design aspects and strategies for
addressing the challenges. VRStory offers a variety of predefined and
customizable VR elements, as well as modules for layout design, navigation
control, and asset generation. A user study was then conducted with 12
participants to investigate their opinions and authoring experience with
VRStory. Our results demonstrated that, while acknowledging the advantages of
immersive and spatial features in VR, users often have a consistent mental
model for traditional 2D presentations and may still prefer planar and static
formats in VR for better accessibility and efficient communication. We finally
shared our learned design considerations for future development of VR
presentation tools, emphasizing the importance of balancing of promoting
immersive features and ensuring accessibility.

### 2. [Toward a Cognitive-Affective-Systemic Framework for Art and Sustainability](http://arxiv.org/pdf/2510.17083v1)

Authors: Ivan C. H. Liu

This paper proposes a ognitive-Affective-Systemic (CAS) framework that
integrates cognition, emotion, and systemic understanding to cultivate
sustainability awareness through art. Drawing from eco-aesthetics, affect
theory, complexity science, and posthuman ethics, the framework defines
artistic practice as both epistemic and performative--a way of knowing through
making and feeling. Central to this is logomotion, an aesthetic mode where
comprehension and emotion move together as a unified experience. Two artworks,
SPill, visualizing antimicrobial resistance through avalanche dynamics, and
Echoes of the Land, modeling anthropogenic seismicity, demonstrate how systemic
modeling and sensory immersion transform complex science into embodied
ecological understanding. The framework offers a methodological foundation for
artists, theorists, and activists to translate awareness into engagement,
advancing collective creativity toward sustainable futures.

### 3. [Kinesthetic Weight Modulation: The Effects of Whole-Arm Tendon Vibration on the Perceived Heaviness](http://arxiv.org/pdf/2510.17102v1)

Authors: Keigo Ushiyama, Hiroyuki Kajimoto

Kinesthetic illusions, which arise when muscle spindles are activated by
vibration, provide a compact means of presenting kinesthetic sensations.
Because muscle spindles contribute not only to sensing body movement but also
to perceiving heaviness, vibration-induced illusions could potentially modulate
weight perception. While prior studies have primarily focused on conveying
virtual movement, the modulation of perceived heaviness has received little
attention. Presenting a sense of heaviness is essential for enriching haptic
interactions with virtual objects. This study investigates whether multi-point
tendon vibration can increase or decrease perceived heaviness (Experiment 1)
and how the magnitude of the effect can be systematically controlled
(Experiment 2). The results show that tendon vibration significantly increases
perceived heaviness but does not significantly decrease it, although a
decreasing trend was observed. Moreover, the increase can be adjusted across at
least three levels within the range of 350-450 g. Finally, we discuss plausible
mechanisms underlying this vibration-induced modulation of weight perception.

### 4. [Design Framework for Conversational Agent in Couple relationships: A Systematic Review](http://arxiv.org/pdf/2510.17119v1)

Authors: Soyoung Jung, Sung Park

The development of conversational agents (CAs) has shown strong potential in
supporting mental health through dialogue. While many studies focus on CAs for
individual psychological care, research on agents designed for couples facing
relational or emotional challenges remains limited. This study aims to identify
design considerations for CAs that address the relational context of couples
and support their well-being. Following PRISMA guidelines, a systematic review
was conducted across seven databases: CINAHL, Embase, PubMed, PsycINFO, Scopus,
Web of Science, and the ACM Digital Library. Peer-reviewed empirical studies
were screened, duplicates removed, and selection criteria applied, resulting in
twelve studies for analysis. Thematic analysis was conducted across three
dimensions: AI interaction design, relational framing, and technical
limitations. Three key themes emerged: (1) the need for a relational expert
persona, (2) technological directions leveraging state-of-the-art AI for
relational specificity and emotional competence, and (3) a shift from
content-centered to relationship-centered design. Based on these insights,
eight design considerations are proposed for couple-oriented CAs: (1) agent
persona, (2) individual mode, (3) concurrent mode, (4) conjoint mode, (5)
ethics, (6) data and privacy, (7) interaction pattern, and (8) safety
mechanism. These principles guide CAs as relational mediators capable of
maintaining multiple alliances, respecting cultural and ethical boundaries, and
ensuring fairness and emotional safety between partners. Ultimately, this
review introduces a design framework that integrates relational theory with
advanced AI technologies to inform future development of CAs for couple-based
mental health interventions.

### 5. [SmartSustain Recommender System: Navigating Sustainability Trade-offs in Personalized City Trip Planning](http://arxiv.org/pdf/2510.17355v1)

Authors: Ashmi Banerjee, Melih Mert Aksoy, Wolfgang Wörndl

Tourism is a major contributor to global carbon emissions and over-tourism,
creating an urgent need for recommender systems that not only inform but also
gently steer users toward more sustainable travel decisions. Such choices,
however, often require balancing complex trade-offs between environmental
impact, cost, convenience, and personal interests. To address this, we present
the SmartSustain Recommender, a web application designed to nudge users toward
eco-friendlier options through an interactive, user-centric interface. The
system visualizes the broader consequences of travel decisions by combining
CO2e emissions, destination popularity, and seasonality with personalized
interest matching. It employs mechanisms such as interactive city cards for
quick comparisons, dynamic banners that surface sustainable alternatives in
specific trade-off scenarios, and real-time impact feedback using animated
environmental indicators. A preliminary user study with 21 participants
indicated strong usability and perceived effectiveness. The system is
accessible at https://smartsustainrecommender.web.app.

### 6. [NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based Guidance](http://arxiv.org/pdf/2510.17534v1)

Authors: Yichen Yu, Qiaoran Wang

Today's young people are facing increasing psychological stress due to
various social issues. Traditional stress management tools often rely on static
scripts or passive content, which are ineffective in alleviating stress. NieNie
addresses this gap by combining rhythm biofeedback with real-time psychological
guidance through a large language model (LLM), offering an interactive, tactile
response. The system is specifically designed for young people experiencing
emotional stress, collecting physiological signals such as heart rate
variability and generating adaptive squeeze-release rhythms via soft, tactile
devices. Utilising LLM, the system provides timely squeezing rhythms and
psychologically guided feedback prompts, offering personalised rhythm games
while reinforcing stress restructuring. Unlike traditional mental health apps,
NieNie places users within an embodied interactive loop, leveraging tactile
interaction, biofeedback, and adaptive language support to create an immersive
stress regulation experience. This study demonstrates how embodied systems can
connect bodily actions with mental health in everyday contexts.

### 7. [DeTAILS: Deep Thematic Analysis with Iterative LLM Support](http://arxiv.org/pdf/2510.17575v1)

Authors: Ash Sharma, Karen Cochrane, James R. Wallace

Thematic analysis is widely used in qualitative research but can be difficult
to scale because of its iterative, interpretive demands. We introduce DeTAILS,
a toolkit that integrates large language model (LLM) assistance into a workflow
inspired by Braun and Clarke's thematic analysis framework. DeTAILS supports
researchers in generating and refining codes, reviewing clusters, and
synthesizing themes through interactive feedback loops designed to preserve
analytic agency. We evaluated the system with 18 qualitative researchers
analyzing Reddit data. Quantitative results showed strong alignment between
LLM-supported outputs and participants' refinements, alongside reduced workload
and high perceived usefulness. Qualitatively, participants reported that
DeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,
and fostered trust through transparency and control. We contribute: (1) an
interactive human-LLM workflow for large-scale qualitative analysis, (2)
empirical evidence of its feasibility and researcher experience, and (3) design
implications for trustworthy AI-assisted qualitative research.

### 8. [Muscle Anatomy-aware Geometric Deep Learning for sEMG-based Gesture Decoding](http://arxiv.org/pdf/2510.17660v1)

Authors: Adyasha Dash, Giulia Zappoli, Laya Das, Robert Riener

Robust and accurate decoding of gesture from non-invasive surface
electromyography (sEMG) is important for various applications including spatial
computing, healthcare, and entertainment, and has been actively pursued by
researchers and industry. Majority of sEMG-based gesture decoding algorithms
employ deep neural networks that are designed for Euclidean data, and may not
be suitable for analyzing multi-dimensional, non-stationary time-series with
long-range dependencies such as sEMG. State-of-the-art sEMG-based decoding
methods also demonstrate high variability across subjects and sessions,
requiring re-calibration and adaptive fine-tuning to boost performance. To
address these shortcomings, this work proposes a geometric deep learning model
that learns on symmetric positive definite (SPD) manifolds and leverages
unsupervised domain adaptation to desensitize the model to subjects and
sessions. The model captures the features in time and across sensors with
multiple kernels, projects the features onto SPD manifold, learns on manifolds
and projects back to Euclidean space for classification. It uses a
domain-specific batch normalization layer to address variability between
sessions, alleviating the need for re-calibration or fine-tuning. Experiments
with publicly available benchmark gesture decoding datasets (Ninapro DB6,
Flexwear-HD) demonstrate the superior generalizability of the model compared to
Euclidean and other SPD-based models in the inter-session scenario, with up to
8.83 and 4.63 points improvement in accuracy, respectively. Detailed analyses
reveal that the model extracts muscle-specific information for different tasks
and ablation studies highlight the importance of modules introduced in the
work. The proposed method pushes the state-of-the-art in sEMG-based gesture
recognition and opens new research avenues for manifold-based learning for
muscle signals.

### 9. [Rethinking Search: A Study of University Students' Perspectives on Using LLMs and Traditional Search Engines in Academic Problem Solving](http://arxiv.org/pdf/2510.17726v1)

Authors: Md. Faiyaz Abdullah Sayeedi, Md. Sadman Haque, Zobaer Ibn Razzaque, Robiul Awoul Robin, Sabila Nawshin

With the increasing integration of Artificial Intelligence (AI) in academic
problem solving, university students frequently alternate between traditional
search engines like Google and large language models (LLMs) for information
retrieval. This study explores students' perceptions of both tools, emphasizing
usability, efficiency, and their integration into academic workflows. Employing
a mixed-methods approach, we surveyed 109 students from diverse disciplines and
conducted in-depth interviews with 12 participants. Quantitative analyses,
including ANOVA and chi-square tests, were used to assess differences in
efficiency, satisfaction, and tool preference. Qualitative insights revealed
that students commonly switch between GPT and Google: using Google for
credible, multi-source information and GPT for summarization, explanation, and
drafting. While neither tool proved sufficient on its own, there was a strong
demand for a hybrid solution. In response, we developed a prototype, a chatbot
embedded within the search interface, that combines GPT's conversational
capabilities with Google's reliability to enhance academic research and reduce
cognitive load.

### 10. [Augmented Web Usage Mining and User Experience Optimization with CAWAL's Enriched Analytics Data](http://arxiv.org/pdf/2510.17253v1)

Authors: Özkan Canay, {Ü}mit Kocabıcak

Understanding user behavior on the web is increasingly critical for
optimizing user experience (UX). This study introduces Augmented Web Usage
Mining (AWUM), a methodology designed to enhance web usage mining and improve
UX by enriching the interaction data provided by CAWAL (Combined Application
Log and Web Analytics), a framework for advanced web analytics. Over 1.2
million session records collected in one month (~8.5GB of data) were processed
and transformed into enriched datasets. AWUM analyzes session structures, page
requests, service interactions, and exit methods. Results show that 87.16% of
sessions involved multiple pages, contributing 98.05% of total pageviews; 40%
of users accessed various services and 50% opted for secure exits. Association
rule mining revealed patterns of frequently accessed services, highlighting
CAWAL's precision and efficiency over conventional methods. AWUM offers a
comprehensive understanding of user behavior and strong potential for
large-scale UX optimization.

### Information Retrieval

### 1. [DSEBench: A Test Collection for Explainable Dataset Search with Examples](http://arxiv.org/pdf/2510.17228v1)

Authors: Qing Shi, Jing He, Qiaosheng Chen, Gong Cheng

Dataset search has been an established information retrieval task. Current
paradigms either retrieve datasets that are relevant to a keyword query or find
datasets that are similar to an input target dataset. To allow for their
combined specification of information needs, in this article, we investigate
the more generalized task of Dataset Search with Examples (DSE) and further
extend it to Explainable DSE that requires identifying the metadata and content
fields of a dataset that indicate its relevance to the query and similarity to
the target datasets. To facilitate this research, we construct DSEBench, a test
collection that provides high-quality dataset- and field-level annotations to
enable the evaluation of explainable DSE. We also employ a large language model
to generate numerous annotations to be used for training. We establish
extensive baselines on DSEBench by adapting and evaluating a variety of sparse,
dense, and LLM-based retrieval, reranking, and explanation methods.

### 2. [On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders](http://arxiv.org/pdf/2510.17245v1)

Authors: Wenyu Mao, Jiancan Wu, Guoqing Hu, Wei Ji, Xiang Wang

Diffusion models have emerged as a powerful paradigm for generative
sequential recommendation, which typically generate next items to recommend
guided by user interaction histories with a multi-step denoising process.
However, the multi-step process relies on discrete approximations, introducing
discretization error that creates a trade-off between computational efficiency
and recommendation effectiveness. To address this trade-off, we propose TA-Rec,
a two-stage framework that achieves one-step generation by smoothing the
denoising function during pretraining while alleviating trajectory deviation by
aligning with user preferences during fine-tuning. Specifically, to improve the
efficiency without sacrificing the recommendation performance, TA-Rec pretrains
the denoising model with Temporal Consistency Regularization (TCR), enforcing
the consistency between the denoising results across adjacent steps. Thus, we
can smooth the denoising function to map the noise as oracle items in one step
with bounded error. To further enhance effectiveness, TA-Rec introduces
Adaptive Preference Alignment (APA) that aligns the denoising process with user
preference adaptively based on preference pair similarity and timesteps.
Extensive experiments prove that TA-Rec's two-stage objective effectively
mitigates the discretization errors-induced trade-off, enhancing both
efficiency and effectiveness of diffusion-based recommenders.

### 3. [How role-play shapes relevance judgment in zero-shot LLM rankers](http://arxiv.org/pdf/2510.17535v1)

Authors: Yumeng Wang, Jirui Qi, Catherine Chen, Panagiotis Eustratiadis, Suzan Verberne

Large Language Models (LLMs) have emerged as promising zero-shot rankers, but
their performance is highly sensitive to prompt formulation. In particular,
role-play prompts, where the model is assigned a functional role or identity,
often give more robust and accurate relevance rankings. However, the mechanisms
and diversity of role-play effects remain underexplored, limiting both
effective use and interpretability. In this work, we systematically examine how
role-play variations influence zero-shot LLM rankers. We employ causal
intervention techniques from mechanistic interpretability to trace how
role-play information shapes relevance judgments in LLMs. Our analysis reveals
that (1) careful formulation of role descriptions have a large effect on the
ranking quality of the LLM; (2) role-play signals are predominantly encoded in
early layers and communicate with task instructions in middle layers, while
receiving limited interaction with query or document representations.
Specifically, we identify a group of attention heads that encode information
critical for role-conditioned relevance. These findings not only shed light on
the inner workings of role-play in LLM ranking but also offer guidance for
designing more effective prompts in IR and beyond, pointing toward broader
opportunities for leveraging role-play in zero-shot applications.

### 4. [Rethinking On-policy Optimization for Query Augmentation](http://arxiv.org/pdf/2510.17139v1)

Authors: Zhichao Xu, Shengyao Zhuang, Xueguang Ma, Bingsen Chen, Yijun Tian, Fengran Mo, Jie Cao, Vivek Srikumar

Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

### 5. [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](http://arxiv.org/pdf/2510.17614v1)

Authors: Praphul Singh, Corey Barrett, Sumana Srivasta, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi

Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

### 6. [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](http://arxiv.org/pdf/2510.17281v1)

Authors: Qingyao Ai, Yichen Tang, Changyue Wang, Jianming Long, Weihang Su, Yiqun Liu

Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

### 7. [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](http://arxiv.org/pdf/2510.17354v1)

Authors: Chenghao Zhang, Guanting Dong, Xinyu Yang, Zhicheng Dou

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

### 8. [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](http://arxiv.org/pdf/2510.17670v1)

Authors: Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel

Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

### Machine Learning

### 1. [Consistent Zero-Shot Imitation with Contrastive Goal Inference](http://arxiv.org/pdf/2510.17059v1)

Authors: Kathryn Wantlin, Chongyi Zheng, Benjamin Eysenbach

In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

### 2. [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](http://arxiv.org/pdf/2510.17106v1)

Authors: Chen Zhang, Weixin Bu, Wendong Xu, Runsheng Yu, Yik-Chung Wu, Ngai Wong

Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

### 3. [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](http://arxiv.org/pdf/2510.17136v1)

Authors: Enhao Gu, Haolin Hou

The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

### 4. [Learning After Model Deployment](http://arxiv.org/pdf/2510.17160v1)

Authors: Derda Kaymak, Gyuhak Kim, Tomoya Kaichi, Tatsuya Konishi, Bing Liu

In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

### 5. [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](http://arxiv.org/pdf/2510.17162v1)

Authors: Guanjie Cheng, Siyang Liu, Junqin Huang, Xinkui Zhao, Yin Wang, Mengying Zhu, Linghe Kong, Shuiguang Deng

Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

### 6. [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](http://arxiv.org/pdf/2510.17185v1)

Authors: Runlin Lei, Lu Yi, Mingguo He, Pengyu Qiu, Zhewei Wei, Yongchao Liu, Chuntao Hong

While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

### 7. [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](http://arxiv.org/pdf/2510.17250v1)

Authors: Wei-Hsun Lee, Che-Yu Chang, Kuang-Yu Li

Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

### 8. [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](http://arxiv.org/pdf/2510.17313v1)

Authors: Tal Barami, Nimrod Berman, Ilan Naiman, Amos H. Hason, Rotem Ezra, Omri Azencot

Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

### 9. [Model Metamers Reveal Invariances in Graph Neural Networks](http://arxiv.org/pdf/2510.17378v1)

Authors: Wei Xu, Xiaoyi Jiang, Lixiang Xu, Dechao Tang

In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

### 10. [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](http://arxiv.org/pdf/2510.17381v1)

Authors: Achref Jaziri, Martin Rogmann, Martin Mundt, Visvanathan Ramesh

Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

### Neural and Evolutionary Computing

### 1. [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](http://arxiv.org/pdf/2510.17392v1)

Authors: Sonu Kumar, Arjun S. Nair, Bhawna Chaudhary, Mukul Lokhande, Santosh Kumar Vishvakarma

We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.

### 2. [A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications](http://arxiv.org/pdf/2510.17745v1)

Authors: Lars Niedermeier, Vyom Shah, Jeffrey L. Krichmar

Spiking Neural Networks (SNNs) have sparse, event driven processing that can
leverage neuromorphic applications. In this work, we introduce a
multi-threading kernel that enables neuromorphic applications running at the
edge, meaning they process sensory input directly and without any up-link to or
dependency on a cloud service. The kernel shows speed-up gains over single
thread processing by a factor of four on moderately sized SNNs and 1.7X on a
Synfire network. Furthermore, it load-balances all cores available on
multi-core processors, such as ARM, which run today's mobile devices and is up
to 70% more energy efficient compared to statical core assignment. The present
work can enable the development of edge applications that have low Size,
Weight, and Power (SWaP), and can prototype the integration of neuromorphic
chips.

### Networking and Internet Architecture

### 1. [Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking](http://arxiv.org/pdf/2510.17147v1)

Authors: Linhan Xia, Mingzhan Yang, Jingjing Wang, Ziwei Yan, Yakun Ren, Guo Yu, Kai Lei

Transformer-based large language models (LLMs) are increasingly being adopted
in networking research to address domain-specific challenges. However, their
quadratic time complexity and substantial model sizes often result in
significant computational overhead and memory constraints, particularly in
resource-constrained environments. Drawing inspiration from the efficiency and
performance of the Deepseek-R1 model within the knowledge distillation
paradigm, this paper introduces Mamba4Net, a novel cross-architecture
distillation framework. Mamba4Net transfers networking-specific knowledge from
transformer-based LLMs to student models built on the Mamba architecture, which
features linear time complexity. This design substantially enhances
computational efficiency compared to the quadratic complexity of
transformer-based models, while the reduced model size further minimizes
computational demands, improving overall performance and resource utilization.
To evaluate its effectiveness, Mamba4Net was tested across three diverse
networking tasks: viewport prediction, adaptive bitrate streaming, and cluster
job scheduling. Compared to existing methods that do not leverage LLMs,
Mamba4Net demonstrates superior task performance. Furthermore, relative to
direct applications of transformer-based LLMs, it achieves significant
efficiency gains, including a throughput 3.96 times higher and a storage
footprint of only 5.48% of that required by previous LLM-based approaches.
These results highlight Mamba4Net's potential to enable the cost-effective
application of LLM-derived knowledge in networking contexts. The source code is
openly available to support further research and development.

### 2. [AoA Services in 5G Networks: A Framework for Real-World Implementation and Systematic Testing](http://arxiv.org/pdf/2510.17342v1)

Authors: Alberto Ceresoli, Viola Bernazzoli, Roberto Pegurri, Ilario Filippini

Accurate positioning is a key enabler for emerging 5G applications. While the
standardized Location Management Function (LMF) operates centrally within the
core network, its scalability and latency limitations hinder low-latency and
fine-grained localization. A practical alternative is to shift positioning
intelligence toward the radio access network (RAN), where uplink sounding
reference signal (SRS)-based angle-of-arrival (AoA) estimation offers a
lightweight, network-native solution. In this work, we present the first fully
open-source 5G testbed for AoA estimation, enabling systematic and repeatable
experimentation under realistic yet controllable channel conditions. The
framework integrates the NVIDIA Sionna RT with a Keysight PROPSIM channel
emulator and includes a novel phase calibration procedure for USRP N310
devices. Experimental results show sub-degree to few-degree accuracy,
validating the feasibility of lightweight, single-anchor, network-native
localization within next-generation 5G systems.

### 3. [Enhancing 5G V2X Mode 2 for Sporadic Traffic](http://arxiv.org/pdf/2510.17395v1)

Authors: Dmitry Bankov, Artem Krasilov, Artem Otmakhov, Aleksei Shashin, Evgeny Khorov

The emerging road safety and autonomous vehicle applications require timely
and reliable data delivery between vehicles and between vehicles and
infrastructure. To satisfy this demand, 3GPP develops a 5G
Vehicle-to-Everything (V2X) technology. Depending on the served traffic type,
5G V2X specifications propose two channel access methods: (i) Mode 1, according
to which a base station allocates resources to users, and (ii) Mode 2,
according to which users autonomously select resources for their transmissions.
In the paper, we consider a scenario with sporadic traffic, e.g., a vehicle
generates a packet at a random time moment when it detects a dangerous
situation, which imposes strict requirements on delay and reliability. To
satisfy strict delay requirements, vehicles use Mode 2. We analyze the
performance of Mode 2 for sporadic traffic and propose several approaches to
improve it. Simulation results show that the proposed approaches can increase
the system capacity by up to 40% with a low impact on complexity.

### 4. [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](http://arxiv.org/pdf/2510.17410v1)

Authors: Dmitry Bankov, Artem Krasilov, Artem Otmakhov, Pavel Savlukovich, Evgeny Khorov

5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

### 5. [Pointing-Error-Induced Fading in an Open-Loop THz Uplink with Hardware Impairments](http://arxiv.org/pdf/2510.17647v1)

Authors: P. Brach del Prever, P. Testolina, A. Masihi, S. Petrushkevich, M. Polese, T. Melodia, J. M. Jornet

We analyze the open-loop mechanical tracking performance of a sub-Terahertz
(sub-THz) and Terahertz (THz) uplink communication system. These high-frequency
bands enable multi-gigabit links through large bandwidths and narrow beams, but
require precise pointing to overcome spreading loss. A tracking system can be
used to orient horn antennas toward mobile targets. We develop a mathematical
model that captures the mechanical dynamics of a real tracking system, which
includes motion latency and acceleration and velocity limits, to quantify
pointing errors during satellite passes and integrate these effects into the
link budget. We evaluate the trade-offs between beam directionality and
pointing tolerance across different Low Earth Orbit (LEO) satellite
trajectories and control strategies. The results link the hardware limitations
to the communications performance, providing design guidelines for
high-frequency Non-Terrestrial Network (NTN) uplink under practical mechanical
constraints.

### 6. [Adaptive Local Combining with Decentralized Decoding for Distributed Massive MIMO](http://arxiv.org/pdf/2510.17445v1)

Authors: Mohd Saif Ali Khan, Karthik RM, Samar Agnihotri

A major bottleneck in uplink distributed massive multiple-input
multiple-output networks is the sub-optimal performance of local combining
schemes, coupled with high fronthaul load and computational cost inherent in
centralized large scale fading decoding (LSFD) architectures. This paper
introduces a decentralized decoding architecture that fundamentally breaks from
the conventional LSFD, by allowing each AP calculates interference-suppressing
local weights independently and applies them to its data estimates before
transmission. Furthermore, two generalized local zero-forcing (ZF) framework,
generalized partial full-pilot ZF (G-PFZF) and generalized protected weak PFZF
(G-PWPFZF), are introduced, where each access point (AP) adaptively and
independently determines its combining strategy through a local sum spectral
efficiency optimization that classifies user equipments (UEs) as strong or weak
using only local information, eliminating the fixed thresholds used in PFZF and
PWPFZF. To further enhance scalability, pilot-dependent combining vectors
instead of user-dependent ones are introduced and are shared among users with
the same pilot. The corresponding closed-form spectral efficiency expressions
are derived. Numerical results show that the proposed generalized schemes
consistently outperform fixed-threshold counterparts, while the introduction of
local weights yields lower overhead and computation costs with minimal
performance penalty compared to them.

### Robotics

### 1. [Learning to Design Soft Hands using Reward Models](http://arxiv.org/pdf/2510.17086v1)

Authors: Xueqian Bai, Nicklas Hansen, Adabhav Singh, Michael T. Tolley, Yan Duan, Pieter Abbeel, Xiaolong Wang, Sha Yi

Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

### 2. [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](http://arxiv.org/pdf/2510.17143v1)

Authors: Shantnav Agarwal, Javier Alonso-Mora, Sihao Sun

Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

### 3. [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](http://arxiv.org/pdf/2510.17150v1)

Authors: Heng Zhang, Wei-Hsing Huang, Gokhan Solak, Arash Ajoudani

We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

### 4. [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](http://arxiv.org/pdf/2510.17203v1)

Authors: Ryota Soga, Masataka Kobayashi, Tsukasa Shimizu, Shintaro Shiba, Quan Kong, Shan Lu, Takaya Yamazato

Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

### 5. [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](http://arxiv.org/pdf/2510.17237v1)

Authors: Wuhao Xie, Kanji Tanaka

Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

### 6. [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](http://arxiv.org/pdf/2510.17249v1)

Authors: Franek Stark, Rohit Kumar, Shubham Vyas, Hannah Isermann, Jonas Haack, Mihaela Popescu, Jakob Middelberg, Dennis Mronga, Frank Kirchner

Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

### 7. [Implicit State Estimation via Video Replanning](http://arxiv.org/pdf/2510.17315v1)

Authors: Po-Chen Ko, Jiayuan Mao, Yu-Hsiang Fu, Hsien-Jeng Yeh, Chu-Rong Chen, Wei-Chiu Ma, Yilun Du, Shao-Hua Sun

Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

### 8. [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](http://arxiv.org/pdf/2510.17335v1)

Authors: Xintong Yang, Minglun Wei, Ze Ji, Yu-Kun Lai

Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

### 9. [Interactive Force-Impedance Control](http://arxiv.org/pdf/2510.17341v1)

Authors: Fan Shao, Satoshi Endo, Sandra Hirche, Fanny Ficuciello

Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

### 10. [HumanMPC - Safe and Efficient MAV Navigation among Humans](http://arxiv.org/pdf/2510.17525v1)

Authors: Simon Schaefer, Helen Oleynikova, Sandra Hirche, Stefan Leutenegger

Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

### Software Engineering

### 1. [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](http://arxiv.org/pdf/2510.17110v1)

Authors: Xiaoyu Guo, Shinobu Saito, Jianjun Zhao

With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

### 2. [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](http://arxiv.org/pdf/2510.17130v1)

Authors: Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Michael R. Lyu

Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

### 3. [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](http://arxiv.org/pdf/2510.17142v1)

Authors: Xiaoxue Ren, Jun Wan, Yun Peng, Zhongxin Liu, Ming Liang, Dajun Chen, Wei Jiang, Yong Li

Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

### 4. [Software Testing with Large Language Models: An Interview Study with Practitioners](http://arxiv.org/pdf/2510.17164v1)

Authors: Maria Deolinda Santana, Cleyton Magalhaes, Ronnie de Souza Santos

\textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

### 5. [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](http://arxiv.org/pdf/2510.17184v1)

Authors: Nicolas Robert, Fabien Gandon, Maxime Lefrançois

Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

### 6. [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](http://arxiv.org/pdf/2510.17376v1)

Authors: Yongmin Li, Jia Li, Ge Li, Zhi Jin

Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

### 7. [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](http://arxiv.org/pdf/2510.17430v1)

Authors: Kuniaki Kudo, Sherine Devi

We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

### 8. [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](http://arxiv.org/pdf/2510.17163v1)

Authors: Shuzheng Gao, Eric John Li, Man Ho Lam, Jingyu Xiao, Yuxuan Wan, Chaozheng Wang, Ng Man Tik, Michael R. Lyu

Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

### 9. [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](http://arxiv.org/pdf/2510.17158v1)

Authors: Daniel Nichols, Konstantinos Parasyris, Charles Jekel, Abhinav Bhatele, Harshitha Menon

Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

### 10. [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](http://arxiv.org/pdf/2510.17591v1)

Authors: Guang Yang, Yujie Zhu

Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

### Social and Information Networks

### 1. [HyperSearch: Prediction of New Hyperedges through Unconstrained yet Efficient Search](http://arxiv.org/pdf/2510.17153v1)

Authors: Hyunjin Choo, Fanchen Bu, Hyunjin Hwang, Young-Gyu Yoon, Kijung Shin

Higher-order interactions (HOIs) in complex systems, such as scientific
collaborations, multi-protein complexes, and multi-user communications, are
commonly modeled as hypergraphs, where each hyperedge (i.e., a subset of nodes)
represents an HOI among the nodes. Given a hypergraph, hyperedge prediction
aims to identify hyperedges that are either missing or likely to form in the
future, and it has broad applications, including recommending interest-based
social groups, predicting collaborations, and uncovering functional complexes
in biological systems. However, the vast search space of hyperedge candidates
(i.e., all possible subsets of nodes) poses a significant computational
challenge, making naive exhaustive search infeasible. As a result, existing
approaches rely on either heuristic sampling to obtain constrained candidate
sets or ungrounded assumptions on hypergraph structure to select promising
hyperedges.
  In this work, we propose HyperSearch, a search-based algorithm for hyperedge
prediction that efficiently evaluates unconstrained candidate sets, by
incorporating two key components: (1) an empirically grounded scoring function
derived from observations in real-world hypergraphs and (2) an efficient search
mechanism, where we derive and use an anti-monotonic upper bound of the
original scoring function (which is not antimonotonic) to prune the search
space. This pruning comes with theoretical guarantees, ensuring that discarded
candidates are never better than the kept ones w.r.t. the original scoring
function. In extensive experiments on 10 real-world hypergraphs across five
domains, HyperSearch consistently outperforms state-of-the-art baselines,
achieving higher accuracy in predicting new (i.e., not in the training set)
hyperedges.

### 2. [Opinion Maximization in Social Networks by Modifying Internal Opinions](http://arxiv.org/pdf/2510.17226v1)

Authors: Gengyu Wang, Runze Zhang, Zhongzhi Zhang

Public opinion governance in social networks is critical for public health
campaigns, political elections, and commercial marketing. In this paper, we
addresse the problem of maximizing overall opinion in social networks by
strategically modifying the internal opinions of key nodes. Traditional matrix
inversion methods suffer from prohibitively high computational costs, prompting
us to propose two efficient sampling-based algorithms. Furthermore, we develop
a deterministic asynchronous algorithm that exactly identifies the optimal set
of nodes through asynchronous update operations and progressive refinement,
ensuring both efficiency and precision. Extensive experiments on real-world
datasets demonstrate that our methods outperform baseline approaches. Notably,
our asynchronous algorithm delivers exceptional efficiency and accuracy across
all scenarios, even in networks with tens of millions of nodes.

### Systems and Control

### 1. [Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents](http://arxiv.org/pdf/2510.17129v1)

Authors: Wenbing Tang, Meilin Zhu, Fenghua Wu, Yang Liu

Recent advancements in Large Language Models (LLMs) have greatly enhanced
natural language understanding and content generation. However, these models
primarily operate in disembodied digital environments and lack interaction with
the physical world. To address this limitation, Embodied Artificial
Intelligence (EAI) has emerged, focusing on agents that can perceive and
interact with their surroundings. Despite progress, current embodied agents
face challenges in unstructured real-world environments due to insufficient
semantic intelligence, which is critical for understanding and reasoning about
complex tasks. This paper introduces the Semantic Intelligence-Driven Embodied
(SIDE) agent framework, which integrates a hierarchical semantic cognition
architecture with a semantic-driven decision-making process. This enables
agents to reason about and interact with the physical world in a contextually
adaptive manner. The framework is inspired by biological cognitive mechanisms
and utilizes bio-inspired principles to design a semantic cognitive
architecture that mimics how humans and animals integrate and process sensory
information. We present this framework as a step toward developing more
intelligent and versatile embodied agents.

### 2. [A Data-Driven Framework for Online Mitigation of False Data Injection Signals in Networked Control Systems](http://arxiv.org/pdf/2510.17155v1)

Authors: Mohammadamin Lari

This paper introduces a novel two-stage framework for online mitigation of
False Data Injection (FDI) signals to improve the resiliency of Networked
Control Systems (NCSs) and ensure their safe operation in the presence of
malicious activities. The first stage involves meta learning to select a base
time series forecasting model within a stacked ensemble learning architecture.
This is achieved by converting time series data into scalograms using
continuous wavelet transform, which are then split into image frames to
generate a scalo-temporal representation of the data and to distinguish between
different complexity levels of time series data based on an entropy metric
using a convolutional neural network. In the second stage, the selected model
mitigates false data injection signals in real-time. The proposed framework's
effectiveness is demonstrated through rigorous simulations involving the
formation control of differential drive mobile robots. By addressing the
security challenges in NCSs, this framework offers a promising approach to
maintaining system integrity and ensuring operational safety.

### 3. [Generalized Group Selection Strategies for Self-sustainable RIS-aided Communication](http://arxiv.org/pdf/2510.17176v1)

Authors: Lakshmikanta Sau, Priyadarshi Mukherjee, Sasthi C. Ghosh

Reconfigurable intelligent surface (RIS) is a cutting-edge communication
technology that has been proposed as aviable option for beyond fifth-generation
wireless communication networks. This paper investigates various group
selection strategies in the context of grouping-based self-sustainable
RIS-aided device-to-device (D2D) communication with spatially correlated
wireless channels. Specifically, we consider both power splitting (PS) and time
switching (TS) configurations, of the self-sustainable RIS to analyze the
system performance and propose appropriate bounds on the choice of system
parameters. The analysis takes into account a simplified linear energy
harvesting (EH) model as well as a practical non-linear EH model. Based on the
application requirements, we propose various group selection strategies at the
RIS. Notably, each strategy schedules the k-th best available group at the RIS
based on the end-to-end signal-to-noise ratio (SNR) and also the energy
harvested at a particular group of the RIS. Accordingly, by using tools from
high order statistics, we derive analytical expressions for the outage
probability of each selection strategy. Moreover, by applying the tools from
extreme value theory, we also investigate an asymptotic scenario, where the
number of groups available for selection at an RIS approaches infinity. The
nontrivial insights obtained from this approach is especially beneficial in
applications like large intelligent surface-aided wireless communication.
Finally, the numerical results demonstrate the importance and benefits of the
proposed approaches in terms of metrics such as the data throughput and the
outage (both data and energy) performance.

### 4. [Enhanced Ground-Satellite Direct Access via Onboard Rydberg Atomic Quantum Receivers](http://arxiv.org/pdf/2510.17290v1)

Authors: Qihao Peng, Tierui Gong, Zihang Song, Qu Luo, Zihuai Lin, Pei Xiao, Chau Yuen

Ground-satellite links for 6G networks face critical challenges, including
severe path loss, tight size-weight-power limits, and congested spectrum, all
of which significantly hinder the performance of traditional radio frequency
(RF) front ends. This article introduces the Rydberg Atomic Quantum Receiver
(RAQR) for onboard satellite systems, a millimeter-scale front end that
converts radio fields to optical signals through atomic electromagnetically
induced transparency. RAQR's high sensitivity and high frequency selectivity
address link budget, payload, and interference challenges while fitting within
space constraints. A hybrid atomic-electronic design and supporting signal
model demonstrate enhanced data rate, coverage, and sensing accuracy relative
to conventional RF receivers. The article concludes with integration
strategies, distributed-satellite concepts, and open research problems for
bringing RAQR-enabled satellite payloads into service.

### 5. [Artificial magnetic conductor backed dual-mode sectoral cylindrical DRA for off-body biomedical telemetry](http://arxiv.org/pdf/2510.17619v1)

Authors: Nayab Gogosh, Sohail Khalid, Bilal Tariq Malik, Slawomir Koziel

This research investigates the potential of a sectoral Cylindrical Dielectric
Resonator Antenna (CDRA) for biomedical telemetry. CDRAs are known for their
low loss, ruggedness, and stability, but their limited bandwidth and size make
them unsuitable for wearable devices. The research addresses these limitations
by proposing a dual mode antenna that operates in EH110 and TE210 modes. The
sectoral CDRA is a quarter segment with Perfect Electric Conductor boundaries,
reducing its size by a factor of four. Mathematical derivations of the field
components for both modes are derived to support the design. To minimize
specific absorption rate (SAR), an Artificial Magnetic Conductor (AMC) surface
is applied to the antennas backside, enhancing compatibility with the
transverse electric modes. The antenna achieves a bandwidth of 0.7 GHz (5.2-5.9
GHz), suitable for biomedical applications, with a measured peak gain of 7.9
dBi and a SAR of 1.24 W/kg when applied to a human arm.

### 6. [Trajectory Optimization for Minimum Threat Exposure using Physics-Informed Neural Networks](http://arxiv.org/pdf/2510.17762v1)

Authors: Alexandra E. Ballentine, Raghvendra V. Cowlagi

We apply a physics-informed neural network (PINN) to solve the two-point
boundary value problem (BVP) arising from the necessary conditions postulated
by Pontryagin's Minimum Principle for optimal control. Such BVPs are known to
be numerically difficult to solve by traditional shooting methods due to
extremely high sensitivity to initial guesses. In the light of recent successes
in applying PINNs for solving high-dimensional differential equations, we
develop a PINN to solve the problem of finding trajectories with minimum
exposure to a spatiotemporal threat for a vehicle kinematic model. First, we
implement PINNs that are trained to solve the BVP for a given pair of initial
and final states for a given threat field. Next, we implement a PINN
conditioned on the initial state for a given threat field, which eliminates the
need for retraining for each initial state. We demonstrate that the PINN
outputs satisfy the necessary conditions with low numerical error.

### 7. [Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters](http://arxiv.org/pdf/2510.17769v1)

Authors: Michael Nestor, Jiaxin Wang, Ning Zhang, Fei Teng

The increasing penetration of inverter-based resources into the power grid,
with often only black-box models available, challenges long-standing frequency
control methods. Most recent works take a decentralized approach without online
device coordination via communication. This paper considers both dynamic
behavior and communication within secondary frequency control on an
intermediate timescale. We develop a distributed data-driven approach that
utilizes peer-to-peer communication between inverters to avoid the need for a
central control center. To enable a trade off between communication network
requirements and control performance, we present a framework to guide
communication topology design for secondary frequency regulation. Following
design of the inter-agent information exchange scheme, we design a controller
that is structured according to the communication topology with a closed-loop
stability guarantee. Case studies on the IEEE 39-bus system validate the
framework and illustrate the trade-off between communication requirements and
control performance that is enabled by our approach.

### 8. [Differentiating Through Power Flow Solutions for Admittance and Topology Control](http://arxiv.org/pdf/2510.17071v1)

Authors: Samuel Talkington, Daniel Turizo, Sergio A. Dorado-Rojas, Rahul K. Gupta, Daniel K. Molzahn

The power flow equations relate bus voltage phasors to power injections via
the network admittance matrix. These equations are central to the key
operational and protection functions of power systems (e.g., optimal power flow
scheduling and control, state estimation, protection, and fault location, among
others). As control, optimization, and estimation of network admittance
parameters are central to multiple avenues of research in electric power
systems, we propose a linearization of power flow solutions obtained by
implicitly differentiating them with respect to the network admittance
parameters. This is achieved by utilizing the implicit function theorem, in
which we show that such a differentiation is guaranteed to exist under mild
conditions and is applicable to generic power systems (radial or meshed). The
proposed theory is applied to derive sensitivities of complex voltages, line
currents, and power flows. The developed theory of linearizing the power flow
equations around changes in the complex network admittance parameters has
numerous applications. We demonstrate several of these applications, such as
predicting the nodal voltages when the network topology changes without solving
the power flow equations. We showcase the application for continuous admittance
control, which is used to increase the hosting capacity of a given distribution
network.

### 9. [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](http://arxiv.org/pdf/2510.17087v1)

Authors: Ziqing Zhu

Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

### 10. [Floating-Base Deep Lagrangian Networks](http://arxiv.org/pdf/2510.17270v1)

Authors: Lucas Schulze, Juliano Decico Negri, Victor Barasuol, Vivian Suzano Medeiros, Marcelo Becker, Jan Peters, Oleg Arenz

Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

### Machine Learning (Statistics Category)

### 1. [Mode Collapse of Mean-Field Variational Inference](http://arxiv.org/pdf/2510.17063v1)

Authors: Shunan Sheng, Bohan Wu, Alberto González-Sanz

Mean-field variational inference (MFVI) is a widely used method for
approximating high-dimensional probability distributions by product measures.
It has been empirically observed that MFVI optimizers often suffer from mode
collapse. Specifically, when the target measure $\pi$ is a mixture $\pi = w P_0
+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a
single component of the mixture. This work provides the first theoretical
explanation of mode collapse in MFVI. We introduce the notion to capture the
separatedness of the two mixture components -- called
$\varepsilon$-separateness -- and derive explicit bounds on the fraction of
mass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are
$\varepsilon$-separated for sufficiently small $\varepsilon$. Our results
suggest that the occurrence of mode collapse crucially depends on the relative
position of the components. To address this issue, we propose the rotational
variational inference (RoVI), which augments MFVI with a rotation matrix. The
numerical studies support our theoretical findings and demonstrate the benefits
of RoVI.

### 2. [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](http://arxiv.org/pdf/2510.17103v1)

Authors: Shinji Ito, Kevin Jamieson, Haipeng Luo, Arnab Maiti, Taira Tsuchiya

We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

### 3. [Adaptive Discretization for Consistency Models](http://arxiv.org/pdf/2510.17266v1)

Authors: Jiayu Bai, Zhanbo Feng, Zhijie Deng, Tianqi Hou, Robert C. Qiu, Zenan Ling

Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

### 4. [Uncertainty-aware data assimilation through variational inference](http://arxiv.org/pdf/2510.17268v1)

Authors: Anthony Frion, David S Greenberg

Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

### 5. [Symmetries in PAC-Bayesian Learning](http://arxiv.org/pdf/2510.17303v1)

Authors: Armin Beck, Peter Ochs

Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

### 6. [Optimal Best Arm Identification under Differential Privacy](http://arxiv.org/pdf/2510.17348v1)

Authors: Marc Jourdan, Achraf Azize

Best Arm Identification (BAI) algorithms are deployed in data-sensitive
applications, such as adaptive clinical trials or user studies. Driven by the
privacy concerns of these applications, we study the problem of
fixed-confidence BAI under global Differential Privacy (DP) for Bernoulli
distributions. While numerous asymptotically optimal BAI algorithms exist in
the non-private setting, a significant gap remains between the best lower and
upper bounds in the global DP setting. This work reduces this gap to a small
multiplicative constant, for any privacy budget $\epsilon$. First, we provide a
tighter lower bound on the expected sample complexity of any $\delta$-correct
and $\epsilon$-global DP strategy. Our lower bound replaces the
Kullback-Leibler (KL) divergence in the transportation cost used by the
non-private characteristic time with a new information-theoretic quantity that
optimally trades off between the KL divergence and the Total Variation distance
scaled by $\epsilon$. Second, we introduce a stopping rule based on these
transportation costs and a private estimator of the means computed using an
arm-dependent geometric batching. En route to proving the correctness of our
stopping rule, we derive concentration results of independent interest for the
Laplace distribution and for the sum of Bernoulli and Laplace distributions.
Third, we propose a Top Two sampling rule based on these transportation costs.
For any budget $\epsilon$, we show an asymptotic upper bound on its expected
sample complexity that matches our lower bound to a multiplicative constant
smaller than $8$. Our algorithm outperforms existing $\delta$-correct and
$\epsilon$-global DP BAI algorithms for different values of $\epsilon$.

### 7. [Exploration via Feature Perturbation in Contextual Bandits](http://arxiv.org/pdf/2510.17390v1)

Authors: Seouh-won Yi, Min-hwan Oh

We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

### 8. [Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs](http://arxiv.org/pdf/2510.17472v1)

Authors: Paula Cordero-Encinar, Andrew B. Duncan

Recent advances such as self-consistency and test-time reinforcement learning
(TTRL) improve the reliability of large language models (LLMs) without
additional supervision, yet their underlying mechanisms and statistical
guarantees remain poorly understood. We present a unified framework for
certifiable inference in LLMs, showing that majority voting provides a
statistical certificate of self-consistency: under mild assumptions, the
aggregated answer coincides with the mode of the model's terminal distribution
with high probability. We derive finite-sample and anytime-valid concentration
bounds that quantify this confidence, and introduce the Martingale Majority
Certificate (MMC), a sequential stopping rule that adaptively determines when
sufficient samples have been drawn. We further prove that label-free
post-training methods such as TTRL implicitly sharpen the answer distribution
by exponentially tilting it toward its mode, thereby reducing the number of
samples required for certification. Building on this insight, we propose new
post-training objectives that explicitly optimise this trade-off between
sharpness and bias. Together, these results explain and connect two central
test-time scaling strategies, self-consistency and TTRL, within a single
statistical framework for label-free, certifiable reliability in reasoning
LLMs.

### 9. [Functional Distribution Networks (FDN)](http://arxiv.org/pdf/2510.17794v1)

Authors: Omer Haq

Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

### 10. [DFNN: A Deep Fréchet Neural Network Framework for Learning Metric-Space-Valued Responses](http://arxiv.org/pdf/2510.17072v1)

Authors: Kyum Kim, Yaqing Chen, Paromita Dubey

Regression with non-Euclidean responses -- e.g., probability distributions,
networks, symmetric positive-definite matrices, and compositions -- has become
increasingly important in modern applications. In this paper, we propose deep
Fr\'echet neural networks (DFNNs), an end-to-end deep learning framework for
predicting non-Euclidean responses -- which are considered as random objects in
a metric space -- from Euclidean predictors. Our method leverages the
representation-learning power of deep neural networks (DNNs) to the task of
approximating conditional Fr\'echet means of the response given the predictors,
the metric-space analogue of conditional expectations, by minimizing a
Fr\'echet risk. The framework is highly flexible, accommodating diverse metrics
and high-dimensional predictors. We establish a universal approximation theorem
for DFNNs, advancing the state-of-the-art of neural network approximation
theory to general metric-space-valued responses without making model
assumptions or relying on local smoothing. Empirical studies on synthetic
distributional and network-valued responses, as well as a real-world
application to predicting employment occupational compositions, demonstrate
that DFNNs consistently outperform existing methods.



---

# Nature Computer Science Reports

Collection of today's Computer Science research papers pulled from Nature Open Access Reports.

---

Pulled on 2025-10-21 PST.

### 1. [Text mining and machine learning reveal global determinants of food insecurity](https://www.nature.com/articles/s41598-025-20670-x)

Authors: Bia Carneiro et al.

### 2. [Isometric representations in neural networks improve robustness](https://www.nature.com/articles/s41598-025-20619-0)

Authors: Kosio Beshkov et al.

### 3. [Collaborative optimization of layout and cutting scheduling for large-scale customized metal structural parts](https://www.nature.com/articles/s41598-025-20522-8)

Authors: Ronghua Meng et al.

### 4. [Privacy preserving blockchain integrated explainable artificial intelligence with two tier optimization for cyber threat detection and mitigation in the internet of things](https://www.nature.com/articles/s41598-025-10601-1)

Authors: Manal Abdullah Alohali et al.

### 5. [Charging stations demand forecasting using LSTM based hybrid transformer model](https://www.nature.com/articles/s41598-025-20421-y)

Authors: Adil Hussain et al.

### 6. [Integrating deep learning and radiomics for preoperative glioma grading using multi-center MRI data](https://www.nature.com/articles/s41598-025-20711-5)

Authors: Shi Yin et al.

### 7. [Leveraging RegNet and CBAM for precise detection of honey adulteration using thermal image analysis](https://www.nature.com/articles/s41598-025-19815-9)

Authors: Boulbarj Ilias et al.

### 8. [CIRE: A Chinese EEG Dataset for decoding speech intention modulated by prosodic emotion](https://www.nature.com/articles/s41597-025-05957-y)

Authors: Shengrui He et al.

### 9. [Comparative performance evaluation of machine learning models for predicting the ultimate bearing capacity of shallow foundations on granular soils](https://www.nature.com/articles/s41598-025-13926-z)

Authors: Jalal Shah et al.

### 10. [Enhanced isolation unique fractal diminutive MIMO antenna with identical elliptical notch and cross stub for wireless uses](https://www.nature.com/articles/s41598-025-22587-x)

Authors: Nairaj Jat et al.

### 11. [Short term demand forecasting of electric vehicle charging stations using context aware temporal transformer model](https://www.nature.com/articles/s41598-025-20557-x)

Authors: Adil Hussain et al.

### 12. [Cloud-based DDoS detection using hybrid feature selection with deep reinforcement learning (DRL)](https://www.nature.com/articles/s41598-025-18857-3)

Authors: Suneeta Satpathy et al.

### 13. [A segmentation network for enhancing autonomous driving scene understanding using skip connection and adaptive weighting](https://www.nature.com/articles/s41598-025-20592-8)

Authors: Jiayao Li et al.

### 14. [Towards deployment-centric multimodal AI beyond vision and language](https://www.nature.com/articles/s42256-025-01116-5)

Authors: Xianyuan Liu et al.

