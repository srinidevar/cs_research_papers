# Computer Science arXiv Papers

Collection of top 10 Computer Science research papers pulled daily from arXiv.

---

Pulled on 2025-05-13 17:58:42.655471 PST.

### Artificial Intelligence

### 1. [Accountability of Generative AI: Exploring a Precautionary Approach for "Artificially Created Nature"](http://arxiv.org/pdf/2505.07178v1)

Authors: Yuri Nakao

The rapid development of generative artificial intelligence (AI) technologies
raises concerns about the accountability of sociotechnical systems. Current
generative AI systems rely on complex mechanisms that make it difficult for
even experts to fully trace the reasons behind the outputs. This paper first
examines existing research on AI transparency and accountability and argues
that transparency is not a sufficient condition for accountability but can
contribute to its improvement. We then discuss that if it is not possible to
make generative AI transparent, generative AI technology becomes ``artificially
created nature'' in a metaphorical sense, and suggest using the precautionary
principle approach to consider AI risks. Finally, we propose that a platform
for citizen participation is needed to address the risks of generative AI.

### 2. [Measuring General Intelligence with Generated Games](http://arxiv.org/pdf/2505.07215v1)

Authors: Vivek Verma, David Huang, William Chen, Dan Klein, Nicholas Tomlin

We present gg-bench, a collection of game environments designed to evaluate
general reasoning capabilities in language models. Unlike most static
benchmarks, gg-bench is a data generating process where new evaluation
instances can be generated at will. In particular, gg-bench is synthetically
generated by (1) using a large language model (LLM) to generate natural
language descriptions of novel games, (2) using the LLM to implement each game
in code as a Gym environment, and (3) training reinforcement learning (RL)
agents via self-play on the generated games. We evaluate language models by
their winrate against these RL agents by prompting models with the game
description, current board state, and a list of valid moves, after which models
output the moves they wish to take. gg-bench is challenging: state-of-the-art
LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench
using in-context learning, while reasoning models such as o1, o3-mini and
DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,
data generation process, and evaluation code in order to support future
modeling work and expansion of our benchmark.

### 3. [How well do LLMs reason over tabular data, really?](http://arxiv.org/pdf/2505.07453v1)

Authors: Cornelius Wolff, Madelon Hulsebos

Large Language Models (LLMs) excel in natural language tasks, but less is
known about their reasoning capabilities over tabular data. Prior analyses
devise evaluation strategies that poorly reflect an LLM's realistic performance
on tabular queries. Moreover, we have a limited understanding of the robustness
of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can
general-purpose LLMs reason over tabular data, really?, and focus on two
questions 1) are tabular reasoning capabilities of general-purpose LLMs robust
to real-world characteristics of tabular inputs, and 2) how can we
realistically evaluate an LLM's performance on analytical tabular queries?
Building on a recent tabular reasoning benchmark, we first surface shortcomings
of its multiple-choice prompt evaluation strategy, as well as commonly used
free-form text metrics such as SacreBleu and BERT-score. We show that an
LLM-as-a-judge procedure yields more reliable performance insights and unveil a
significant deficit in tabular reasoning performance of LLMs. We then extend
the tabular inputs reflecting three common characteristics in practice: 1)
missing values, 2) duplicate entities, and 3) structural variations.
Experiments show that the tabular reasoning capabilities of general-purpose
LLMs suffer from these variations, stressing the importance of improving their
robustness for realistic tabular inputs.

### 4. [Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks](http://arxiv.org/pdf/2505.07473v1)

Authors: Kai Xu, YiWei Mao, XinYi Guan, ZiLong Feng

The application of large language models (LLMs) in the field of coding is
evolving rapidly: from code assistants, to autonomous coding agents, and then
to generating complete projects through natural language. Early LLM code
benchmarks primarily focused on code generation accuracy, but these benchmarks
have gradually become saturated. Benchmark saturation weakens their guiding
role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.
Among various attempts to address benchmark saturation, approaches based on
software engineering have stood out, but the saturation of existing software
engineering benchmarks is rapidly increasing. To address this, we propose a new
benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks
with sequential dependencies. The tasks implement project features in sequence,
simulating real-world human development workflows. When designing Web-Bench, we
aim to cover the foundational elements of Web development: Web Standards and
Web Frameworks. Given the scale and complexity of these projects, which were
designed by engineers with 5 to 10 years of experience, each presents a
significant challenge. On average, a single project takes 4 to 8 hours for a
senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA
(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)
than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss
that in any development field, Standards and Frameworks represent foundational
knowledge and efficiency tools, respectively, and LLMs require optimization
tailored to them.

### 5. [HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs](http://arxiv.org/pdf/2505.07509v1)

Authors: Feng Ding, Tingting Wang, Yupeng Gao, Shuo Yu, Jing Ren, Feng Xia

Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the
expiration date of facts, which negatively impact reasoning performance on
TKGs. However, existing reasoning methods primarily focus on positive
importance of historical facts, neglecting adverse effects of outdated facts.
Besides, training on these outdated facts yields extra computational cost. To
address these challenges, we propose an outdated fact filtering framework named
HALO, which quantifies the temporal validity of historical facts by exploring
the half-life theory to filter outdated facts in TKGs. HALO consists of three
modules: the temporal fact attention module, the dynamic relation-aware encoder
module, and the outdated fact filtering module. Firstly, the temporal fact
attention module captures the evolution of historical facts over time to
identify relevant facts. Secondly, the dynamic relation-aware encoder module is
designed for efficiently predicting the half life of each fact. Finally, we
construct a time decay function based on the half-life theory to quantify the
temporal validity of facts and filter outdated facts. Experimental results show
that HALO outperforms the state-of-the-art TKG reasoning methods on three
public datasets, demonstrating its effectiveness in detecting and filtering
outdated facts (Codes are available at
https://github.com/yushuowiki/K-Half/tree/main ).

### 6. [QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads](http://arxiv.org/pdf/2505.07531v1)

Authors: Khurram Mazher, Saad Bin Nasir

We present QuantX: a tailored suite of recipes for LLM and VLM quantization.
It is capable of quantizing down to 3-bit resolutions with minimal loss in
performance. The quantization strategies in QuantX take into account
hardware-specific constraints to achieve efficient dequantization during
inference ensuring flexible trade-off between runtime speed, memory requirement
and model accuracy. Our results demonstrate that QuantX achieves performance
within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for
multiple end user tasks and outperforms recently published state-of-the-art
quantization techniques. This manuscript provides insights into the LLM
quantization process that motivated the range of recipes and options that are
incorporated in QuantX.

### 7. [Belief Injection for Epistemic Control in Linguistic State Space](http://arxiv.org/pdf/2505.07693v1)

Authors: Sebastian Dumbrava

This work introduces belief injection, a proactive epistemic control
mechanism for artificial agents whose cognitive states are structured as
dynamic ensembles of linguistic belief fragments. Grounded in the Semantic
Manifold framework, belief injection directly incorporates targeted linguistic
beliefs into an agent's internal cognitive state, influencing reasoning and
alignment proactively rather than reactively. We delineate various injection
strategies, such as direct, context-aware, goal-oriented, and reflective
approaches, and contrast belief injection with related epistemic control
mechanisms, notably belief filtering. Additionally, this work discusses
practical applications, implementation considerations, ethical implications,
and outlines promising directions for future research into cognitive governance
using architecturally embedded belief injection.

### 8. ["I Apologize For Not Understanding Your Policy": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants](http://arxiv.org/pdf/2505.07759v1)

Authors: Jennifer Mondragon, Carlos Rubio-Medrano, Gael Cruz, Dvijesh Shastri

The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants
(VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek
has turned them into convenient interfaces for managing emerging technologies
such as Smart Homes, Smart Cars, Electronic Health Records, by means of
explicit commands,e.g., prompts, which can be even launched via voice, thus
providing a very convenient interface for end-users. However, the proper
specification and evaluation of User-Managed Access Control Policies (U-MAPs),
the rules issued and managed by end-users to govern access to sensitive data
and device functionality - within these VAs presents significant challenges,
since such a process is crucial for preventing security vulnerabilities and
privacy leaks without impacting user experience. This study provides an initial
exploratory investigation on whether current publicly-available VAs can manage
U-MAPs effectively across differing scenarios. By conducting unstructured to
structured tests, we evaluated the comprehension of such VAs, revealing a lack
of understanding in varying U-MAP approaches. Our research not only identifies
key limitations, but offers valuable insights into how VAs can be further
improved to manage complex authorization rules and adapt to dynamic changes.

### 9. [Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving](http://arxiv.org/pdf/2505.07773v1)

Authors: Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang

Large Language Models (LLMs) often struggle with mathematical reasoning tasks
requiring precise, verifiable computation. While Reinforcement Learning (RL)
from outcome-based rewards enhances text-based reasoning, understanding how
agents autonomously learn to leverage external tools like code execution
remains crucial. We investigate RL from outcome-based rewards for
Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously
generate and execute Python code for mathematical problems without supervised
tool-use examples. Our central contribution is we demonstrate that as RL
training progresses, key metrics scale predictably. Specifically, we observe
strong positive correlations where increased training steps lead to increases
in the spontaneous code execution frequency, the average response length, and,
critically, the final task accuracy. This suggests a quantifiable relationship
between computational effort invested in training and the emergence of
effective, tool-augmented reasoning strategies. We implement a robust framework
featuring a decoupled code execution environment and validate our findings
across standard RL algorithms and frameworks. Experiments show ZeroTIR
significantly surpasses non-tool ZeroRL baselines on challenging math
benchmarks. Our findings provide a foundational understanding of how autonomous
tool use is acquired and scales within Agent RL, offering a reproducible
benchmark for future studies. Code is released at
\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.

### 10. [ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion](http://arxiv.org/pdf/2505.07171v1)

Authors: Jeongho Kim, Chanyeong Heo, Jaehee Jung

Knowledge Graphs (KGs), composed of triples in the form of (head, relation,
tail) and consisting of entities and relations, play a key role in information
retrieval systems such as question answering, entity search, and
recommendation. In real-world KGs, although many entities exist, the relations
exhibit a long-tail distribution, which can hinder information retrieval
performance. Previous few-shot knowledge graph completion studies focused
exclusively on the positive triple information that exists in the graph or,
when negative triples were incorporated, used them merely as a signal to
indicate incorrect triples. To overcome this limitation, we propose
Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,
negative triples are generated by randomly replacing the tail entity in the
support set. By conditionally incorporating positive information in the KG and
non-existent negative information into the diffusion process, the model
separately estimates the latent distributions for positive and negative
relations. Moreover, including an attention pooler enables the model to
leverage the differences between positive and negative cases explicitly.
Experiments on two widely used datasets demonstrate that our method outperforms
existing approaches, achieving state-of-the-art performance. The code is
available at https://github.com/hou27/ReCDAP-FKGC.

### Computational Complexity

### 1. [Cluster Vertex Deletion Problems on Cubic Graphs](http://arxiv.org/pdf/2505.07443v1)

Authors: Irena Rusu

The problems Cluster Vertex Deletion (or Cluster-VD) and its generalization
s-Club Cluster Vertex Deletion (or s-Club-VD, for any integer s>= 1), have been
introduced with the aim of detecting highly-connected parts in complex systems.
Their NP-completeness has been established for several classes of graphs, but
remains open for smaller classes, including subcubic planar bipartite graphs
and cubic graphs. In this paper, we show that Cluster-VD and more generally
s-Club-VD are NP-complete for cubic planar bipartite graphs. We also deduce new
results for the related k-Path Vertex Cover problem (or k-PVC), namely 3-PVC is
NP-complete for cubic planar bipartite graphs, whereas k-PVC with k>= 4 is
NP-complete for subcubic planar (and bipartite, when k is odd) graphs of
arbitrarily large girth.

### 2. [Undecidability of Polynomial Inequalities in Subset Densities and Additive Energies](http://arxiv.org/pdf/2505.07378v1)

Authors: Yaqiao Li

Many results in extremal graph theory can be formulated as certain polynomial
inequalities in graph homomorphism densities. Answering fundamental questions
raised by Lov{\'a}sz, Szegedy and Razborov, Hatami and Norine proved that
determining the validity of an arbitrary such polynomial inequality in graph
homomorphism densities is undecidable. We observe that many results in additive
combinatorics can also be formulated as polynomial inequalities in subset's
density and its variants. Based on techniques introduced in Hatami and Norine,
together with algebraic and graph construction and Fourier analysis, we prove
similarly two theorems of undecidability, thus showing that establishing such
polynomial inequalities in additive combinatorics are inherently difficult in
their full generality.

### 3. [Quon Classical Simulation: Unifying Clifford, Matchgates and Entanglement](http://arxiv.org/pdf/2505.07804v1)

Authors: Zixuan Feng, Zhengwei Liu, Fan Lu, Ningfeng Wang

We propose a unified classical simulation framework for quantum circuits,
termed Quon Classical Simulation (QCS), built upon the diagrammatic formalism
of the Quon language. Central to this framework is the introduction of magic
holes, a topological feature that captures the global source of computational
hardness in simulating quantum systems. Unlike conventional measures, the
complexity of QCS is governed by the topological entanglement entropy
associated with these magic holes. We show that Clifford circuits and Matchgate
circuits are free of magic holes and thus efficiently simulable within our
model. To capture the interaction structure of magic holes, we define a
topological tensor network representation and develop novel skein relations and
reduction algorithms to simplify circuit representations. This approach
significantly improves the efficiency of classical simulations and provides a
unified explanation for the tractability of various known quantum circuit
classes. Our work offers a new topological perspective on the classical
simulability of quantum systems and topological complexity.

### Computational Engineering

### 1. [A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms](http://arxiv.org/pdf/2505.07660v1)

Authors: Dieu-Donne Fangnon, Armandine Sorel Kouyim Meli, Verlon Roel Mbingui, Phanie Dianelle Negho, Regis Konan Marcel Djaha

Artificial intelligence (AI) has demonstrated remarkable success across
various applications. In light of this trend, the field of automated trading
has developed a keen interest in leveraging AI techniques to forecast the
future prices of financial assets. This interest stems from the need to address
trading challenges posed by the inherent volatility and dynamic nature of asset
prices. However, crafting a flawless strategy becomes a formidable task when
dealing with assets characterized by intricate and ever-changing price
dynamics. To surmount these formidable challenges, this research employs an
innovative rule-based strategy approach to train Deep Reinforcement Learning
(DRL). This application is carried out specifically in the context of trading
Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration
of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks,
alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an
optimal policy for our application. To evaluate the effectiveness of our Deep
Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the
trade signal as performance metrics. The experimental outcomes highlight that
Duelling and Double Deep Q-Network outperformed when using XRP with the
increasing of the portfolio wealth. All codes are available in this
\href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\color{blue}Github
link}.

### 2. [Simulating many-engine spacecraft: Exceeding 100 trillion grid points via information~geometric regularization and the MFC flow solver](http://arxiv.org/pdf/2505.07392v1)

Authors: Benjamin Wilfong, Anand Radhakrishnan, Henry Le Berre, Nikolaos Tselepidis, Benedikt Dorschner, Reuben Budiardja, Brian Cornille, Stephen Abbott, Florian Schäfer, Spencer H. Bryngelson

This work proposes a method and optimized implementation for exascale
simulations of high-speed compressible fluid flows, enabling the simulation of
multi-engine rocket craft at an unprecedented scale. We significantly improve
upon the state-of-the-art in terms of computational cost and memory footprint
through a carefully crafted implementation of the recently proposed information
geometric regularization, which eliminates the need for numerical shock
capturing. Unified addressing on tightly coupled CPU--GPU platforms increases
the total problem size with negligible performance hit. Despite linear stencil
algorithms being memory-bound, we achieve wall clock times that are four times
faster than optimized baseline numerics. This enables the execution of CFD
simulations at more than 100 trillion grid points, surpassing the largest
state-of-the-art publicly available simulations by an order of magnitude. Ideal
weak scaling is demonstrated on OLCF Frontier and CSCS Alps using the full
system, entailing 37.8K AMD MI250X GPUs (Frontier) or 9.2K NVIDIA GH200
superchips (Alps).

### Computation and Language

### 1. [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](http://arxiv.org/pdf/2505.07157v1)

Authors: Hajar Sakai, Sarah S. Lam

Traditional topic models often struggle with contextual nuances and fail to
adequately handle polysemy and rare words. This limitation typically results in
topics that lack coherence and quality. Large Language Models (LLMs) can
mitigate this issue by generating an initial set of topics. However, these raw
topics frequently lack refinement and representativeness, which leads to
redundancy without lexical similarity and reduced interpretability. This paper
introduces HAMLET, a graph-driven architecture for cross-lingual healthcare
topic modeling that uses LLMs. The proposed approach leverages neural-enhanced
semantic fusion to refine the embeddings of topics generated by the LLM.
Instead of relying solely on statistical co-occurrence or human interpretation
to extract topics from a document corpus, this method introduces a topic
embedding refinement that uses Bidirectional Encoder Representations from
Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a
hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for
embedding. The topic representations are further refined using a GNN, which
establishes connections between documents, topics, words, similar topics, and
similar words. A novel method is introduced to compute similarities.
Consequently, the topic embeddings are refined, and the top k topics are
extracted. Experiments were conducted using two healthcare datasets, one in
English and one in French, from which six sets were derived. The results
demonstrate the effectiveness of HAMLET.

### 2. [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](http://arxiv.org/pdf/2505.07162v1)

Authors: Hajar Sakai, Sarah S. Lam

The increasing volume of healthcare textual data requires computationally
efficient, yet highly accurate classification approaches able to handle the
nuanced and complex nature of medical terminology. This research presents
Knowledge Distillation for Healthcare Multi-Label Text Classification
(KDH-MLTC), a framework leveraging model compression and Large Language Models
(LLMs). The proposed approach addresses conventional healthcare Multi-Label
Text Classification (MLTC) challenges by integrating knowledge distillation and
sequential fine-tuning, subsequently optimized through Particle Swarm
Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from
a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,
DistilBERT) through sequential training adapted to MLTC that preserves the
teacher's learned information while significantly reducing computational
requirements. As a result, the classification is enabled to be conducted
locally, making it suitable for healthcare textual data characterized by
sensitivity and, therefore, ensuring HIPAA compliance. The experiments
conducted on three medical literature datasets of different sizes, sampled from
the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves
superior performance compared to existing approaches, particularly for the
largest dataset, reaching an F1 score of 82.70%. Additionally, statistical
validation and an ablation study are carried out, proving the robustness of
KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process
allowed the identification of optimal configurations. The proposed approach
contributes to healthcare text classification research, balancing efficiency
requirements in resource-constrained healthcare settings with satisfactory
accuracy demands.

### 3. [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](http://arxiv.org/pdf/2505.07184v1)

Authors: Yifan Wei, Xiaoyan Yu, Tengfei Pan, Angsheng Li, Li Du

Large language models (LLMs) have achieved unprecedented performance by
leveraging vast pretraining corpora, yet their performance remains suboptimal
in knowledge-intensive domains such as medicine and scientific research, where
high factual precision is required. While synthetic data provides a promising
avenue for augmenting domain knowledge, existing methods frequently generate
redundant samples that do not align with the model's true knowledge gaps. To
overcome this limitation, we propose a novel Structural Entropy-guided
Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge
deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to
quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree
Search (MCTS) to selectively explore regions where the model lacks
domain-specific knowledge. Guided by these insights, the framework generates
targeted synthetic data for supervised fine-tuning, enabling continuous
self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple
domain-specific benchmarks show that SENATOR effectively detects and repairs
knowledge deficiencies, achieving notable performance improvements. The code
and data for our methods and experiments are available at
https://github.com/weiyifan1023/senator.

### 4. [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](http://arxiv.org/pdf/2505.07205v1)

Authors: Mouxiao Bian, Rongzhao Zhang, Chao Ding, Xinwei Peng, Jie Xu

Large Language Models (LLMs) are poised to transform healthcare under China's
Healthy China 2030 initiative, yet they introduce new ethical and
patient-safety challenges. We present a novel 12,000-item Q&A benchmark
covering 11 ethics and 9 safety dimensions in medical contexts, to
quantitatively evaluate these risks. Using this dataset, we assess
state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing
moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant
improvements after fine-tuning on our data (up to 50.8% accuracy). Results show
notable gaps in LLM decision-making on ethics and safety scenarios, reflecting
insufficient institutional oversight. We then identify systemic governance
shortfalls-including the lack of fine-grained ethical audit protocols, slow
adaptation by hospital IRBs, and insufficient evaluation tools-that currently
hinder safe LLM deployment. Finally, we propose a practical governance
framework for healthcare institutions (embedding LLM auditing teams, enacting
data ethics guidelines, and implementing safety simulation pipelines) to
proactively manage LLM risks. Our study highlights the urgent need for robust
LLM governance in Chinese healthcare, aligning AI innovation with patient
safety and ethical standards.

### 5. [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](http://arxiv.org/pdf/2505.07293v1)

Authors: Kai Hua, Steven Wu, Ge Zhang, Ke Shen

Recently, there has been growing interest in collecting reasoning-intensive
pretraining data to improve LLMs' complex reasoning ability. Prior approaches
typically rely on supervised classifiers to identify such data, which requires
labeling by humans or LLMs, often introducing domain-specific biases. Due to
the attention heads being crucial to in-context reasoning, we propose
AttentionInfluence, a simple yet effective, training-free method without
supervision signal. Our approach enables a small pretrained language model to
act as a strong data selector through a simple attention head masking
operation. Specifically, we identify retrieval heads and compute the loss
difference when masking these heads. We apply AttentionInfluence to a
1.3B-parameter dense model to conduct data selection on the SmolLM corpus of
241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B
tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD
learning rate scheduling. Our experimental results demonstrate substantial
improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive
and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and
HumanEval). This demonstrates an effective weak-to-strong scaling property,
with small models improving the final performance of larger models-offering a
promising and scalable path for reasoning-centric data selection.

### 6. [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](http://arxiv.org/pdf/2505.07409v1)

Authors: Tim Wittenborg, Constantin Sebastian Tremel, Markus Stocker, Sören Auer

Democratic societies need reliable information. Misinformation in popular
media such as news articles or videos threatens to impair civic discourse.
Citizens are, unfortunately, not equipped to verify this content flood consumed
daily at increasing rates. This work aims to semi-automatically quantify
scientific accuracy of online media. By semantifying media of unknown veracity,
their statements can be compared against equally processed trusted sources. We
implemented a workflow using LLM-based statement extraction and knowledge graph
analysis. Our neurosymbolic system was able to evidently streamline
state-of-the-art veracity quantification. Evaluated via expert interviews and a
user survey, the tool provides a beneficial veracity indication. This
indicator, however, is unable to annotate public media at the required
granularity and scale. Further work towards a FAIR (Findable, Accessible,
Interoperable, Reusable) ground truth and complementary metrics are required to
scientifically support civic discourse.

### 7. [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](http://arxiv.org/pdf/2505.07416v1)

Authors: Truc Mai-Thanh Nguyen, Dat Minh Nguyen, Son T. Luu, Kiet Van Nguyen

Multimodal Review Helpfulness Prediction (MRHP) is an essential task in
recommender systems, particularly in E-commerce platforms. Determining the
helpfulness of user-generated reviews enhances user experience and improves
consumer decision-making. However, existing datasets focus predominantly on
English and Indonesian, resulting in a lack of linguistic diversity, especially
for low-resource languages such as Vietnamese. In this paper, we introduce
ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale
benchmark dataset for MRHP task in Vietnamese. This dataset covers four
domains, including 2K products with 46K reviews. Meanwhile, a large-scale
dataset requires considerable time and cost. To optimize the annotation
process, we leverage AI to assist annotators in constructing the ViMRHP
dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per
task down to 20 to 40 seconds per task) while maintaining data quality and
lowering overall costs by approximately 65%. However, AI-generated annotations
still have limitations in complex annotation tasks, which we further examine
through a detailed performance analysis. In our experiment on ViMRHP, we
evaluate baseline models on human-verified and AI-generated annotations to
assess their quality differences. The ViMRHP dataset is publicly available at
https://github.com/trng28/ViMRHP

### 8. [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](http://arxiv.org/pdf/2505.07440v1)

Authors: Rituraj Singh, Sachin Pawar, Girish Palshikar

Commonsense knowledge bases (KB) are a source of specialized knowledge that
is widely used to improve machine learning applications. However, even for a
large KB such as ConceptNet, capturing explicit knowledge from each industry
domain is challenging. For example, only a few samples of general {\em tasks}
performed by various industries are available in ConceptNet. Here, a task is a
well-defined knowledge-based volitional action to achieve a particular goal. In
this paper, we aim to fill this gap and present a weakly-supervised framework
to augment commonsense KB with tasks carried out by various industry groups
(IG). We attempt to {\em match} each task with one or more suitable IGs by
training a neural model to learn task-IG affinity and apply clustering to
select the top-k tasks per IG. We extract a total of 2339 triples of the form
$\langle IG, is~capable~of, task \rangle$ from two publicly available news
datasets for 24 IGs with the precision of 0.86. This validates the reliability
of the extracted task-IG pairs that can be directly added to existing KBs.

### 9. [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](http://arxiv.org/pdf/2505.07495v1)

Authors: Isabelle van der Vegt, Bennett Kleinberg, Marilu Miotto, Jonas Festor

This paper introduces and evaluates three translations of the Grievance
Dictionary, a psycholinguistic dictionary for the analysis of violent,
threatening or grievance-fuelled texts. Considering the relevance of these
themes in languages beyond English, we translated the Grievance Dictionary to
Dutch, German, and Italian. We describe the process of automated translation
supplemented by human annotation. Psychometric analyses are performed,
including internal reliability of dictionary categories and correlations with
the LIWC dictionary. The Dutch and German translations perform similarly to the
original English version, whereas the Italian dictionary shows low reliability
for some categories. Finally, we make suggestions for further validation and
application of the dictionary, as well as for future dictionary translations
following a similar approach.

### 10. [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](http://arxiv.org/pdf/2505.07528v1)

Authors: Lei Wang

Retrieval-Augmented Generation (RAG) models frequently encounter
hallucination phenomena when integrating external information with internal
parametric knowledge. Empirical studies demonstrate that the disequilibrium
between external contextual information and internal parametric knowledge
constitutes a primary factor in hallucination generation. Existing
hallucination detection methodologies predominantly emphasize either the
external or internal mechanism in isolation, thereby overlooking their
synergistic effects. The recently proposed ReDeEP framework decouples these
dual mechanisms, identifying two critical contributors to hallucinations:
excessive reliance on parametric knowledge encoded in feed-forward networks
(FFN) and insufficient utilization of external information by attention
mechanisms (particularly copy heads). ReDeEP quantitatively assesses these
factors to detect hallucinations and dynamically modulates the contributions of
FFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and
numerous other hallucination detection approaches have been employed at
logit-level uncertainty estimation or language-level self-consistency
evaluation, inadequately address the semantic dimensions of model responses,
resulting in inconsistent hallucination assessments in RAG implementations.
Building upon ReDeEP's foundation, this paper introduces SEReDeEP, which
enhances computational processes through semantic entropy captured via trained
linear probes, thereby achieving hallucination assessments that more accurately
reflect ground truth evaluations.

### Cryptography and Security

### 1. [Real-Time Bit-Level Encryption of Full High-Definition Video Without Diffusion](http://arxiv.org/pdf/2505.07158v1)

Authors: Dong Jiang, Hui-ran Luo, Zi-jian Cui, Xi-jue Zhao, Lin-sheng Huang, Liang-liang Lu

Despite the widespread adoption of Shannon's confusion-diffusion architecture
in image encryption, the implementation of diffusion to sequentially establish
inter-pixel dependencies for attaining plaintext sensitivity constrains
algorithmic parallelism, while the execution of multiple rounds of diffusion
operations to meet the required sensitivity metrics incurs excessive
computational overhead. Consequently, the pursuit of plaintext sensitivity
through diffusion operations is the primary factor limiting the computational
efficiency and throughput of video encryption algorithms, rendering them
inadequate to meet the demands of real-time encryption for high-resolution
video. To address the performance limitation, this paper proposes a real-time
video encryption protocol based on heterogeneous parallel computing, which
incorporates the SHA-256 hashes of original frames as input, employs multiple
CPU threads to concurrently generate encryption-related data, and deploys
numerous GPU threads to simultaneously encrypt pixels. By leveraging the
extreme input sensitivity of the SHA hash, the proposed protocol achieves the
required plaintext sensitivity metrics with only a single round of confusion
and XOR operations, significantly reducing computational overhead. Furthermore,
through eliminating the reliance on diffusion, it realizes the allocation of a
dedicated GPU thread for encrypting each pixel within every channel,
effectively enhancing algorithm's parallelism. The experimental results
demonstrate that our approach not only exhibits superior statistical properties
and robust security but also achieving delay-free bit-level encryption for
1920$\times$1080 resolution (full high definition) video at 30 FPS, with an
average encryption time of 25.84 ms on a server equipped with an Intel Xeon
Gold 6226R CPU and an NVIDIA GeForce RTX 3090 GPU.

### 2. [Security through the Eyes of AI: How Visualization is Shaping Malware Detection](http://arxiv.org/pdf/2505.07574v1)

Authors: Asmitha K. A., Matteo Brosolo, Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A., Muhammed Shafi K. P

Malware, a persistent cybersecurity threat, increasingly targets
interconnected digital systems such as desktop, mobile, and IoT platforms
through sophisticated attack vectors. By exploiting these vulnerabilities,
attackers compromise the integrity and resilience of modern digital ecosystems.
To address this risk, security experts actively employ Machine Learning or Deep
Learning-based strategies, integrating static, dynamic, or hybrid approaches to
categorize malware instances. Despite their advantages, these methods have
inherent drawbacks and malware variants persistently evolve with increased
sophistication, necessitating advancements in detection strategies.
Visualization-based techniques are emerging as scalable and interpretable
solutions for detecting and understanding malicious behaviors across diverse
platforms including desktop, mobile, IoT, and distributed systems as well as
through analysis of network packet capture files. In this comprehensive survey
of more than 100 high-quality research articles, we evaluate existing
visualization-based approaches applied to malware detection and classification.
As a first contribution, we propose a new all-encompassing framework to study
the landscape of visualization-based malware detection techniques. Within this
framework, we systematically analyze state-of-the-art approaches across the
critical stages of the malware detection pipeline. By analyzing not only the
single techniques but also how they are combined to produce the final solution,
we shed light on the main challenges in visualization-based approaches and
provide insights into the advancements and potential future directions in this
critical field.

### 3. [SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models](http://arxiv.org/pdf/2505.07584v1)

Authors: Huining Cui, Wei Liu

The increasing deployment of large language models in security-sensitive
domains necessitates rigorous evaluation of their resilience against
adversarial prompt-based attacks. While previous benchmarks have focused on
security evaluations with limited and predefined attack domains, such as
cybersecurity attacks, they often lack a comprehensive assessment of
intent-driven adversarial prompts and the consideration of real-life
scenario-based multi-turn attacks. To address this gap, we present
SecReEvalBench, the Security Resilience Evaluation Benchmark, which defines
four novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic
Score, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection
Time Score. Moreover, SecReEvalBench employs six questioning sequences for
model assessment: one-off attack, successive attack, successive reverse attack,
alternative attack, sequential ascending attack with escalating threat levels
and sequential descending attack with diminishing threat levels. In addition,
we introduce a dataset customized for the benchmark, which incorporates both
neutral and malicious prompts, categorised across seven security domains and
sixteen attack techniques. In applying this benchmark, we systematically
evaluate five state-of-the-art open-weighted large language models, Llama 3.1,
Gemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical
insights into the strengths and weaknesses of modern large language models in
defending against evolving adversarial threats. The SecReEvalBench dataset is
publicly available at
https://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,
which provides a groundwork for advancing research in large language model
security.

### 4. [Securing WiFi Fingerprint-based Indoor Localization Systems from Malicious Access Points](http://arxiv.org/pdf/2505.07724v1)

Authors: Fariha Tanjim Shifat, Sayma Sarwar Ela, Mosarrat Jahan

WiFi fingerprint-based indoor localization schemes deliver highly accurate
location data by matching the received signal strength indicator (RSSI) with an
offline database using machine learning (ML) or deep learning (DL) models.
However, over time, RSSI values degrade due to the malicious behavior of access
points (APs), causing low positional accuracy due to RSSI value mismatch with
the offline database. Existing literature lacks detection of malicious APs in
the online phase and mitigating their effects. This research addresses these
limitations and proposes a long-term reliable indoor localization scheme by
incorporating malicious AP detection and their effect mitigation techniques.
The proposed scheme uses a Light Gradient-Boosting Machine (LGBM) classifier to
estimate locations and integrates simple yet efficient techniques to detect
malicious APs based on online query data. Subsequently, a mitigation technique
is incorporated that updates the offline database and online queries by
imputing stable values for malicious APs using LGBM Regressors. Additionally,
we introduce a noise addition mechanism in the offline database to capture the
dynamic environmental effects. Extensive experimental evaluation shows that the
proposed scheme attains a detection accuracy above 95% for each attack type.
The mitigation strategy effectively restores the system's performance nearly to
its original state when no malicious AP is present. The noise addition module
reduces localization errors by nearly 16%. Furthermore, the proposed solution
is lightweight, reducing the execution time by approximately 94% compared to
the existing methods.

### 5. [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](http://arxiv.org/pdf/2505.07167v1)

Authors: Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin

Large Language Models (LLMs) have been extensively used across diverse
domains, including virtual assistants, automated code generation, and
scientific research. However, they remain vulnerable to jailbreak attacks,
which manipulate the models into generating harmful responses despite safety
alignment. Recent studies have shown that current safety-aligned LLMs often
undergo the shallow safety alignment, where the first few tokens largely
determine whether the response will be harmful. Through comprehensive
observations, we find that safety-aligned LLMs and various defense strategies
generate highly similar initial tokens in their refusal responses, which we
define as safety trigger tokens. Building on this insight, we propose
\texttt{D-STT}, a simple yet effective defense algorithm that identifies and
explicitly decodes safety trigger tokens of the given safety-aligned LLM to
trigger the model's learned safety patterns. In this process, the safety
trigger is constrained to a single token, which effectively preserves model
usability by introducing minimum intervention in the decoding process.
Extensive experiments across diverse jailbreak attacks and benign prompts
demonstrate that \ours significantly reduces output harmfulness while
preserving model usability and incurring negligible response time overhead,
outperforming ten baseline methods.

### 6. [Securing Genomic Data Against Inference Attacks in Federated Learning Environments](http://arxiv.org/pdf/2505.07188v1)

Authors: Chetan Pathade, Shubham Patil

Federated Learning (FL) offers a promising framework for collaboratively
training machine learning models across decentralized genomic datasets without
direct data sharing. While this approach preserves data locality, it remains
susceptible to sophisticated inference attacks that can compromise individual
privacy. In this study, we simulate a federated learning setup using synthetic
genomic data and assess its vulnerability to three key attack vectors:
Membership Inference Attack (MIA), Gradient-Based Membership Inference Attack,
and Label Inference Attack (LIA). Our experiments reveal that Gradient-Based
MIA achieves the highest effectiveness, with a precision of 0.79 and F1-score
of 0.87, underscoring the risk posed by gradient exposure in federated updates.
Additionally, we visualize comparative attack performance through radar plots
and quantify model leakage across clients. The findings emphasize the
inadequacy of na\"ive FL setups in safeguarding genomic privacy and motivate
the development of more robust privacy-preserving mechanisms tailored to the
unique sensitivity of genomic data.

### 7. [Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity](http://arxiv.org/pdf/2505.07239v1)

Authors: Guang Yan, Yuhui Zhang, Zimu Guo, Lutan Zhao, Xiaojun Chen, Chen Wang, Wenhao Wang, Dan Meng, Rui Hou

With the growing use of large language models (LLMs) hosted on cloud
platforms to offer inference services, privacy concerns about the potential
leakage of sensitive information are escalating. Secure multi-party computation
(MPC) is a promising solution to protect the privacy in LLM inference. However,
MPC requires frequent inter-server communication, causing high performance
overhead.
  Inspired by the prevalent activation sparsity of LLMs, where most neuron are
not activated after non-linear activation functions, we propose an efficient
private inference system, Comet. This system employs an accurate and fast
predictor to predict the sparsity distribution of activation function output.
Additionally, we introduce a new private inference protocol. It efficiently and
securely avoids computations involving zero values by exploiting the spatial
locality of the predicted sparse distribution. While this computation-avoidance
approach impacts the spatiotemporal continuity of KV cache entries, we address
this challenge with a low-communication overhead cache refilling strategy that
merges miss requests and incorporates a prefetching mechanism. Finally, we
evaluate Comet on four common LLMs and compare it with six state-of-the-art
private inference systems. Comet achieves a 1.87x-2.63x speedup and a
1.94x-2.64x communication reduction.

### 8. [Assessing the Latency of Network Layer Security in 5G Networks](http://arxiv.org/pdf/2505.07328v1)

Authors: Sotiris Michaelides, Jonathan Mucke, Martin Henze

In contrast to its predecessors, 5G supports a wide range of commercial,
industrial, and critical infrastructure scenarios. One key feature of 5G,
ultra-reliable low latency communication, is particularly appealing to such
scenarios for its real-time capabilities. However, 5G's enhanced security,
mostly realized through optional security controls, imposes additional overhead
on the network performance, potentially hindering its real-time capabilities.
To better assess this impact and guide operators in choosing between different
options, we measure the latency overhead of IPsec when applied over the N3 and
the service-based interfaces to protect user and control plane data,
respectively. Furthermore, we evaluate whether WireGuard constitutes an
alternative to reduce this overhead. Our findings show that IPsec, if
configured correctly, has minimal latency impact and thus is a prime candidate
to secure real-time critical scenarios.

### 9. [Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption](http://arxiv.org/pdf/2505.07329v1)

Authors: Jordan Frery, Roman Bredehoft, Jakub Klemsa, Arthur Meyre, Andrei Stoian

Preserving data confidentiality during the fine-tuning of open-source Large
Language Models (LLMs) is crucial for sensitive applications. This work
introduces an interactive protocol adapting the Low-Rank Adaptation (LoRA)
technique for private fine-tuning. Homomorphic Encryption (HE) protects the
confidentiality of training data and gradients handled by remote worker nodes
performing the bulk of computations involving the base model weights. The data
owner orchestrates training, requiring minimal local computing power and
memory, thus alleviating the need for expensive client-side GPUs. We
demonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting
convergence results using HE-compatible quantization and performance benchmarks
for HE computations on GPU hardware. This approach enables applications such as
confidential knowledge base question answering, private codebase fine-tuning
for AI code assistants, AI agents for drafting emails based on a company's
email archive, and adapting models to analyze sensitive legal or healthcare
documents.

### 10. [Post-Quantum Secure Decentralized Random Number Generation Protocol with Two Rounds of Communication in the Standard Model](http://arxiv.org/pdf/2505.07536v1)

Authors: Pham Nhat Minh, Khuong Nguyen-An

Randomness plays a vital role in numerous applications, including simulation,
cryptography, distributed systems, and gaming. Consequently, extensive research
has been conducted to generate randomness. One such method is to design a
decentralized random number generator (DRNG), a protocol that enables multiple
participants to collaboratively generate random outputs that must be publicly
verifiable. However, existing DRNGs are either not secure against quantum
computers or depend on the random oracle model (ROM) to achieve security. In
this paper, we design a DRNG based on lattice-based publicly verifiable secret
sharing (PVSS) that is post-quantum secure and proven secure in the standard
model. Additionally, our DRNG requires only two rounds of communication to
generate a single (pseudo)random value and can tolerate up to any t < n/2
dishonest participants. To our knowledge, the proposed DRNG construction is the
first to achieve all these properties.

### Computer Vision and Pattern Recognition

### 1. [Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework](http://arxiv.org/pdf/2505.07165v1)

Authors: Jun Li, Hongzhang Zhu, Tao Chen, Xiaohua Qian

Recently, numerous pancreas segmentation methods have achieved promising
performance on local single-source datasets. However, these methods don't
adequately account for generalizability issues, and hence typically show
limited performance and low stability on test data from other sources.
Considering the limited availability of distinct data sources, we seek to
improve the generalization performance of a pancreas segmentation model trained
with a single-source dataset, i.e., the single source generalization task. In
particular, we propose a dual self-supervised learning model that incorporates
both global and local anatomical contexts. Our model aims to fully exploit the
anatomical features of the intra-pancreatic and extra-pancreatic regions, and
hence enhance the characterization of the high-uncertainty regions for more
robust generalization. Specifically, we first construct a global-feature
contrastive self-supervised learning module that is guided by the pancreatic
spatial structure. This module obtains complete and consistent pancreatic
features through promoting intra-class cohesion, and also extracts more
discriminative features for differentiating between pancreatic and
non-pancreatic tissues through maximizing inter-class separation. It mitigates
the influence of surrounding tissue on the segmentation outcomes in
high-uncertainty regions. Subsequently, a local-image restoration
self-supervised learning module is introduced to further enhance the
characterization of the high uncertainty regions. In this module, informative
anatomical contexts are actually learned to recover randomly corrupted
appearance patterns in those regions.

### 2. [Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning](http://arxiv.org/pdf/2505.07172v1)

Authors: Zexian Yang, Dian Li, Dayan Wu, Gang Liu, Weiping Wang

Despite significant advancements in multimodal reasoning tasks, existing
Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded
responses when interpreting associated images. In contrast, when humans embark
on learning new knowledge, they often rely on a set of fundamental pre-study
principles: reviewing outlines to grasp core concepts, summarizing key points
to guide their focus and enhance understanding. However, such preparatory
actions are notably absent in the current instruction tuning processes. This
paper presents Re-Critic, an easily scalable rationale-augmented framework
designed to incorporate fundamental rules and chain-of-thought (CoT) as a
bridge to enhance reasoning abilities. Specifically, Re-Critic develops a
visual rationale synthesizer that scalably augments raw instructions with
rationale explanation. To probe more contextually grounded responses, Re-Critic
employs an in-context self-critic mechanism to select response pairs for
preference tuning. Experiments demonstrate that models fine-tuned with our
rationale-augmented dataset yield gains that extend beyond
hallucination-specific tasks to broader multimodal reasoning tasks.

### 3. [Ranking-aware Continual Learning for LiDAR Place Recognition](http://arxiv.org/pdf/2505.07198v1)

Authors: Xufei Wang, Gengxuan Tian, Junqiao Zhao, Siyue Tao, Qiwen Gu, Qiankun Yu, Tiantian Feng

Place recognition plays a significant role in SLAM, robot navigation, and
autonomous driving applications. Benefiting from deep learning, the performance
of LiDAR place recognition (LPR) has been greatly improved. However, many
existing learning-based LPR methods suffer from catastrophic forgetting, which
severely harms the performance of LPR on previously trained places after
training on a new environment. In this paper, we introduce a continual learning
framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate
forgetting. Inspired by the ranking process of place recognition retrieval, we
present a ranking-aware knowledge distillation loss that encourages the network
to preserve the high-level place recognition knowledge. We also introduce a
knowledge fusion module to integrate the knowledge of old and new models for
LiDAR place recognition. Our extensive experiments demonstrate that KDF can be
applied to different networks to overcome catastrophic forgetting, surpassing
the state-of-the-art methods in terms of mean Recall@1 and forgetting score.

### 4. [Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models](http://arxiv.org/pdf/2505.07209v1)

Authors: Yan Xie, Zequn Zeng, Hao Zhang, Yucheng Ding, Yi Wang, Zhengjue Wang, Bo Chen, Hongwei Liu

Concept Bottleneck Models (CBMs) try to make the decision-making process
transparent by exploring an intermediate concept space between the input image
and the output prediction. Existing CBMs just learn coarse-grained relations
between the whole image and the concepts, less considering local image
information, leading to two main drawbacks: i) they often produce spurious
visual-concept relations, hence decreasing model reliability; and ii) though
CBMs could explain the importance of every concept to the final prediction, it
is still challenging to tell which visual region produces the prediction. To
solve these problems, this paper proposes a Disentangled Optimal Transport CBM
(DOT-CBM) framework to explore fine-grained visual-concept relations between
local image patches and concepts. Specifically, we model the concept prediction
process as a transportation problem between the patches and concepts, thereby
achieving explicit fine-grained feature alignment. We also incorporate
orthogonal projection losses within the modality to enhance local feature
disentanglement. To further address the shortcut issues caused by statistical
biases in the data, we utilize the visual saliency map and concept label
statistics as transportation priors. Thus, DOT-CBM can visualize inversion
heatmaps, provide more reliable concept predictions, and produce more accurate
class predictions. Comprehensive experiments demonstrate that our proposed
DOT-CBM achieves SOTA performance on several tasks, including image
classification, local part detection and out-of-distribution generalization.

### 5. [When Dance Video Archives Challenge Computer Vision](http://arxiv.org/pdf/2505.07249v1)

Authors: Philippe Colantoni, Rafique Ahmed, Prashant Ghimire, Damien Muselet, Alain Trémeau

The accuracy and efficiency of human body pose estimation depend on the
quality of the data to be processed and of the particularities of these data.
To demonstrate how dance videos can challenge pose estimation techniques, we
proposed a new 3D human body pose estimation pipeline which combined up-to-date
techniques and methods that had not been yet used in dance analysis. Second, we
performed tests and extensive experimentations from dance video archives, and
used visual analytic tools to evaluate the impact of several data parameters on
human body pose. Our results are publicly available for research at
https://www.couleur.org/articles/arXiv-1-2025/

### 6. [Synthetic Similarity Search in Automotive Production](http://arxiv.org/pdf/2505.07256v1)

Authors: Christoph Huber, Ludwig Schleeh, Dino Knoll, Michael Guthe

Visual quality inspection in automotive production is essential for ensuring
the safety and reliability of vehicles. Computer vision (CV) has become a
popular solution for these inspections due to its cost-effectiveness and
reliability. However, CV models require large, annotated datasets, which are
costly and time-consuming to collect. To reduce the need for extensive training
data, we propose a novel image classification pipeline that combines similarity
search using a vision-based foundation model with synthetic data. Our approach
leverages a DINOv2 model to transform input images into feature vectors, which
are then compared to pre-classified reference images using cosine distance
measurements. By utilizing synthetic data instead of real images as references,
our pipeline achieves high classification accuracy without relying on real
data. We evaluate this approach in eight real-world inspection scenarios and
demonstrate that it meets the high performance requirements of production
environments.

### 7. [Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning](http://arxiv.org/pdf/2505.07263v1)

Authors: Xiaokun Wang, Chris, Jiangbo Pei, Wei Shen, Yi Peng, Yunzhuo Hao, Weijie Qiu, Ai Jian, Tianyidan Xie, Xuchen Song, Yang Liu, Yahui Zhou

We propose Skywork-VL Reward, a multimodal reward model that provides reward
signals for both multimodal understanding and reasoning tasks. Our technical
approach comprises two key components: First, we construct a large-scale
multimodal preference dataset that covers a wide range of tasks and scenarios,
with responses collected from both standard vision-language models (VLMs) and
advanced VLM reasoners. Second, we design a reward model architecture based on
Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage
fine-tuning using pairwise ranking loss on pairwise preference data.
Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art
results on multimodal VL-RewardBench and exhibits competitive performance on
the text-only RewardBench benchmark. Furthermore, preference data constructed
based on our Skywork-VL Reward proves highly effective for training Mixed
Preference Optimization (MPO), leading to significant improvements in
multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as
a significant advancement toward general-purpose, reliable reward models for
multimodal alignment. Our model has been publicly released to promote
transparency and reproducibility.

### 8. [L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers](http://arxiv.org/pdf/2505.07300v1)

Authors: Sofia Casarin, Sergio Escalera, Oswald Lanz

Training-free Neural Architecture Search (NAS) efficiently identifies
high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot
and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the
need for model training, and (ii) interpretable, with proxy designs often
theoretically grounded. Despite rapid developments in the field, current SOTA
ZC proxies are typically constrained to well-established convolutional search
spaces. With the rise of Large Language Models shaping the future of deep
learning, this work extends ZC proxy applicability to Vision Transformers
(ViTs). We present a new benchmark using the Autoformer search space evaluated
on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients
information (L-SWAG), a novel, generalizable metric that characterizes both
convolutional and transformer architectures across 14 tasks. Additionally,
previous works highlighted how different proxies contain complementary
information, motivating the need for a ML model to identify useful
combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low
Information gain and Bias Re-Alignment), a method that strategically combines
proxies to best represent a specific benchmark. Integrated into the NAS search,
LIBRA-NAS outperforms evolution and gradient-based NAS techniques by
identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1
GPU days.

### 9. [Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos](http://arxiv.org/pdf/2505.07301v2)

Authors: Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita

In 3D Human Motion Prediction (HMP), conventional methods train HMP models
with expensive motion capture data. However, the data collection cost of such
motion capture data limits the data diversity, which leads to poor
generalizability to unseen motions or subjects. To address this issue, this
paper proposes to enhance HMP with additional learning using estimated poses
from easily available videos. The 2D poses estimated from the monocular videos
are carefully transformed into motion capture-style 3D motions through our
pipeline. By additional learning with the obtained motions, the HMP model is
adapted to the test domain. The experimental results demonstrate the
quantitative and qualitative impact of our method.

### 10. [Enabling Privacy-Aware AI-Based Ergonomic Analysis](http://arxiv.org/pdf/2505.07306v1)

Authors: Sander De Coninck, Emilio Gamba, Bart Van Doninck, Abdellatif Bey-Temsamani, Sam Leroux, Pieter Simoens

Musculoskeletal disorders (MSDs) are a leading cause of injury and
productivity loss in the manufacturing industry, incurring substantial economic
costs. Ergonomic assessments can mitigate these risks by identifying workplace
adjustments that improve posture and reduce strain. Camera-based systems offer
a non-intrusive, cost-effective method for continuous ergonomic tracking, but
they also raise significant privacy concerns. To address this, we propose a
privacy-aware ergonomic assessment framework utilizing machine learning
techniques. Our approach employs adversarial training to develop a lightweight
neural network that obfuscates video data, preserving only the essential
information needed for human pose estimation. This obfuscation ensures
compatibility with standard pose estimation algorithms, maintaining high
accuracy while protecting privacy. The obfuscated video data is transmitted to
a central server, where state-of-the-art keypoint detection algorithms extract
body landmarks. Using multi-view integration, 3D keypoints are reconstructed
and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system
provides a secure, effective solution for ergonomic monitoring in industrial
environments, addressing both privacy and workplace safety concerns.

### Computers and Society

### 1. [Promising Topics for U.S.-China Dialogues on AI Risks and Governance](http://arxiv.org/pdf/2505.07468v1)

Authors: Saad Siddiqui, Lujain Ibrahim, Kristy Loke, Stephen Clare, Marianne Lu, Aris Richardson, Conor McGlynn, Jeffrey Ding

Cooperation between the United States and China, the world's leading
artificial intelligence (AI) powers, is crucial for effective global AI
governance and responsible AI development. Although geopolitical tensions have
emphasized areas of conflict, in this work, we identify potential common ground
for productive dialogue by conducting a systematic analysis of more than 40
primary AI policy and corporate governance documents from both nations.
Specifically, using an adapted version of the AI Governance and Regulatory
Archive (AGORA) - a comprehensive repository of global AI governance documents
- we analyze these materials in their original languages to identify areas of
convergence in (1) sociotechnical risk perception and (2) governance
approaches. We find strong and moderate overlap in several areas such as on
concerns about algorithmic transparency, system reliability, agreement on the
importance of inclusive multi-stakeholder engagement, and AI's role in
enhancing safety. These findings suggest that despite strategic competition,
there exist concrete opportunities for bilateral U.S.-China cooperation in the
development of responsible AI. Thus, we present recommendations for furthering
diplomatic dialogues that can facilitate such cooperation. Our analysis
contributes to understanding how different international governance frameworks
might be harmonized to promote global responsible AI development.

### 2. [The Value of Disagreement in AI Design, Evaluation, and Alignment](http://arxiv.org/pdf/2505.07772v1)

Authors: Sina Fazelpour, Will Fleisher

Disagreements are widespread across the design, evaluation, and alignment
pipelines of artificial intelligence (AI) systems. Yet, standard practices in
AI development often obscure or eliminate disagreement, resulting in an
engineered homogenization that can be epistemically and ethically harmful,
particularly for marginalized groups. In this paper, we characterize this risk,
and develop a normative framework to guide practical reasoning about
disagreement in the AI lifecycle. Our contributions are two-fold. First, we
introduce the notion of perspectival homogenization, characterizing it as a
coupled ethical-epistemic risk that arises when an aspect of an AI system's
development unjustifiably suppresses disagreement and diversity of
perspectives. We argue that perspectival homogenization is best understood as a
procedural risk, which calls for targeted interventions throughout the AI
development pipeline. Second, we propose a normative framework to guide such
interventions, grounded in lines of research that explain why disagreement can
be epistemically beneficial, and how its benefits can be realized in practice.
We apply this framework to key design questions across three stages of AI
development tasks: when disagreement is epistemically valuable; whose
perspectives should be included and preserved; how to structure tasks and
navigate trade-offs; and how disagreement should be documented and
communicated. In doing so, we challenge common assumptions in AI practice,
offer a principled foundation for emerging participatory and pluralistic
approaches, and identify actionable pathways for future work in AI design and
governance.

### 3. [A Turing Test for ''Localness'': Conceptualizing, Defining, and Recognizing Localness in People and Machines](http://arxiv.org/pdf/2505.07282v1)

Authors: Zihan Gao, Justin Cranshaw, Jacob Thebault-Spieker

As digital platforms increasingly mediate interactions tied to place,
ensuring genuine local participation is essential for maintaining trust and
credibility in location-based services, community-driven platforms, and civic
engagement systems. However, localness is a social and relational identity
shaped by knowledge, participation, and community recognition. Drawing on the
German philosopher Heidegger's concept of dwelling -- which extends beyond
physical presence to encompass meaningful connection to place -- we investigate
how people conceptualize and evaluate localness in both human and artificial
agents. Using a chat-based interaction paradigm inspired by Turing's Imitation
Game and Von Ahn's Games With A Purpose, we engaged 230 participants in
conversations designed to examine the cues people rely on to assess local
presence. Our findings reveal a multi-dimensional framework of localness,
highlighting differences in how locals and nonlocals emphasize various aspects
of local identity. We show that people are significantly more accurate in
recognizing locals than nonlocals, suggesting that localness is an affirmative
status requiring active demonstration rather than merely the absence of
nonlocal traits. Additionally, we identify conditions under which artificial
agents are perceived as local and analyze participants' sensemaking strategies
in evaluating localness. Through predictive modeling, we determine key factors
that drive accurate localness judgments. By bridging theoretical perspectives
on human-place relationships with practical challenges in digital environments,
our work informs the design of location-based services that foster meaningful
local engagement. Our findings contribute to a broader understanding of
localness as a dynamic and relational construct, reinforcing the importance of
dwelling as a process of belonging, recognition, and engagement with place.

### 4. [How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations](http://arxiv.org/pdf/2505.07317v1)

Authors: Ashmita Sampatsing, Sophie Vos, Emma Beauxis-Aussalet, Justus Bogner

With the ever-growing adoption of artificial intelligence (AI), AI-based
software and its negative impact on the environment are no longer negligible,
and studying and mitigating this impact has become a critical area of research.
However, it is currently unclear which role environmental sustainability plays
during AI adoption in industry and how AI regulations influence Green AI
practices and decision-making in industry. We therefore aim to investigate the
Green AI perception and management of industry practitioners. To this end, we
conducted a total of 11 interviews with participants from 10 different
organizations that adopted AI-based software. The interviews explored three
main themes: AI adoption, current efforts in mitigating the negative
environmental impact of AI, and the influence of the EU AI Act and the
Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that
9 of 11 participants prioritized business efficiency during AI adoption, with
minimal consideration of environmental sustainability. Monitoring and
mitigation of AI's environmental impact were very limited. Only one participant
monitored negative environmental effects. Regarding applied mitigation
practices, six participants reported no actions, with the others sporadically
mentioning techniques like prompt engineering, relying on smaller models, or
not overusing AI. Awareness and compliance with the EU AI Act are low, with
only one participant reporting on its influence, while the CSRD drove
sustainability reporting efforts primarily in larger companies. All in all, our
findings reflect a lack of urgency and priority for sustainable AI among these
companies. We suggest that current regulations are not very effective, which
has implications for policymakers. Additionally, there is a need to raise
industry awareness, but also to provide user-friendly techniques and tools for
Green AI practices.

### 5. [AI in Money Matters](http://arxiv.org/pdf/2505.07393v1)

Authors: Nadine Sandjo Tchatchoua, Richard Harper

In November 2022, Europe and the world by and large were stunned by the birth
of a new large language model : ChatGPT. Ever since then, both academic and
populist discussions have taken place in various public spheres such as
LinkedIn and X(formerly known as Twitter) with the view to both understand the
tool and its benefits for the society. The views of real actors in professional
spaces, especially in regulated industries such as finance and law have been
largely missing. We aim to begin to close this gap by presenting results from
an empirical investigation conducted through interviews with professional
actors in the Fintech industry. The paper asks the question, how and to what
extent are large language models in general and ChatGPT in particular being
adopted and used in the Fintech industry? The results show that while the
fintech experts we spoke with see a potential in using large language models in
the future, a lot of questions marks remain concerning how they are policed and
therefore might be adopted in a regulated industry such as Fintech. This paper
aims to add to the existing academic discussing around large language models,
with a contribution to our understanding of professional viewpoints.

### 6. [YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models](http://arxiv.org/pdf/2505.07581v1)

Authors: Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen

Leveraging large language model (LLM) based agents to simulate human social
behaviors has recently gained significant attention. In this paper, we
introduce a novel social simulator called YuLan-OneSim. Compared to previous
works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free
scenario construction: Users can simply describe and refine their simulation
scenarios through natural language interactions with our simulator. All
simulation code is automatically generated, significantly reducing the need for
programming expertise. (2) Comprehensive default scenarios: We implement 50
default simulation scenarios spanning 8 domains, including economics,
sociology, politics, psychology, organization, demographics, law, and
communication, broadening access for a diverse range of social researchers. (3)
Evolvable simulation: Our simulator is capable of receiving external feedback
and automatically fine-tuning the backbone LLMs, significantly enhancing the
simulation quality. (4) Large-scale simulation: By developing a fully
responsive agent framework and a distributed simulation architecture, our
simulator can handle up to 100,000 agents, ensuring more stable and reliable
simulation results. (5) AI social researcher: Leveraging the above features, we
develop an AI social researcher. Users only need to propose a research topic,
and the AI researcher will automatically analyze the input, construct
simulation environments, summarize results, generate technical reports, review
and refine the reports--completing the social science research loop. To
demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate
the quality of the automatically generated scenarios, the reliability,
efficiency, and scalability of the simulation process, as well as the
performance of the AI social researcher.

### 7. [4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients](http://arxiv.org/pdf/2505.07702v1)

Authors: Onthada Preedasawakul, Nathakhun Wiroonsri

Diabetes is one of the most prevalent diseases worldwide, characterized by
persistently high blood sugar levels, capable of damaging various internal
organs and systems. Diabetes patients require routine check-ups, resulting in a
time series of laboratory records, such as hemoglobin A1c, which reflects each
patient's health behavior over time and informs their doctor's recommendations.
Clustering patients into groups based on their entire time series data assists
doctors in making recommendations and choosing treatments without the need to
review all records. However, time series clustering of this type of dataset
introduces some challenges; patients visit their doctors at different time
points, making it difficult to capture and match trends, peaks, and patterns.
Additionally, two aspects must be considered: differences in the levels of
laboratory results and differences in trends and patterns. To address these
challenges, we introduce a new clustering algorithm called Time and Trend
Traveling Time Series Clustering (4TaStiC), using a base dissimilarity measure
combined with Euclidean and Pearson correlation metrics. We evaluated this
algorithm on artificial datasets, comparing its performance with that of seven
existing methods. The results show that 4TaStiC outperformed the other methods
on the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of
1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients
exhibits clear characteristics that will benefit doctors in making efficient
clinical decisions. Furthermore, the proposed algorithm can be applied to
contexts outside the medical field.

### 8. [Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms](http://arxiv.org/pdf/2505.07339v1)

Authors: Gabriel Lima, Nina Grgić-Hlača, Markus Langer, Yixin Zou

Affirmative algorithms have emerged as a potential answer to algorithmic
discrimination, seeking to redress past harms and rectify the source of
historical injustices. We present the results of two experiments ($N$$=$$1193$)
capturing laypeople's perceptions of affirmative algorithms -- those which
explicitly prioritize the historically marginalized -- in hiring and criminal
justice. We contrast these opinions about affirmative algorithms with folk
attitudes towards algorithms that prioritize the privileged (i.e.,
discriminatory) and systems that make decisions independently of demographic
groups (i.e., fair). We find that people -- regardless of their political
leaning and identity -- view fair algorithms favorably and denounce
discriminatory systems. In contrast, we identify disagreements concerning
affirmative algorithms: liberals and racial minorities rate affirmative systems
as positively as their fair counterparts, whereas conservatives and those from
the dominant racial group evaluate affirmative algorithms as negatively as
discriminatory systems. We identify a source of these divisions: people have
varying beliefs about who (if anyone) is marginalized, shaping their views of
affirmative algorithms. We discuss the possibility of bridging these
disagreements to bring people together towards affirmative algorithms.

### 9. [Energy personas in Danish households](http://arxiv.org/pdf/2505.07408v1)

Authors: Nadine Sandjo Tchatchoua, Line Valdorff Madsen, Anders Rhiger Hansen

Technologies to monitor the provision of renewable energy are part of
emerging technologies to help address the discrepancy between renewable energy
production and its related usage in households. This paper presents various
ways householders use a technological artifact for the real-time monitoring of
renewable energy provision. Such a monitoring thus affords householders with an
opportunity to adjust their energy consumption according to renewable energy
provision. In Denmark, Ewii, previously Barry, is a Danish energy supplier
which provides householders with an opportunity to monitor energy sources in
real time through a technological solution of the same name. This paper use
provision afforded by Ewii as a case for exploring how householders organize
themselves to use a technological artefact that supports the monitoring of
energy and its related usage. This study aims to inform technology design
through the derivation of four personas. The derived personas highlight the
differences in energy monitoring practices for the householders and their
engagement. These personas are characterised as dedicated, organised, sporadic,
and convenient. Understanding these differences in energy monitoring practice
using the technological artefact form a solid element in the design of future
energy technologies that interfere with the everyday practices and energy
consumption for households. This is paramount for future energy related
technology design, and for the clarification of usage assumptions that are
embedded in the rollout of energy related technology as a country such as
Denmark moves through its green transition.

### 10. [Must Read: A Systematic Survey of Computational Persuasion](http://arxiv.org/pdf/2505.07775v1)

Authors: Nimet Beyza Bozdag, Shuhaib Mehri, Xiaocheng Yang, Hyeonjeong Ha, Zirui Cheng, Esin Durmus, Jiaxuan You, Heng Ji, Gokhan Tur, Dilek Hakkani-Tür

Persuasion is a fundamental aspect of communication, influencing
decision-making across diverse contexts, from everyday conversations to
high-stakes scenarios such as politics, marketing, and law. The rise of
conversational AI systems has significantly expanded the scope of persuasion,
introducing both opportunities and risks. AI-driven persuasion can be leveraged
for beneficial applications, but also poses threats through manipulation and
unethical influence. Moreover, AI systems are not only persuaders, but also
susceptible to persuasion, making them vulnerable to adversarial attacks and
bias reinforcement. Despite rapid advancements in AI-generated persuasive
content, our understanding of what makes persuasion effective remains limited
due to its inherently subjective and context-dependent nature. In this survey,
we provide a comprehensive overview of computational persuasion, structured
around three key perspectives: (1) AI as a Persuader, which explores
AI-generated persuasive content and its applications; (2) AI as a Persuadee,
which examines AI's susceptibility to influence and manipulation; and (3) AI as
a Persuasion Judge, which analyzes AI's role in evaluating persuasive
strategies, detecting manipulation, and ensuring ethical persuasion. We
introduce a taxonomy for computational persuasion research and discuss key
challenges, including evaluating persuasiveness, mitigating manipulative
persuasion, and developing responsible AI-driven persuasive systems. Our survey
outlines future research directions to enhance the safety, fairness, and
effectiveness of AI-powered persuasion while addressing the risks posed by
increasingly capable language models.

### Databases

### 1. [Towards Cross-Model Efficiency in SQL/PGQ](http://arxiv.org/pdf/2505.07595v1)

Authors: Hadar Rotschield, Liat Peterfreund

SQL/PGQ is a new standard that integrates graph querying into relational
systems, allowing users to freely switch between graph patterns and SQL. Our
experiments show performance gaps between these models, as queries written in
both formalisms can exhibit varying performance depending on the formalism
used, suggesting that current approaches handle each query type separately,
applying distinct optimizations to each formalism. We argue that a holistic
optimization is necessary, where the system internally decides on the best
algorithms regardless of whether queries are written in SQL or as graph
patterns. We propose possible future research direction to unify these
optimizations and mitigate performance gaps.

### 2. [ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and Dynamic Workloads in Large-scale Cloud Environments](http://arxiv.org/pdf/2505.07692v1)

Authors: Rong Kang, Yanbin Chen, Ye Liu, Fuxin Jiang, Qingshuo Li, Miao Ma, Jian Liu, Guangliang Zhao, Tieying Zhang, Jianjun Chen, Lei Zhang

Multi-tenant architectures enhance the elasticity and resource utilization of
NoSQL databases by allowing multiple tenants to co-locate and share resources.
However, in large-scale cloud environments, the diverse and dynamic nature of
workloads poses significant challenges for multi-tenant NoSQL databases. Based
on our practical observations, we have identified three crucial challenges: (1)
the impact of caching on performance isolation, as cache hits alter request
execution and resource consumption, leading to inaccurate traffic control; (2)
the dynamic changes in traffic, with changes in tenant traffic trends causing
throttling or resource wastage, and changes in access distribution causing hot
key pressure or cache hit ratio drops; and (3) the imbalanced layout of data
nodes due to tenants' diverse resource requirements, leading to low resource
utilization. To address these challenges, we introduce ABase, a multi-tenant
NoSQL serverless database developed at ByteDance. ABase introduces a two-layer
caching mechanism with a cache-aware isolation mechanism to ensure accurate
resource consumption estimates. Furthermore, ABase employs a predictive
autoscaling policy to dynamically adjust resources in response to tenant
traffic changes and a multi-resource rescheduling algorithm to balance resource
utilization across data nodes. With these innovations, ABase has successfully
served ByteDance's large-scale cloud environment, supporting a total workload
that has achieved a peak QPS of over 13 billion and total storage exceeding 1
EB.

### 3. [Bang for the Buck: Vector Search on Cloud CPUs](http://arxiv.org/pdf/2505.07621v1)

Authors: Leonardo Kuffo, Peter Boncz

Vector databases have emerged as a new type of systems that support efficient
querying of high-dimensional vectors. Many of these offer their database as a
service in the cloud. However, the variety of available CPUs and the lack of
vector search benchmarks across CPUs make it difficult for users to choose one.
In this study, we show that CPU microarchitectures available in the cloud
perform significantly differently across vector search scenarios. For instance,
in an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per
second (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the
tables turn. However, when looking at the number of queries per dollar (QP$),
Graviton3 is the best option for most indexes and quantization settings, even
over Graviton4 (Table 1). With this work, we hope to guide users in getting the
best "bang for the buck" when deploying vector search systems.

### 4. [Generating Skyline Explanations for Graph Neural Networks](http://arxiv.org/pdf/2505.07635v1)

Authors: Dazhuo Qiu, Haolai Che, Arijit Khan, Yinghui Wu

This paper proposes a novel approach to generate subgraph explanations for
graph neural networks GNNs that simultaneously optimize multiple measures for
explainability. Existing GNN explanation methods often compute subgraphs
(called ``explanatory subgraphs'') that optimize a pre-defined, single
explainability measure, such as fidelity or conciseness. This can lead to
biased explanations that cannot provide a comprehensive explanation to clarify
the output of GNN models. We introduce skyline explanation, a GNN explanation
paradigm that aims to identify k explanatory subgraphs by simultaneously
optimizing multiple explainability measures. (1) We formulate skyline
explanation generation as a multi-objective optimization problem, and pursue
explanations that approximate a skyline set of explanatory subgraphs. We show
the hardness for skyline explanation generation. (2) We design efficient
algorithms with an onion-peeling approach that strategically removes edges from
neighbors of nodes of interests, and incrementally improves explanations as it
explores an interpretation domain, with provable quality guarantees. (3) We
further develop an algorithm to diversify explanations to provide more
comprehensive perspectives. Using real-world graphs, we empirically verify the
effectiveness, efficiency, and scalability of our algorithms.

### 5. [LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning](http://arxiv.org/pdf/2505.07437v1)

Authors: Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo

Instruction tuning has emerged as a critical paradigm for improving the
capabilities and alignment of large language models (LLMs). However, existing
iterative model-aware data selection methods incur significant computational
overhead, as they rely on repeatedly performing full-dataset model inference to
estimate sample utility for subsequent training iterations, creating a
fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient
iterative data selection framework that accurately estimates sample utility
entirely within the standard training loop, eliminating the need for costly
additional model inference. At its core, LEAD introduces Instance-Level Dynamic
Uncertainty (IDU), a theoretically grounded utility function combining
instantaneous training loss, gradient-based approximation of loss changes, and
exponential smoothing of historical loss signals. To further scale efficiently
to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,
adaptively prioritizing informative clusters through a multi-armed bandit
mechanism, followed by precise fine-grained selection of high-utility samples
using IDU. Extensive experiments across four diverse benchmarks show that LEAD
significantly outperforms state-of-the-art methods, improving average model
performance by 6.1%-10.8% while using only 2.5% of the training data and
reducing overall training time by 5-10x.

### Distributed, Parallel, and Cluster Computing

### 1. [PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications](http://arxiv.org/pdf/2505.07203v1)

Authors: Kuntai Du, Bowen Wang, Chen Zhang, Yiming Cheng, Qing Lan, Hejian Sang, Yihua Cheng, Jiayi Yao, Xiaoxuan Liu, Yifan Qiao, Ion Stoica, Junchen Jiang

Besides typical generative applications, like ChatGPT, GitHub Copilot, and
Cursor, we observe an emerging trend that LLMs are increasingly used in
traditional discriminative tasks, such as recommendation, credit verification,
and data labeling. The key characteristic of these emerging use cases is that
the LLM generates only a single output token, rather than an arbitrarily long
sequence of tokens. We call this prefill-only workload. However, since existing
LLM engines assume arbitrary output lengths, they fail to leverage the unique
properties of prefill-only workloads. In this paper, we present PrefillOnly,
the first LLM inference engine that improves the inference throughput and
latency by fully embracing the properties of prefill-only workloads. First,
since it generates only one token, PrefillOnly only needs to store the KV cache
of only the last computed layer, rather than of all layers. This drastically
reduces the GPU memory footprint of LLM inference and allows handling long
inputs without using solutions that reduces throughput, such as cross-GPU KV
cache parallelization. Second, because the output length is fixed, rather than
arbitrary, PrefillOnly can precisely determine the job completion time (JCT) of
each prefill-only request before it starts. This enables efficient JCT-aware
scheduling policies such as shortest remaining job first. PrefillOnly can
process upto 4x larger queries per second without inflating average and P99
latency.

### 2. [LA-IMR: Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling for Tail-Latency-Sensitive Cloud Robotics](http://arxiv.org/pdf/2505.07417v1)

Authors: Eunil Seo, Chanh Nguyen, Erik Elmroth

Hybrid cloud-edge infrastructures now support latency-critical workloads
ranging from autonomous vehicles and surgical robotics to immersive AR/VR.
However, they continue to experience crippling long-tail latency spikes
whenever bursty request streams exceed the capacity of heterogeneous edge and
cloud tiers. To address these long-tail latency issues, we present
Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling (LA-IMR).
This control layer integrates a closed-form, utilization-driven latency model
with event-driven scheduling, replica autoscaling, and edge-to-cloud offloading
to mitigate 99th-percentile (P99) delays. Our analytic model decomposes
end-to-end latency into processing, network, and queuing components, expressing
inference latency as an affine power-law function of instance utilization. Once
calibrated, it produces two complementary functions that drive: (i)
millisecond-scale routing decisions for traffic offloading, and (ii) capacity
planning that jointly determines replica pool sizes. LA-IMR enacts these
decisions through a quality-differentiated, multi-queue scheduler and a
custom-metric Kubernetes autoscaler that scales replicas proactively -- before
queues build up -- rather than reactively based on lagging CPU metrics. Across
representative vision workloads (YOLOv5m and EfficientDet) and bursty arrival
traces, LA-IMR reduces P99 latency by up to 20.7 percent compared to
traditional latency-only autoscaling, laying a principled foundation for
next-generation, tail-tolerant cloud-edge inference services.

### 3. [SwarmSearch: Decentralized Search Engine with Self-Funding Economy](http://arxiv.org/pdf/2505.07452v1)

Authors: Marcel Gregoriadis, Rowdy Chotkan, Petru Neague, Johan Pouwelse

Centralized search engines control what we see, read, believe, and vote.
Consequently, they raise concerns over information control, censorship, and
bias. Decentralized search engines offer a remedy to this problem, but their
adoption has been hindered by their inferior quality and lack of a
self-sustaining economic framework. We present SwarmSearch, a fully
decentralized, AI-powered search engine with a self-funding architecture. Our
system is designed for deployment within the decentralized file-sharing
software Tribler. SwarmSearch integrates volunteer-based with profit-driven
mechanisms to foster an implicit marketplace for resources. Employing the
state-of-the-art of AI-based retrieval and relevance ranking, we also aim to
close the quality gap between decentralized search and centralized
alternatives. Our system demonstrates high retrieval accuracy while showing
robustness in the presence of 50% adversarial nodes.

### 4. [AgentFlow: Resilient Adaptive Cloud-Edge Framework for Multi-Agent Coordination](http://arxiv.org/pdf/2505.07603v1)

Authors: Ching Han Chen, Ming Fang Shiu

This paper presents AgentFlow, a MAS-based framework for programmable
distributed systems in heterogeneous cloud-edge environments. It introduces
logistics objects and abstract agent interfaces to enable dynamic service flows
and modular orchestration. AgentFlow supports decentralized publish-subscribe
messaging and many-to-many service elections, enabling decision coordination
without a central server. It features plug-and-play node discovery, flexible
task reorganization, and highly adaptable fault tolerance and substitution
mechanisms. AgentFlow advances scalable, real-time coordination for resilient
and autonomous mission-critical systems.

### 5. [INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning](http://arxiv.org/pdf/2505.07291v1)

Authors: Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, Johannes Hagemann

We introduce INTELLECT-2, the first globally distributed reinforcement
learning (RL) training run of a 32 billion parameter language model. Unlike
traditional centralized training efforts, INTELLECT-2 trains a reasoning model
using fully asynchronous RL across a dynamic, heterogeneous swarm of
permissionless compute contributors.
  To enable a training run with this unique infrastructure, we built various
components from scratch: we introduce PRIME-RL, our training framework
purpose-built for distributed asynchronous reinforcement learning, based on top
of novel components such as TOPLOC, which verifies rollouts from untrusted
inference workers, and SHARDCAST, which efficiently broadcasts policy weights
from training nodes to inference workers.
  Beyond infrastructure components, we propose modifications to the standard
GRPO training recipe and data filtering techniques that were crucial to achieve
training stability and ensure that our model successfully learned its training
objective, thus improving upon QwQ-32B, the state of the art reasoning model in
the 32B parameter range.
  We open-source INTELLECT-2 along with all of our code and data, hoping to
encourage and enable more open research in the field of decentralized training.

### 6. [SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models](http://arxiv.org/pdf/2505.07680v1)

Authors: Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, Jidong Zhai

Large Language Models (LLMs) present a critical trade-off between inference
quality and computational cost: larger models offer superior capabilities but
incur significant latency, while smaller models are faster but less powerful.
Existing serving strategies often employ fixed model scales or static two-stage
speculative decoding, failing to dynamically adapt to the varying complexities
of user requests or fluctuations in system performance. This paper introduces
\systemname{}, a novel framework that reimagines LLM inference as an adaptive
routing problem solved through multi-level speculative decoding. \systemname{}
dynamically constructs and optimizes inference "paths" (chains of models) based
on real-time feedback, addressing the limitations of static approaches. Our
contributions are threefold: (1) An \textbf{adaptive model chain scheduling}
mechanism that leverages performance profiling (execution times) and predictive
similarity metrics (derived from token distribution divergence) to continuously
select the optimal sequence of draft and verifier models, minimizing predicted
latency per generated token. (2) A \textbf{multi-level collaborative
verification} framework where intermediate models within the selected chain can
validate speculative tokens, reducing the verification burden on the final,
most powerful target model. (3) A \textbf{synchronized state management} system
providing efficient, consistent KV cache handling across heterogeneous models
in the chain, including precise, low-overhead rollbacks tailored for
asynchronous batch processing inherent in multi-level speculation. Preliminary
experiments demonstrate the validity of our method.

### 7. [Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems](http://arxiv.org/pdf/2505.07755v1)

Authors: Tomasz Szydlo, Viacheslaw Horbanow, Dev Nandan Jha, Shashikant Ilager, Aleksander Slominski, Rajiv Ranjan

Edge computing has emerged as a pivotal technology, offering significant
advantages such as low latency, enhanced data security, and reduced reliance on
centralized cloud infrastructure. These benefits are crucial for applications
requiring real-time data processing or strict security measures. Despite these
advantages, edge devices operating within edge clusters are often
underutilized. This inefficiency is mainly due to the absence of a holistic
performance profiling mechanism which can help dynamically adjust the desired
system configuration for a given workload. Since edge computing environments
involve a complex interplay between CPU frequency, power consumption, and
application performance, a deeper understanding of these correlations is
essential. By uncovering these relationships, it becomes possible to make
informed decisions that enhance both computational efficiency and energy
savings. To address this gap, this paper evaluates the power consumption and
performance characteristics of a single processing node within an edge cluster
using a synthetic microbenchmark by varying the workload size and CPU
frequency. The results show how an optimal measure can lead to optimized usage
of edge resources, given both performance and power consumption.

### Digital Libraries

### 1. [From raw affiliations to organization identifiers](http://arxiv.org/pdf/2505.07577v2)

Authors: Myrto Kallipoliti, Serafeim Chatzopoulos, Miriam Baglioni, Eleni Adamidi, Paris Koloveas, Thanasis Vergoulis

Accurate affiliation matching, which links affiliation strings to
standardized organization identifiers, is critical for improving research
metadata quality, facilitating comprehensive bibliometric analyses, and
supporting data interoperability across scholarly knowledge bases. Existing
approaches fail to handle the complexity of affiliation strings that often
include mentions of multiple organizations or extraneous information. In this
paper, we present AffRo, a novel approach designed to address these challenges,
leveraging advanced parsing and disambiguation techniques. We also introduce
AffRoDB, an expert-curated dataset to systematically evaluate affiliation
matching algorithms, ensuring robust benchmarking. Results demonstrate the
effectiveness of AffRp in accurately identifying organizations from complex
affiliation strings.

### Discrete Mathematics

### 1. [Efficient Lifting of Discrete Logarithms Modulo Prime Powers](http://arxiv.org/pdf/2505.07434v1)

Authors: Giovanni Viglietta, Yasuyuki Kachi

We present a deterministic algorithm that, given a prime $p$ and a solution
$x \in \mathbb Z$ to the discrete logarithm problem $a^x \equiv b \pmod p$ with
$p\nmid a$, efficiently lifts it to a solution modulo $p^k$, i.e., $a^x \equiv
b \pmod {p^k}$, for any fixed $k \geq 1$.
  The algorithm performs $k(\lceil \log_2 p\rceil +2)+O(\log p)$
multiplications modulo $p^k$ in the worst case, improving upon prior lifting
methods by at least a factor of 8.

### 2. [On core of categorical product of (di)graphs](http://arxiv.org/pdf/2505.07463v1)

Authors: Reza Naserasr, Cyril Pujol

The core of a graph is the smallest graph (in terms of number of vertices) to
which it is homomorphically equivalent.
  The question of the possible order of the core of the tensor product (also
known as categorical, Heidetnemi or direct product) of two graphs captures some
well known problems. For instance, the recent counterexample to the Hedetniemi
conjecture for 5-chromatic graphs is equivalent to saying that there are cores
of order at least 5 whose product has a core of order 4.
  In this work, motivated by a question from Leonid Libkin in the area of graph
databases, we first present methods of building cores whose categorical product
is also a core. Extending on this we present sufficient conditions for a set of
cores to have a product which is also a core. Presenting an example of such a
family of digraphs, we construct a family of $\binom{2n}{n}$ digraphs, where
the number of vertices of each is between $n^2+5n+2$ and $3n^2+3n+2$ and the
product is a core. We then present a method of transforming the example into a
family of graphs.

### 3. [Isomorphisms of unit distance graphs of layers](http://arxiv.org/pdf/2505.07799v1)

Authors: Arthur Igorevich Bikeev

For any $\varepsilon\in (0,+\infty)$, consider the metric spaces $\mathbb{R}
\times [0,\varepsilon]$ in the Euclidean plane named layers or strips. B.
Baslaugh in 1998 found the minimal width $\varepsilon \in (0,1)$ of a layer
such that its unit distance graph contains a cycle of a given odd length $k$.
The first of the main results of this paper is the fact that the unit distance
graphs of two layers $\mathbb{R} \times [0,\varepsilon_1], \mathbb{R} \times
[0,\varepsilon_2]$ are non-isomorphic for any different values
$\varepsilon_1,\varepsilon_2 \in (0,+\infty)$. We also get a multidimensional
analogue of this theorem. For given $n,m \in \mathbb{N}, p \in (1,+\infty),
\varepsilon \in (0,+\infty)$, we say that the metric space on $\mathbb{R}^n
\times [0,\varepsilon]^m$ with the metric space distance generated by
$l_p$-norm in $\mathbb{R}^{n+m}$ is a layer $L(n,m,p,\varepsilon)$. We show
that the unit distance graphs of multi-layers $L(n,m,p,\varepsilon_1),
L(n,m,p,\varepsilon_2)$ are non-isomorphic for $\varepsilon_1 \neq
\varepsilon_2$.

### 4. [Reflexive Composition of Elementary State Machines, with an Application to the Reversal of Cellular Automata Rule 90](http://arxiv.org/pdf/2505.07186v2)

Authors: Chris Salzberg, Hiroki Sayama

We explore the dynamics of a one-dimensional lattice of state machines on two
states and two symbols sequentially updated via a process of "reflexive
composition." The space of 256 machines exhibits a variety of behavior,
including substitution, reversible "billiard ball" dynamics, and fractal
nesting. We show that one machine generates the Sierpinski Triangle and, for a
subset of boundary conditions, is isomorphic to cellular automata Rule 90 in
Wolfram's naming scheme. More surprisingly, two other machines follow
trajectories that map to Rule 90 in reverse. Whereas previous techniques have
been developed to uncover preimages of Rule 90, this is the first study to
produce such inverse dynamics naturally from the formalism itself. We argue
that the system's symmetric treatment of state and message underlies its
expressive power.

### 5. [Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning](http://arxiv.org/pdf/2505.07163v1)

Authors: Natalia G. Berloff

We present an exact spin-elimination technique that reduces the
dimensionality of both quadratic and k-local Ising Hamiltonians while
preserving their original ground-state configurations. By systematically
replacing each removed spin with an effective interaction among its neighbors,
our method lowers the total spin count without invoking approximations or
iterative recalculations. This capability is especially beneficial for
hardware-constrained platforms, classical or quantum, that can directly
implement multi-body interactions but have limited qubit or spin resources. We
demonstrate three key advances enabled by this technique. First, we handle
larger instances of benchmark problems such as Max-Cut on cubic graphs without
exceeding a 2-local interaction limit. Second, we reduce qubit requirements in
QAOA-based integer factorization on near-term quantum devices, thus extending
the feasible range of integers to be factorized. Third, we improve memory
capacity in Hopfield associative memories and enhance memory retrieval by
suppressing spurious attractors, enhancing retrieval performance. Our
spin-elimination procedure trades local spin complexity for higher-order
couplings or higher node degrees in a single pass, opening new avenues for
scaling up combinatorial optimization and energy-based machine learning on
near-term hardware. Finally, these results underscore that the next-generation
physical spin machines will likely capitalize on k-local spin Hamiltonians to
offer an alternative to classical computations.

### 6. [Revisiting Sparse Matrix Coloring and Bicoloring](http://arxiv.org/pdf/2505.07308v1)

Authors: Alexis Montoison, Guillaume Dalle, Assefaw Gebremedhin

Sparse matrix coloring and bicoloring are fundamental building blocks of
sparse automatic differentiation. Bicoloring is particularly advantageous for
rectangular Jacobian matrices with at least one dense row and column. Indeed,
in such cases, unidirectional row or column coloring demands a number of colors
equal to the number of rows or columns. We introduce a new strategy for
bicoloring that encompasses both direct and substitution-based decompression
approaches. Our method reformulates the two variants of bicoloring as star and
acyclic colorings of an augmented symmetric matrix. We extend the concept of
neutral colors, previously exclusive to bicoloring, to symmetric colorings, and
we propose a post-processing routine that neutralizes colors to further reduce
the overall color count. We also present the Julia package
SparseMatrixColorings, which includes these new bicoloring algorithms alongside
all standard coloring methods for sparse derivative matrix computation.
Compared to ColPack, the Julia package also offers enhanced implementations for
star and acyclic coloring, vertex ordering, as well as decompression.

### Data Structures and Algorithms

### 1. [Reconfiguring Multiple Connected Components with Size Multiset Constraints](http://arxiv.org/pdf/2505.07268v1)

Authors: Yu Nakahata

We propose a novel generalization of Independent Set Reconfiguration (ISR):
Connected Components Reconfiguration (CCR). In CCR, we are given a graph $G$,
two vertex subsets $A$ and $B$, and a multiset $\mathcal{M}$ of positive
integers. The question is whether $A$ and $B$ are reconfigurable under a
certain rule, while ensuring that each vertex subset induces connected
components whose sizes match the multiset $\mathcal{M}$. ISR is a special case
of CCR where $\mathcal{M}$ only contains 1. We also propose new reconfiguration
rules: component jumping (CJ) and component sliding (CS), which regard
connected components as tokens. Since CCR generalizes ISR, the problem is
PSPACE-complete. In contrast, we show three positive results: First, CCR-CS and
CCR-CJ are solvable in linear and quadratic time, respectively, when $G$ is a
path. Second, we show that CCR-CS is solvable in linear time for cographs.
Third, when $\mathcal{M}$ contains only the same elements (i.e., all connected
components have the same size), we show that CCR-CJ is solvable in linear time
if $G$ is chordal. The second and third results generalize known results for
ISR and exhibit an interesting difference between the reconfiguration rules.

### 2. [Improved Mixing of Critical Hardcore Model](http://arxiv.org/pdf/2505.07515v1)

Authors: Zongchen Chen, Tianhui Jiang

The hardcore model is one of the most classic and widely studied examples of
undirected graphical models. Given a graph $G$, the hardcore model describes a
Gibbs distribution of $\lambda$-weighted independent sets of $G$. In the last
two decades, a beautiful computational phase transition has been established at
a precise threshold $\lambda_c(\Delta)$ where $\Delta$ denotes the maximum
degree, where the task of sampling independent sets transfers from
polynomial-time solvable to computationally intractable. We study the critical
hardcore model where $\lambda = \lambda_c(\Delta)$ and show that the Glauber
dynamics, a simple yet popular Markov chain algorithm, mixes in
$\tilde{O}(n^{7.44 + O(1/\Delta)})$ time on any $n$-vertex graph of maximum
degree $\Delta\geq3$, significantly improving the previous upper bound
$\tilde{O}(n^{12.88+O(1/\Delta)})$ by the recent work arXiv:2411.03413. The
core property we establish in this work is that the critical hardcore model is
$O(\sqrt{n})$-spectrally independent, improving the trivial bound of $n$ and
matching the critical behavior of the Ising model. Our proof approach utilizes
an online decision-making framework to study a site percolation model on the
infinite $(\Delta-1)$-ary tree, which can be interesting by itself.

### 3. [Verified Purely Functional Catenable Real-Time Deques](http://arxiv.org/pdf/2505.07681v1)

Authors: Jules Viennot, Arthur Wendling, Armaël Guéneau, François Pottier

We present OCaml and Rocq implementations of Kaplan and Tarjan's purely
functional, real-time catenable deques. The correctness of our Rocq
implementation is machine-checked.

### 4. [Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning](http://arxiv.org/pdf/2505.07163v1)

Authors: Natalia G. Berloff

We present an exact spin-elimination technique that reduces the
dimensionality of both quadratic and k-local Ising Hamiltonians while
preserving their original ground-state configurations. By systematically
replacing each removed spin with an effective interaction among its neighbors,
our method lowers the total spin count without invoking approximations or
iterative recalculations. This capability is especially beneficial for
hardware-constrained platforms, classical or quantum, that can directly
implement multi-body interactions but have limited qubit or spin resources. We
demonstrate three key advances enabled by this technique. First, we handle
larger instances of benchmark problems such as Max-Cut on cubic graphs without
exceeding a 2-local interaction limit. Second, we reduce qubit requirements in
QAOA-based integer factorization on near-term quantum devices, thus extending
the feasible range of integers to be factorized. Third, we improve memory
capacity in Hopfield associative memories and enhance memory retrieval by
suppressing spurious attractors, enhancing retrieval performance. Our
spin-elimination procedure trades local spin complexity for higher-order
couplings or higher node degrees in a single pass, opening new avenues for
scaling up combinatorial optimization and energy-based machine learning on
near-term hardware. Finally, these results underscore that the next-generation
physical spin machines will likely capitalize on k-local spin Hamiltonians to
offer an alternative to classical computations.

### Emerging Technologies

### 1. [Empowering the Grid: Collaborative Edge Artificial Intelligence for Decentralized Energy Systems](http://arxiv.org/pdf/2505.07170v1)

Authors: Eddie de Paula Jr, Niel Bunda, Hezerul Abdul Karim, Nouar AlDahoul, Myles Joshua Toledo Tan

This paper examines how decentralized energy systems can be enhanced using
collaborative Edge Artificial Intelligence. Decentralized grids use local
renewable sources to reduce transmission losses and improve energy security.
Edge AI enables real-time, privacy-preserving data processing at the network
edge. Techniques such as federated learning and distributed control improve
demand response, equipment maintenance, and energy optimization. The paper
discusses key challenges including data privacy, scalability, and
interoperability, and suggests solutions such as blockchain integration and
adaptive architectures. Examples from virtual power plants and smart grids
highlight the potential of these technologies. The paper calls for increased
investment, policy support, and collaboration to advance sustainable energy
systems.

### 2. [Lagrange Oscillatory Neural Networks for Constraint Satisfaction and Optimization](http://arxiv.org/pdf/2505.07179v1)

Authors: Corentin Delacour, Bram Haverkort, Filip Sabo, Nadine Azemard, Aida Todri-Sanial

Physics-inspired computing paradigms are receiving renewed attention to
enhance efficiency in compute-intensive tasks such as artificial intelligence
and optimization. Similar to Hopfield neural networks, oscillatory neural
networks (ONNs) minimize an Ising energy function that embeds the solutions of
hard combinatorial optimization problems. Despite their success in solving
unconstrained optimization problems, Ising machines still face challenges with
constrained problems as they can get stuck at infeasible local minima. In this
paper, we introduce a Lagrange ONN (LagONN) designed to escape infeasible
states based on the theory of Lagrange multipliers. Unlike existing oscillatory
Ising machines, LagONN employs additional Lagrange oscillators to guide the
system towards feasible states in an augmented energy landscape and settles
only when constraints are met. Taking the maximum satisfiability problem with
three literals as a use case (Max-3-SAT), we harness LagONN's constraint
satisfaction mechanism to find optimal solutions for random SATlib instances
with up to 200 variables and 860 clauses, which provides a deterministic
alternative to simulated annealing for coupled oscillators. We further discuss
the potential of Lagrange oscillators to address other constraints, such as
phase copying, which is useful in oscillatory Ising machines with limited
connectivity.

### 3. [Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations](http://arxiv.org/pdf/2505.07711v1)

Authors: Pranav Sinha, Sumit Kumar Jha, Sunny Raj

We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where
quantum computers are limited by noisy gates, some of which are more
error-prone than others and can render the final computation incomprehensible.
Quantum circuit compilation algorithms attempt to minimize these noisy gates
when mapping quantum algorithms onto quantum hardware but face computational
challenges that restrict their application to circuits with no more than 5-6
qubits, necessitating the need to partition large circuits before the
application of noisy quantum gate minimization algorithms. The existing
generation of these algorithms is heuristic in nature and does not account for
downstream gate minimization tasks. Large language models (LLMs) have the
potential to change this and help improve quantum circuit partitions. This
paper investigates the use of LLMs, such as Llama and Mistral, for partitioning
quantum circuits by capitalizing on their abilities to understand and generate
code, including QASM. Specifically, we teach LLMs to partition circuits using
the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through
experimental evaluations, we show that careful fine-tuning of open source LLMs
enables us to obtain an accuracy of 53.4% for the partition task while
over-the-shelf LLMs are unable to correctly partition circuits, using standard
1-shot and few-shot training approaches.

### 4. [SmartUT: Receive Beamforming for Spectral Coexistence of NGSO Satellite Systems](http://arxiv.org/pdf/2505.07714v1)

Authors: Almoatssimbillah Saifaldawla, Eva Lagunas, Flor Ortiz, Abuzar B. M. Adam, Symeon Chatzinotas

In this paper, we investigate downlink co-frequency interference (CFI)
mitigation in non-geostationary satellites orbits (NGSOs) co-existing systems.
Traditional mitigation techniques, such as Zero-forcing (ZF), produce a null
towards the direction of arrivals (DOAs) of the interfering signals, but they
suffer from high computational complexity due to matrix inversions and required
knowledge of the channel state information (CSI). Furthermore, adaptive
beamformers, such as sample matrix inversion (SMI)-based minimum variance,
provide poor performance when the available snapshots are limited. We propose a
Mamba-based beamformer (MambaBF) that leverages an unsupervised deep learning
(DL) approach and can be deployed on the user terminal (UT) antenna array, for
assisting downlink beamforming and CFI mitigation using only a limited number
of available array snapshots as input, and without CSI knowledge. Simulation
results demonstrate that MambaBF consistently outperforms conventional
beamforming techniques in mitigating interference and maximizing the
signal-to-interference-plus-noise ratio (SINR), particularly under challenging
conditions characterized by low SINR, limited snapshots, and imperfect CSI.

### 5. [Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning](http://arxiv.org/pdf/2505.07163v1)

Authors: Natalia G. Berloff

We present an exact spin-elimination technique that reduces the
dimensionality of both quadratic and k-local Ising Hamiltonians while
preserving their original ground-state configurations. By systematically
replacing each removed spin with an effective interaction among its neighbors,
our method lowers the total spin count without invoking approximations or
iterative recalculations. This capability is especially beneficial for
hardware-constrained platforms, classical or quantum, that can directly
implement multi-body interactions but have limited qubit or spin resources. We
demonstrate three key advances enabled by this technique. First, we handle
larger instances of benchmark problems such as Max-Cut on cubic graphs without
exceeding a 2-local interaction limit. Second, we reduce qubit requirements in
QAOA-based integer factorization on near-term quantum devices, thus extending
the feasible range of integers to be factorized. Third, we improve memory
capacity in Hopfield associative memories and enhance memory retrieval by
suppressing spurious attractors, enhancing retrieval performance. Our
spin-elimination procedure trades local spin complexity for higher-order
couplings or higher node degrees in a single pass, opening new avenues for
scaling up combinatorial optimization and energy-based machine learning on
near-term hardware. Finally, these results underscore that the next-generation
physical spin machines will likely capitalize on k-local spin Hamiltonians to
offer an alternative to classical computations.

### Formal Languages and Automata Theory

### 1. [Reflexive Composition of Elementary State Machines, with an Application to the Reversal of Cellular Automata Rule 90](http://arxiv.org/pdf/2505.07186v2)

Authors: Chris Salzberg, Hiroki Sayama

We explore the dynamics of a one-dimensional lattice of state machines on two
states and two symbols sequentially updated via a process of "reflexive
composition." The space of 256 machines exhibits a variety of behavior,
including substitution, reversible "billiard ball" dynamics, and fractal
nesting. We show that one machine generates the Sierpinski Triangle and, for a
subset of boundary conditions, is isomorphic to cellular automata Rule 90 in
Wolfram's naming scheme. More surprisingly, two other machines follow
trajectories that map to Rule 90 in reverse. Whereas previous techniques have
been developed to uncover preimages of Rule 90, this is the first study to
produce such inverse dynamics naturally from the formalism itself. We argue
that the system's symmetric treatment of state and message underlies its
expressive power.

### 2. [The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games](http://arxiv.org/pdf/2505.07501v1)

Authors: Purandar Bhaduri

We study rational synthesis problems for concurrent games with
$\omega$-regular objectives. Our model of rationality considers only pure
strategy Nash equilibria that satisfy either a social welfare or Pareto
optimality condition with respect to an $\omega$-regular objective for each
agent. This extends earlier work on equilibria in concurrent games, without
consideration about their quality. Our results show that the existence of Nash
equilibria satisfying social welfare conditions can be computed as efficiently
as the constrained Nash equilibrium existence problem. On the other hand, the
existence of Nash equilibria satisfying the Pareto optimality condition
possibly involves a higher upper bound, except in the case of B\"uchi and
Muller games, for which all three problems are in the classes P and
PSPACE-complete, respectively.

### Computer Science and Game Theory

### 1. [Dynamic Rental Games with Stagewise Individual Rationality](http://arxiv.org/pdf/2505.07579v1)

Authors: Batya Berzack, Rotem Oshman, Inbal Talgam-Cohen

We study \emph{rental games} -- a single-parameter dynamic mechanism design
problem, in which a designer rents out an indivisible asset over $n$ days. Each
day, an agent arrives with a private valuation per day of rental, drawn from
that day's (known) distribution. The designer can either rent out the asset to
the current agent for any number of remaining days, charging them a (possibly
different) payment per day, or turn the agent away. Agents who arrive when the
asset is not available are turned away. A defining feature of our dynamic model
is that agents are \emph{stagewise-IR} (individually rational), meaning they
reject any rental agreement that results in temporary negative utility, even if
their final utility is positive. We ask whether and under which economic
objectives it is useful for the designer to exploit the stagewise-IR nature of
the agents.
  We show that an optimal rental mechanism can be modeled as a sequence of
dynamic auctions with seller costs. However, the stagewise-IR behavior of the
agents makes these auctions quite different from classical single-parameter
auctions: Myerson's Lemma does not apply, and indeed we show that truthful
mechanisms are not necessarily monotone, and payments do not necessarily follow
Myerson's unique payment rule. We develop alternative characterizations of
optimal mechanisms under several classes of economic objectives, including
generalizations of welfare, revenue and consumer surplus. These
characterizations allow us to use Myerson's unique payment rule in several
cases, and for the other cases we develop optimal mechanisms from scratch. Our
work shows that rental games raise interesting questions even in the
single-parameter regime.

### 2. [Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources](http://arxiv.org/pdf/2505.07688v1)

Authors: Renzhe Xu, Kang Wang, Bo Li

Data heterogeneity across multiple sources is common in real-world machine
learning (ML) settings. Although many methods focus on enabling a single model
to handle diverse data, real-world markets often comprise multiple competing ML
providers. In this paper, we propose a game-theoretic framework -- the
Heterogeneous Data Game -- to analyze how such providers compete across
heterogeneous data sources. We investigate the resulting pure Nash equilibria
(PNE), showing that they can be non-existent, homogeneous (all providers
converge on the same model), or heterogeneous (providers specialize in distinct
data sources). Our analysis spans monopolistic, duopolistic, and more general
markets, illustrating how factors such as the "temperature" of data-source
choice models and the dominance of certain data sources shape equilibrium
outcomes. We offer theoretical insights into both homogeneous and heterogeneous
PNEs, guiding regulatory policies and practical strategies for competitive ML
marketplaces.

### 3. [The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games](http://arxiv.org/pdf/2505.07501v1)

Authors: Purandar Bhaduri

We study rational synthesis problems for concurrent games with
$\omega$-regular objectives. Our model of rationality considers only pure
strategy Nash equilibria that satisfy either a social welfare or Pareto
optimality condition with respect to an $\omega$-regular objective for each
agent. This extends earlier work on equilibria in concurrent games, without
consideration about their quality. Our results show that the existence of Nash
equilibria satisfying social welfare conditions can be computed as efficiently
as the constrained Nash equilibrium existence problem. On the other hand, the
existence of Nash equilibria satisfying the Pareto optimality condition
possibly involves a higher upper bound, except in the case of B\"uchi and
Muller games, for which all three problems are in the classes P and
PSPACE-complete, respectively.

### Human-Computer Interaction

### 1. [Assessing the User Experience of Extended Reality Devices for (Dis)Assembly: A Classroom Study](http://arxiv.org/pdf/2505.07154v1)

Authors: Brandon S. Byers, Eleftherios Triantafyllidis, Thibaut Menny, Martin Schulte, Catherine De Wolf

Despite the current rise and promising capabilities of Extended Reality (XR)
technologies, the architecture, engineering, and construction industry lacks
informed guidance when choosing between these technologies, especially for
complex processes like assembly and disassembly tasks. This research compares
the user experience across different XR devices for (dis)assembly utilizing the
NASA Task Load Index and System Usability Scale metrics. Through a workshop and
surveys with graduate civil engineering and architecture students, the study
found that Augmented Reality scored highest in usability, followed closely by
Mixed Reality. However, Mixed Reality showed the best task load index score,
indicating low cognitive demand. The findings presented in this research may
aid academics and practitioners in making informed decisions when selecting XR
systems in practical, real-world assembly scenarios. Moreover, this study
suggests opportunities and guidelines for more detailed XR system comparisons
and exploration of XR's further role in circular construction practices.

### 2. [User Identification with LFI-Based Eye Movement Data Using Time and Frequency Domain Features](http://arxiv.org/pdf/2505.07326v1)

Authors: Suleyman Ozdel, Johannes Meyer, Yasmeen Abdrabou, Enkelejda Kasneci

Laser interferometry (LFI)-based eye-tracking systems provide an alternative
to traditional camera-based solutions, offering improved privacy by eliminating
the risk of direct visual identification. However, the high-frequency signals
captured by LFI-based trackers may still contain biometric information that
enables user identification. This study investigates user identification from
raw high-frequency LFI-based eye movement data by analyzing features extracted
from both the time and frequency domains. Using velocity and distance
measurements without requiring direct gaze data, we develop a multi-class
classification model to accurately distinguish between individuals across
various activities. Our results demonstrate that even without direct visual
cues, eye movement patterns exhibit sufficient uniqueness for user
identification, achieving 93.14% accuracy and a 2.52% EER with 5-second windows
across both static and dynamic tasks. Additionally, we analyze the impact of
sampling rate and window size on model performance, providing insights into the
feasibility of LFI-based biometric recognition. Our findings demonstrate the
novel potential of LFI-based eye-tracking for user identification, highlighting
both its promise for secure authentication and emerging privacy risks. This
work paves the way for further research into high-frequency eye movement data.

### 3. [Thalamus: A User Simulation Toolkit for Prototyping Multimodal Sensing Studies](http://arxiv.org/pdf/2505.07340v1)

Authors: Kayhan Latifzadeh, Luis A. Leiva

Conducting user studies that involve physiological and behavioral
measurements is very time-consuming and expensive, as it not only involves a
careful experiment design, device calibration, etc. but also a careful software
testing. We propose Thalamus, a software toolkit for collecting and simulating
multimodal signals that can help the experimenters to prepare in advance for
unexpected situations before reaching out to the actual study participants and
even before having to install or purchase a specific device. Among other
features, Thalamus allows the experimenter to modify, synchronize, and
broadcast physiological signals (as coming from various data streams) from
different devices simultaneously and not necessarily located in the same place.
Thalamus is cross-platform, cross-device, and simple to use, making it thus a
valuable asset for HCI research.

### 4. [Time Perception in Virtual Reality: Effects of Emotional Valence and Stress Level](http://arxiv.org/pdf/2505.07354v1)

Authors: Kyriaki Syrigou, Marina Stoforou, Panagiotis Kourtesis

Background & Objective: Emotional states and stress distort time perception,
yet findings are inconsistent, particularly in immersive media. Integrating the
Attentional Gate Model (AGM) and Internal Clock Model (ICM), we examined how
emotional valence and stress alter perceived duration in Virtual Reality (VR).
This study assesses the effects of valence (calming, neutral, stressful) and
stress (low/high) on prospective time estimation, mood, and arousal. Methods:
Fifty-four adults (18-39 years) explored three custom VR environments: (1) a
tranquil Japanese garden, (2) an affectively neutral room, and (3) a
threatening underground sewer. Active navigation promoted presence; a
distraction task separated conditions. Valence and arousal were assessed with
the Visual Analog Mood Scales, stress with the Perceived Stress Scale-10
(PSS-10), and perceived duration with a verbal estimation task. Mixed-model
ANOVAs evaluated main and interaction effects. Results: Valence reliably shaped
perceived duration: calming VR led to overestimation, stressful VR to
underestimation, and neutral VR to intermediate timing. Baseline stress level,
as measured by PSS-10, neither altered timing nor interacted with valence.
Nevertheless, the VR environments affected VAMS' mood metrics: calming
environments elevated mood and reduced perceived stress, whereas stressful
environments lowered mood and heightened stress. Conclusions: Findings support
the AGM-attentionally demanding negative environments shorten perceived
time-and the ICM-valence-linked arousal speeds or slows the pacemaker. Contrary
to classical predictions, in VR, baseline stress did not distort duration,
suggesting valence-driven attentional allocation outweighs pre-exposure stress
levels. VR offers a controllable platform for dissecting time-perception
mechanisms and advancing interventions that target emotion-related temporal
distortions.

### 5. [Shots and Boosters: Exploring the Use of Combined Prebunking Interventions to Raise Critical Thinking and Create Long-Term Protection Against Misinformation](http://arxiv.org/pdf/2505.07486v1)

Authors: Huiyun Tang, Anastasia Sergeeva

The problem of how to effectively mitigate the flow of misinformation remains
a significant challenge. The classical approach to this is public disapproval
of claims or "debunking." The approach is still widely used on social media,
but it has some severe limitations in terms of applicability and efficiency. An
alternative strategy is to enhance individuals' critical thinking through
educational interventions. Instead of merely disproving misinformation, these
approaches aim to strengthen users' reasoning skills, enabling them to evaluate
and reject false information independently. In this position paper, we explore
a combination of intervention methods designed to improve critical thinking in
the context of online media consumption. We highlight the role of AI in
supporting different stages of these interventions and present a design concept
that integrates AI-driven strategies to foster critical reasoning and media
literacy.

### 6. [Design Requirements for Patient-Centered Digital Health Applications: Supporting Patients' Values in Postoperative Delirium Prevention](http://arxiv.org/pdf/2505.07498v1)

Authors: David Leimstädtner, Fatima Halzl-Yürek, Claudia Spies, Claudia Müller-Birn

Postoperative delirium (POD) is among the most common complications after
surgeries for older adults and can entail long-term adverse health
consequences. Active patient participation in POD prevention presents a central
factor in reducing these risks. To support patient engagement through a digital
health application, we use value sensitive design approaches to identify the
requirements for a patient-centered digital health application supporting
patient engagement in POD prevention. Through interviews with medical
professionals and patient representatives, we construct a patient journey,
which serves as the basis for twelve patient value journey interviews. In these
interviews, patients from the high-risk group for POD revisit their recent
experience of undergoing surgery to elicit barriers, needs, and values
concerning POD prevention from a patient perspective. An analysis of the
patient interviews derives four design requirements for a digital health
application supporting patients regarding POD prevention: the adaptation of
patient-centered communication, the provision of procedural transparency,
fostering patient empowerment through consistent guidance, and explicitly
addressing relatives as mediators and supporters for a patient after a POD
occurrence.

### 7. [Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study](http://arxiv.org/pdf/2505.07592v1)

Authors: Matthew Russell, Samuel Youkeles, William Xia, Kenny Zheng, Aman Shah, Robert J. K. Jacob

While consumer-grade electroencephalography (EEG) devices show promise for
Brain-Computer Interface (BCI) applications, their efficacy in detecting subtle
cognitive states remains understudied. Using a combination of established
cognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel
ecological task (Chess puzzles), we demonstrate successful distinctions of
workload levels within some tasks, as well as differentiation between task
types using the MUSE 2 device. With machine learning we further show reliable
predictive power to differentiate between workload levels in the N-Back task,
while also achieving effective cross-task classification. These findings
demonstrate that consumer-grade EEG devices can effectively detect and
differentiate various forms of cognitive workload, and that they can be
leveraged with some success towards real-time classification distinguishing
workload in some tasks, as well as in differentiating between nuanced cognitive
states, supporting their potential use in adaptive BCI applications. Research
code and data are further provided for future researchers.

### 8. [VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections](http://arxiv.org/pdf/2505.07736v2)

Authors: Eason Chen, Xinyi Tang, Aprille Xi, Chenyu Lin, Conrad Borchers, Shivang Gupta, Jionghao Lin, Kenneth R Koedinger

Hybrid tutoring, where a human tutor supports multiple students in learning
with educational technology, is an increasingly common application to deliver
high-impact tutoring at scale. However, past hybrid tutoring applications are
limited in guiding tutor attention to students that require support.
Specifically, existing conferencing tools, commonly used in hybrid tutoring, do
not allow tutors to monitor multiple students' screens while directly
communicating and attending to multiple students simultaneously. To address
this issue, this paper introduces VTutor, a web-based platform leveraging
peer-to-peer screen sharing and virtual avatars to deliver real-time,
context-aware tutoring feedback at scale. By integrating a multi-student
monitoring dashboard with AI-powered avatar prompts, VTutor empowers a single
educator or tutor to rapidly detect off-task or struggling students and
intervene proactively, thus enhancing the benefits of one-on-one interactions
in classroom contexts with several students. Drawing on insight from the
learning sciences and past research on animated pedagogical agents, we
demonstrate how stylized avatars can potentially sustain student engagement
while accommodating varying infrastructure constraints. Finally, we address
open questions on refining large-scale, AI-driven tutoring solutions for
improved learner outcomes, and how VTutor could help interpret real-time
learner interactions to support remote tutors at scale. The VTutor platform can
be accessed at https://ls2025.vtutor.ai. The system demo video is at
https://ls2025.vtutor.ai/video.

### 9. [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](http://arxiv.org/pdf/2505.07161v1)

Authors: Jannatun Naim, Jie Cao, Fareen Tasneem, Jennifer Jacobs, Brent Milne, James Martin, Tamara Sumner

Effective feedback is essential for refining instructional practices in
mathematics education, and researchers often turn to advanced natural language
processing (NLP) models to analyze classroom dialogues from multiple
perspectives. However, utterance-level discourse analysis encounters two
primary challenges: (1) multifunctionality, where a single utterance may serve
multiple purposes that a single tag cannot capture, and (2) the exclusion of
many utterances from domain-specific discourse move classifications, leading to
their omission in feedback. To address these challenges, we proposed a
multi-perspective discourse analysis that integrates domain-specific talk moves
with dialogue act (using the flattened multi-functional SWBD-MASL schema with
43 tags) and discourse relation (applying Segmented Discourse Representation
Theory with 16 relations). Our top-down analysis framework enables a
comprehensive understanding of utterances that contain talk moves, as well as
utterances that do not contain talk moves. This is applied to two mathematics
education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through
distributional unigram analysis, sequential talk move analysis, and multi-view
deep dive, we discovered meaningful discourse patterns, and revealed the vital
role of utterances without talk moves, demonstrating that these utterances, far
from being mere fillers, serve crucial functions in guiding, acknowledging, and
structuring classroom discourse. These insights underscore the importance of
incorporating discourse relations and dialogue acts into AI-assisted education
systems to enhance feedback and create more responsive learning environments.
Our framework may prove helpful for providing human educator feedback, but also
aiding in the development of AI agents that can effectively emulate the roles
of both educators and students.

### 10. [A Turing Test for ''Localness'': Conceptualizing, Defining, and Recognizing Localness in People and Machines](http://arxiv.org/pdf/2505.07282v1)

Authors: Zihan Gao, Justin Cranshaw, Jacob Thebault-Spieker

As digital platforms increasingly mediate interactions tied to place,
ensuring genuine local participation is essential for maintaining trust and
credibility in location-based services, community-driven platforms, and civic
engagement systems. However, localness is a social and relational identity
shaped by knowledge, participation, and community recognition. Drawing on the
German philosopher Heidegger's concept of dwelling -- which extends beyond
physical presence to encompass meaningful connection to place -- we investigate
how people conceptualize and evaluate localness in both human and artificial
agents. Using a chat-based interaction paradigm inspired by Turing's Imitation
Game and Von Ahn's Games With A Purpose, we engaged 230 participants in
conversations designed to examine the cues people rely on to assess local
presence. Our findings reveal a multi-dimensional framework of localness,
highlighting differences in how locals and nonlocals emphasize various aspects
of local identity. We show that people are significantly more accurate in
recognizing locals than nonlocals, suggesting that localness is an affirmative
status requiring active demonstration rather than merely the absence of
nonlocal traits. Additionally, we identify conditions under which artificial
agents are perceived as local and analyze participants' sensemaking strategies
in evaluating localness. Through predictive modeling, we determine key factors
that drive accurate localness judgments. By bridging theoretical perspectives
on human-place relationships with practical challenges in digital environments,
our work informs the design of location-based services that foster meaningful
local engagement. Our findings contribute to a broader understanding of
localness as a dynamic and relational construct, reinforcing the importance of
dwelling as a process of belonging, recognition, and engagement with place.

### Information Retrieval

### 1. [A Generative Re-ranking Model for List-level Multi-objective Optimization at Taobao](http://arxiv.org/pdf/2505.07197v1)

Authors: Yue Meng, Cheng Guo, Yi Cao, Tong Liu, Bo Zheng

E-commerce recommendation systems aim to generate ordered lists of items for
customers, optimizing multiple business objectives, such as clicks, conversions
and Gross Merchandise Volume (GMV). Traditional multi-objective optimization
methods like formulas or Learning-to-rank (LTR) models take effect at
item-level, neglecting dynamic user intent and contextual item interactions.
List-level multi-objective optimization in the re-ranking stage can overcome
this limitation, but most current re-ranking models focus more on accuracy
improvement with context. In addition, re-ranking is faced with the challenges
of time complexity and diversity. In light of this, we propose a novel
end-to-end generative re-ranking model named Sequential Ordered Regression
Transformer-Generator (SORT-Gen) for the less-studied list-level
multi-objective optimization problem. Specifically, SORT-Gen is divided into
two parts: 1)Sequential Ordered Regression Transformer innovatively uses
Transformer and ordered regression to accurately estimate multi-objective
values for variable-length sub-lists. 2)Mask-Driven Fast Generation Algorithm
combines multi-objective candidate queues, efficient item selection and
diversity mechanism into model inference, providing a fast online list
generation method. Comprehensive online experiments demonstrate that SORT-Gen
brings +4.13% CLCK and +8.10% GMV for Baiyibutie, a notable Mini-app of Taobao.
Currently, SORT-Gen has been successfully deployed in multiple scenarios of
Taobao App, serving for a vast number of users.

### 2. [DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward](http://arxiv.org/pdf/2505.07257v1)

Authors: Yi Zhang, Ruihong Qiu, Xuwei Xu, Jiajun Liu, Sen Wang

Model-based offline reinforcement learning (RL) has emerged as a promising
approach for recommender systems, enabling effective policy learning by
interacting with frozen world models. However, the reward functions in these
world models, trained on sparse offline logs, often suffer from inaccuracies.
Specifically, existing methods face two major limitations in addressing this
challenge: (1) deterministic use of reward functions as static look-up tables,
which propagates inaccuracies during policy learning, and (2) static
uncertainty designs that fail to effectively capture decision risks and
mitigate the impact of these inaccuracies. In this work, a dual-agent
framework, DARLR, is proposed to dynamically update world models to enhance
recommendation policies. To achieve this, a \textbf{\textit{selector}} is
introduced to identify reference users by balancing similarity and diversity so
that the \textbf{\textit{recommender}} can aggregate information from these
users and iteratively refine reward estimations for dynamic reward shaping.
Further, the statistical features of the selected users guide the dynamic
adaptation of an uncertainty penalty to better align with evolving
recommendation requirements. Extensive experiments on four benchmark datasets
demonstrate the superior performance of DARLR, validating its effectiveness.
The code is available at https://github.com/ArronDZhang/DARLR.

### 3. [Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination Recommendation](http://arxiv.org/pdf/2505.07431v1)

Authors: Jianan Li, Yangtao Zhou, Zhifu Zhao, Qinglan Huang, Jian Qi, Xiao He, Hua Chu, Fu Li

Recommendation systems in AI-based medical diagnostics and treatment
constitute a critical component of AI in healthcare. Although some studies have
explored this area and made notable progress, healthcare recommendation systems
remain in their nascent stage. And these researches mainly target the treatment
process such as drug or disease recommendations. In addition to the treatment
process, the diagnostic process, particularly determining which medical
examinations are necessary to evaluate the condition, also urgently requires
intelligent decision support. To bridge this gap, we first formalize the task
of medical examination recommendations. Compared to traditional
recommendations, the medical examination recommendation involves more complex
interactions. This complexity arises from two folds: 1) The historical medical
records for examination recommendations are heterogeneous and redundant, which
makes the recommendation results susceptible to noise. 2) The correlation
between the medical history of patients is often irregular, making it
challenging to model spatiotemporal dependencies. Motivated by the above
observation, we propose a novel Diffusion-driven SpatioTemporal Graph
KANsformer for Medical Examination Recommendation (DST-GKAN) with a two-stage
learning paradigm to solve the above challenges. In the first stage, we exploit
a task-adaptive diffusion model to distill recommendation-oriented information
by reducing the noises in heterogeneous medical data. In the second stage, a
spatiotemporal graph KANsformer is proposed to simultaneously model the complex
spatial and temporal relationships. Moreover, to facilitate the medical
examination recommendation research, we introduce a comprehensive dataset. The
experimental results demonstrate the state-of-the-art performance of the
proposed method compared to various competitive baselines.

### 4. [Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic Analysis](http://arxiv.org/pdf/2505.07459v1)

Authors: Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi

Large Language Models (LLMs) are valued for their strong performance across
various tasks, but they also produce inaccurate or misleading outputs.
Uncertainty Estimation (UE) quantifies the model's confidence and helps users
assess response reliability. However, existing UE methods have not been
thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG),
where the input prompt includes non-parametric knowledge. This paper shows that
current UE methods cannot reliably assess correctness in the RAG setting. We
further propose an axiomatic framework to identify deficiencies in existing
methods and guide the development of improved approaches. Our framework
introduces five constraints that an effective UE method should meet after
incorporating retrieved documents into the LLM's prompt. Experimental results
reveal that no existing UE method fully satisfies all the axioms, explaining
their suboptimal performance in RAG. We further introduce a simple yet
effective calibration function based on our framework, which not only satisfies
more axioms than baseline methods but also improves the correlation between
uncertainty estimates and correctness.

### 5. [KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question Generation](http://arxiv.org/pdf/2505.07618v1)

Authors: Ching Han Chen, Ming Fang Shiu

KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation
(RAG) by explicitly tackling the two chronic weaknesses of current pipelines:
transparent multi-step reasoning and fine-grained cognitive difficulty control.
This transforms RAG from a passive retriever into an accountable generator of
calibrated exam items. Technically, the framework fuses knowledge graphs, RAG
retrieval, and educational assessment theory into a single pipeline. Domain
passages are parsed into a structured graph; graph-aware retrieval feeds fact
chains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels
and Item Response Theory (IRT) transforms those chains into psychometrically
sound questions. This cross-disciplinary marriage yields two scholarly
contributions: it shows how semantic graph contexts guide LLM reasoning paths,
and it operationalizes difficulty metrics within the generation process,
producing items whose IRT parameters match expert benchmarks. Every module,
from KG construction scripts to the multi-agent reasoning scheduler and the
automatic IRT validator, is openly released on GitHub. This enables peer
laboratories to replicate experiments, benchmark against baselines, and extend
individual components without licensing barriers. Its reproducible design paves
the way for rigorous ablation studies, cross-domain transfer experiments, and
shared leaderboards on multi-step reasoning benchmarks.

### 6. [Reproducibility, Replicability, and Insights into Visual Document Retrieval with Late Interaction](http://arxiv.org/pdf/2505.07730v1)

Authors: Jingfen Qiao, Jia-Huei Ju, Xinyu Ma, Evangelos Kanoulas, Andrew Yates

Visual Document Retrieval (VDR) is an emerging research area that focuses on
encoding and retrieving document images directly, bypassing the dependence on
Optical Character Recognition (OCR) for document search. A recent advance in
VDR was introduced by ColPali, which significantly improved retrieval
effectiveness through a late interaction mechanism. ColPali's approach
demonstrated substantial performance gains over existing baselines that do not
use late interaction on an established benchmark. In this study, we investigate
the reproducibility and replicability of VDR methods with and without late
interaction mechanisms by systematically evaluating their performance across
multiple pre-trained vision-language models. Our findings confirm that late
interaction yields considerable improvements in retrieval effectiveness;
however, it also introduces computational inefficiencies during inference.
Additionally, we examine the adaptability of VDR models to textual inputs and
assess their robustness across text-intensive datasets within the proposed
benchmark, particularly when scaling the indexing mechanism. Furthermore, our
research investigates the specific contributions of late interaction by looking
into query-patch matching in the context of visual document retrieval. We find
that although query tokens cannot explicitly match image patches as in the text
retrieval scenario, they tend to match the patch contains visually similar
tokens or their surrounding patches.

### 7. [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](http://arxiv.org/pdf/2505.07155v1)

Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon

Systematic reviews are comprehensive literature reviews that address highly
focused research questions and represent the highest form of evidence in
medicine. A critical step in this process is the development of complex Boolean
queries to retrieve relevant literature. Given the difficulty of manually
constructing these queries, recent efforts have explored Large Language Models
(LLMs) to assist in their formulation. One of the first studies,Wang et al.,
investigated ChatGPT for this task, followed by Staudinger et al., which
evaluated multiple LLMs in a reproducibility study. However, the latter
overlooked several key aspects of the original work, including (i) validation
of generated queries, (ii) output formatting constraints, and (iii) selection
of examples for chain-of-thought (Guided) prompting. As a result, its findings
diverged significantly from the original study. In this work, we systematically
reproduce both studies while addressing these overlooked factors. Our results
show that query effectiveness varies significantly across models and prompt
designs, with guided query formulation benefiting from well-chosen seed
studies. Overall, prompt design and model selection are key drivers of
successful query formulation. Our findings provide a clearer understanding of
LLMs' potential in Boolean query generation and highlight the importance of
model- and prompt-specific optimisations. The complex nature of systematic
reviews adds to challenges in both developing and reproducing methods but also
highlights the importance of reproducibility studies in this domain.

### 8. [Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition](http://arxiv.org/pdf/2505.07166v1)

Authors: Zheng Yao, Shuai Wang, Guido Zuccon

Dense retrievers utilize pre-trained backbone language models (e.g., BERT,
LLaMA) that are fine-tuned via contrastive learning to perform the task of
encoding text into sense representations that can be then compared via a
shallow similarity operation, e.g. inner product. Recent research has
questioned the role of fine-tuning vs. that of pre-training within dense
retrievers, specifically arguing that retrieval knowledge is primarily gained
during pre-training, meaning knowledge not acquired during pre-training cannot
be sub-sequentially acquired via fine-tuning. We revisit this idea here as the
claim was only studied in the context of a BERT-based encoder using DPR as
representative dense retriever. We extend the previous analysis by testing
other representation approaches (comparing the use of CLS tokens with that of
mean pooling), backbone architectures (encoder-only BERT vs. decoder-only
LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our
study confirms that in DPR tuning, pre-trained knowledge underpins retrieval
performance, with fine-tuning primarily adjusting neuron activation rather than
reorganizing knowledge. However, this pattern does not hold universally, such
as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full
reproducibility and make our implementation publicly available at
https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.

### 9. [ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion](http://arxiv.org/pdf/2505.07171v1)

Authors: Jeongho Kim, Chanyeong Heo, Jaehee Jung

Knowledge Graphs (KGs), composed of triples in the form of (head, relation,
tail) and consisting of entities and relations, play a key role in information
retrieval systems such as question answering, entity search, and
recommendation. In real-world KGs, although many entities exist, the relations
exhibit a long-tail distribution, which can hinder information retrieval
performance. Previous few-shot knowledge graph completion studies focused
exclusively on the positive triple information that exists in the graph or,
when negative triples were incorporated, used them merely as a signal to
indicate incorrect triples. To overcome this limitation, we propose
Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,
negative triples are generated by randomly replacing the tail entity in the
support set. By conditionally incorporating positive information in the KG and
non-existent negative information into the diffusion process, the model
separately estimates the latent distributions for positive and negative
relations. Moreover, including an attention pooler enables the model to
leverage the differences between positive and negative cases explicitly.
Experiments on two widely used datasets demonstrate that our method outperforms
existing approaches, achieving state-of-the-art performance. The code is
available at https://github.com/hou27/ReCDAP-FKGC.

### 10. [GRADA: Graph-based Reranker against Adversarial Documents Attack](http://arxiv.org/pdf/2505.07546v1)

Authors: Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, Qiongkai Xu

Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large
language models (LLMs) by integrating external knowledge from retrieved
documents, thereby overcoming the limitations of models' static intrinsic
knowledge. However, these systems are susceptible to adversarial attacks that
manipulate the retrieval process by introducing documents that are adversarial
yet semantically similar to the query. Notably, while these adversarial
documents resemble the query, they exhibit weak similarity to benign documents
in the retrieval set. Thus, we propose a simple yet effective Graph-based
Reranking against Adversarial Document Attacks (GRADA) framework aiming at
preserving retrieval quality while significantly reducing the success of
adversaries. Our study evaluates the effectiveness of our approach through
experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,
Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with
results from the Natural Questions dataset demonstrating up to an 80% reduction
in attack success rates while maintaining minimal loss in accuracy.

### Machine Learning

### 1. [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](http://arxiv.org/pdf/2505.07274v1)

Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma

Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

### 2. [Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and Adaptive Model-Metric Selection](http://arxiv.org/pdf/2505.07309v1)

Authors: Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin

Large language models (LLMs) often generate fluent but factually incorrect
outputs, known as hallucinations, which undermine their reliability in
real-world applications. While uncertainty estimation has emerged as a
promising strategy for detecting such errors, current metrics offer limited
interpretability and lack clarity about the types of uncertainty they capture.
In this paper, we present a systematic framework for decomposing LLM
uncertainty into four distinct sources, inspired by previous research. We
develop a source-specific estimation pipeline to quantify these uncertainty
types and evaluate how existing metrics relate to each source across tasks and
models. Our results show that metrics, task, and model exhibit systematic
variation in uncertainty characteristic. Building on this, we propose a method
for task specific metric/model selection guided by the alignment or divergence
between their uncertainty characteristics and that of a given task. Our
experiments across datasets and models demonstrate that our uncertainty-aware
selection strategy consistently outperforms baseline strategies, helping us
select appropriate models or uncertainty metrics, and contributing to more
reliable and efficient deployment in uncertainty estimation.

### 3. [From Search To Sampling: Generative Models For Robust Algorithmic Recourse](http://arxiv.org/pdf/2505.07351v1)

Authors: Prateek Garg, Lokesh Nagalapatti, Sunita Sarawagi

Algorithmic Recourse provides recommendations to individuals who are
adversely impacted by automated model decisions, on how to alter their profiles
to achieve a favorable outcome. Effective recourse methods must balance three
conflicting goals: proximity to the original profile to minimize cost,
plausibility for realistic recourse, and validity to ensure the desired
outcome. We show that existing methods train for these objectives separately
and then search for recourse through a joint optimization over the recourse
goals during inference, leading to poor recourse recommendations. We introduce
GenRe, a generative recourse model designed to train the three recourse
objectives jointly. Training such generative models is non-trivial due to lack
of direct recourse supervision. We propose efficient ways to synthesize such
supervision and further show that GenRe's training leads to a consistent
estimator. Unlike most prior methods, that employ non-robust gradient descent
based search during inference, GenRe simply performs a forward sampling over
the generative model to produce minimum cost recourse, leading to superior
performance across multiple metrics. We also demonstrate GenRe provides the
best trade-off between cost, plausibility and validity, compared to
state-of-art baselines. Our code is available at:
https://github.com/prateekgargx/genre.

### 4. [Adaptive Latent-Space Constraints in Personalized FL](http://arxiv.org/pdf/2505.07525v1)

Authors: Sana Ayromlou, D. B. Emerson

Federated learning (FL) has become an effective and widely used approach to
training deep learning models on decentralized datasets held by distinct
clients. FL also strengthens both security and privacy protections for training
data. Common challenges associated with statistical heterogeneity between
distributed datasets have spurred significant interest in personalized FL (pFL)
methods, where models combine aspects of global learning with local modeling
specific to each client's unique characteristics. In this work, the efficacy of
theoretically supported, adaptive MMD measures within the Ditto framework, a
state-of-the-art technique in pFL, are investigated. The use of such measures
significantly improves model performance across a variety of tasks, especially
those with pronounced feature heterogeneity. While the Ditto algorithm is
specifically considered, such measures are directly applicable to a number of
other pFL settings, and the results motivate the use of constraints tailored to
the various kinds of heterogeneity expected in FL systems.

### 5. [Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning](http://arxiv.org/pdf/2505.07527v1)

Authors: Hu Wang, Congbo Ma, Ian Reid, Mohammad Yaqub

Reward baseline is important for Reinforcement Learning (RL) algorithms to
reduce variance in policy gradient estimates. Recently, for language modeling,
Group Relative Policy Optimization (GRPO) is proposed to compute the advantage
for each output by subtracting the mean reward, as the baseline, for all
outputs in the group. However, it can lead to inaccurate advantage estimates in
environments with highly noisy rewards, potentially introducing bias. In this
work, we propose a model, called Kalman Filter Enhanced Group Relative Policy
Optimization (KRPO), by using lightweight Kalman filtering to dynamically
estimate the latent reward mean and variance. This filtering technique replaces
the naive batch mean baseline, enabling more adaptive advantage normalization.
Our method does not require additional learned parameters over GRPO. This
approach offers a simple yet effective way to incorporate multiple outputs of
GRPO into advantage estimation, improving policy optimization in settings where
highly dynamic reward signals are difficult to model for language models.
Through experiments and analyses, we show that using a more adaptive advantage
estimation model, KRPO can improve the stability and performance of GRPO. The
code is available at https://github.com/billhhh/KRPO_LLMs_RL

### 6. [Personalized Federated Learning under Model Dissimilarity Constraints](http://arxiv.org/pdf/2505.07575v1)

Authors: Samuel Erickson, Mikael Johansson

One of the defining challenges in federated learning is that of statistical
heterogeneity among clients. We address this problem with KARULA, a regularized
strategy for personalized federated learning, which constrains the pairwise
model dissimilarities between clients based on the difference in their
distributions, as measured by a surrogate for the 1-Wasserstein distance
adapted for the federated setting. This allows the strategy to adapt to highly
complex interrelations between clients, that e.g., clustered approaches fail to
capture. We propose an inexact projected stochastic gradient algorithm to solve
the constrained problem that the strategy defines, and show theoretically that
it converges with smooth, possibly non-convex losses to a neighborhood of a
stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA
on synthetic and real federated data sets.

### 7. [Enhancing Federated Learning with Kolmogorov-Arnold Networks: A Comparative Study Across Diverse Aggregation Strategies](http://arxiv.org/pdf/2505.07629v1)

Authors: Yizhou Ma, Zhuoqin Yang, Luis-Daniel Ibáñez

Multilayer Perceptron (MLP), as a simple yet powerful model, continues to be
widely used in classification and regression tasks. However, traditional MLPs
often struggle to efficiently capture nonlinear relationships in load data when
dealing with complex datasets. Kolmogorov-Arnold Networks (KAN), inspired by
the Kolmogorov-Arnold representation theorem, have shown promising capabilities
in modeling complex nonlinear relationships. In this study, we explore the
performance of KANs within federated learning (FL) frameworks and compare them
to traditional Multilayer Perceptrons. Our experiments, conducted across four
diverse datasets demonstrate that KANs consistently outperform MLPs in terms of
accuracy, stability, and convergence efficiency. KANs exhibit remarkable
robustness under varying client numbers and non-IID data distributions,
maintaining superior performance even as client heterogeneity increases.
Notably, KANs require fewer communication rounds to converge compared to MLPs,
highlighting their efficiency in FL scenarios. Additionally, we evaluate
multiple parameter aggregation strategies, with trimmed mean and FedProx
emerging as the most effective for optimizing KAN performance. These findings
establish KANs as a robust and scalable alternative to MLPs for federated
learning tasks, paving the way for their application in decentralized and
privacy-preserving environments.

### 8. [Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation](http://arxiv.org/pdf/2505.07674v1)

Authors: Nan Jiang, Wenxuan Zhu, Xu Han, Weiqiang Huang, Yumeng Sun

This study focuses on the challenge of predicting network traffic within
complex topological environments. It introduces a spatiotemporal modeling
approach that integrates Graph Convolutional Networks (GCN) with Gated
Recurrent Units (GRU). The GCN component captures spatial dependencies among
network nodes, while the GRU component models the temporal evolution of traffic
data. This combination allows for precise forecasting of future traffic
patterns. The effectiveness of the proposed model is validated through
comprehensive experiments on the real-world Abilene network traffic dataset.
The model is benchmarked against several popular deep learning methods.
Furthermore, a set of ablation experiments is conducted to examine the
influence of various components on performance, including changes in the number
of graph convolution layers, different temporal modeling strategies, and
methods for constructing the adjacency matrix. Results indicate that the
proposed approach achieves superior performance across multiple metrics,
demonstrating robust stability and strong generalization capabilities in
complex network traffic forecasting scenarios.

### 9. [Assessing the Chemical Intelligence of Large Language Models](http://arxiv.org/pdf/2505.07735v1)

Authors: Nicholas T. Runcie, Charlotte M. Deane, Fergus Imrie

Large Language Models are versatile, general-purpose tools with a wide range
of applications. Recently, the advent of "reasoning models" has led to
substantial improvements in their abilities in advanced problem-solving domains
such as mathematics and software engineering. In this work, we assessed the
ability of reasoning models to directly perform chemistry tasks, without any
assistance from external tools. We created a novel benchmark, called ChemIQ,
which consists of 796 questions assessing core concepts in organic chemistry,
focused on molecular comprehension and chemical reasoning. Unlike previous
benchmarks, which primarily use multiple choice formats, our approach requires
models to construct short-answer responses, more closely reflecting real-world
applications. The reasoning models, exemplified by OpenAI's o3-mini, correctly
answered 28%-59% of questions depending on the reasoning level used, with
higher reasoning levels significantly increasing performance on all tasks.
These models substantially outperformed the non-reasoning model, GPT-4o, which
achieved only 7% accuracy. We found that Large Language Models can now convert
SMILES strings to IUPAC names, a task earlier models were unable to perform.
Additionally, we show that the latest reasoning models can elucidate structures
from 1H and 13C NMR data, correctly generating SMILES strings for 74% of
molecules containing up to 10 heavy atoms, and in one case solving a structure
comprising 21 heavy atoms. For each task, we found evidence that the reasoning
process mirrors that of a human chemist. Our results demonstrate that the
latest reasoning models have the ability to perform advanced chemical
reasoning.

### 10. [The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong](http://arxiv.org/pdf/2505.07750v1)

Authors: Gašper Petelin, Gjorgjina Cenikj

Algorithm selection, aiming to identify the best algorithm for a given
problem, plays a pivotal role in continuous black-box optimization. A common
approach involves representing optimization functions using a set of features,
which are then used to train a machine learning meta-model for selecting
suitable algorithms. Various approaches have demonstrated the effectiveness of
these algorithm selection meta-models. However, not all evaluation approaches
are equally valid for assessing the performance of meta-models. We highlight
methodological issues that frequently occur in the community and should be
addressed when evaluating algorithm selection approaches. First, we identify
flaws with the "leave-instance-out" evaluation technique. We show that
non-informative features and meta-models can achieve high accuracy, which
should not be the case with a well-designed evaluation framework. Second, we
demonstrate that measuring the performance of optimization algorithms with
metrics sensitive to the scale of the objective function requires careful
consideration of how this impacts the construction of the meta-model, its
predictions, and the model's error. Such metrics can falsely present overly
optimistic performance assessments of the meta-models. This paper emphasizes
the importance of careful evaluation, as loosely defined methodologies can
mislead researchers, divert efforts, and introduce noise into the field

### Neural and Evolutionary Computing

### 1. [Lagrange Oscillatory Neural Networks for Constraint Satisfaction and Optimization](http://arxiv.org/pdf/2505.07179v1)

Authors: Corentin Delacour, Bram Haverkort, Filip Sabo, Nadine Azemard, Aida Todri-Sanial

Physics-inspired computing paradigms are receiving renewed attention to
enhance efficiency in compute-intensive tasks such as artificial intelligence
and optimization. Similar to Hopfield neural networks, oscillatory neural
networks (ONNs) minimize an Ising energy function that embeds the solutions of
hard combinatorial optimization problems. Despite their success in solving
unconstrained optimization problems, Ising machines still face challenges with
constrained problems as they can get stuck at infeasible local minima. In this
paper, we introduce a Lagrange ONN (LagONN) designed to escape infeasible
states based on the theory of Lagrange multipliers. Unlike existing oscillatory
Ising machines, LagONN employs additional Lagrange oscillators to guide the
system towards feasible states in an augmented energy landscape and settles
only when constraints are met. Taking the maximum satisfiability problem with
three literals as a use case (Max-3-SAT), we harness LagONN's constraint
satisfaction mechanism to find optimal solutions for random SATlib instances
with up to 200 variables and 860 clauses, which provides a deterministic
alternative to simulated annealing for coupled oscillators. We further discuss
the potential of Lagrange oscillators to address other constraints, such as
phase copying, which is useful in oscillatory Ising machines with limited
connectivity.

### 2. [The Influence of the Memory Capacity of Neural DDEs on the Universal Approximation Property](http://arxiv.org/pdf/2505.07244v1)

Authors: Christian Kuehn, Sara-Viola Kuntz

Neural Ordinary Differential Equations (Neural ODEs), which are the
continuous-time analog of Residual Neural Networks (ResNets), have gained
significant attention in recent years. Similarly, Neural Delay Differential
Equations (Neural DDEs) can be interpreted as an infinite depth limit of
Densely Connected Residual Neural Networks (DenseResNets). In contrast to
traditional ResNet architectures, DenseResNets are feed-forward networks that
allow for shortcut connections across all layers. These additional connections
introduce memory in the network architecture, as typical in many modern
architectures. In this work, we explore how the memory capacity in neural DDEs
influences the universal approximation property. The key parameter for studying
the memory capacity is the product $K \tau$ of the Lipschitz constant and the
delay of the DDE. In the case of non-augmented architectures, where the network
width is not larger than the input and output dimensions, neural ODEs and
classical feed-forward neural networks cannot have the universal approximation
property. We show that if the memory capacity $K\tau$ is sufficiently small,
the dynamics of the neural DDE can be approximated by a neural ODE.
Consequently, non-augmented neural DDEs with a small memory capacity also lack
the universal approximation property. In contrast, if the memory capacity
$K\tau$ is sufficiently large, we can establish the universal approximation
property of neural DDEs for continuous functions. If the neural DDE
architecture is augmented, we can expand the parameter regions in which
universal approximation is possible. Overall, our results show that by
increasing the memory capacity $K\tau$, the infinite-dimensional phase space of
DDEs with positive delay $\tau>0$ is not sufficient to guarantee a direct jump
transition to universal approximation, but only after a certain memory
threshold, universal approximation holds.

### Networking and Internet Architecture

### 1. [Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE 802.11 MAPC Networks](http://arxiv.org/pdf/2505.07278v2)

Authors: Maksymilian Wojnar, Wojciech Ciężobka, Artur Tomaszewski, Piotr Chołda, Krzysztof Rusek, Katarzyna Kosek-Szott, Jetmir Haxhibeqiri, Jeroen Hoebeke, Boris Bellalta, Anatolij Zubow, Falko Dressler, Szymon Szott

The densification of Wi-Fi deployments means that fully distributed random
channel access is no longer sufficient for high and predictable performance.
Therefore, the upcoming IEEE 802.11bn amendment introduces multi-access point
coordination (MAPC) methods. This paper addresses a variant of MAPC called
coordinated spatial reuse (C-SR), where devices transmit simultaneously on the
same channel, with the power adjusted to minimize interference. The C-SR
scheduling problem is selecting which devices transmit concurrently and with
what settings. We provide a theoretical upper bound model, optimized for either
throughput or fairness, which finds the best possible transmission schedule
using mixed-integer linear programming. Then, a practical, probing-based
approach is proposed which uses multi-armed bandits (MABs), a type of
reinforcement learning, to solve the C-SR scheduling problem. We validate both
classical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and
in a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy
IEEE 802.11 (on average by 80\% in random scenarios), without reducing the
number of transmission opportunities per station. Finally, our framework is
lightweight and ready for implementation in Wi-Fi devices.

### 2. [Multi-Agent DRL for Multi-Objective Twin Migration Routing with Workload Prediction in 6G-enabled IoV](http://arxiv.org/pdf/2505.07290v1)

Authors: Peng Yin, Wentao Liang, Jinbo Wen, Jiawen Kang, Junlong Chen, Dusit Niyato

Sixth Generation (6G)-enabled Internet of Vehicles (IoV) facilitates
efficient data synchronization through ultra-fast bandwidth and high-density
connectivity, enabling the emergence of Vehicle Twins (VTs). As highly accurate
replicas of vehicles, VTs can support intelligent vehicular applications for
occupants in 6G-enabled IoV. Thanks to the full coverage capability of 6G,
resource-constrained vehicles can offload VTs to edge servers, such as roadside
units, unmanned aerial vehicles, and satellites, utilizing their computing and
storage resources for VT construction and updates. However, communication
between vehicles and edge servers with limited coverage is prone to
interruptions due to the dynamic mobility of vehicles. Consequently, VTs must
be migrated among edge servers to maintain uninterrupted and high-quality
services for users. In this paper, we introduce a VT migration framework in
6G-enabled IoV. Specifically, we first propose a Long Short-Term Memory
(LSTM)-based Transformer model to accurately predict long-term workloads of
edge servers for migration decision-making. Then, we propose a Dynamic Mask
Multi-Agent Proximal Policy Optimization (DM-MAPPO) algorithm to identify
optimal migration routes in the highly complex environment of 6G-enabled IoV.
Finally, we develop a practical platform to validate the effectiveness of the
proposed scheme using real datasets. Simulation results demonstrate that the
proposed DM-MAPPO algorithm significantly reduces migration latency by 20.82%
and packet loss by 75.07% compared with traditional deep reinforcement learning
algorithms.

### 3. [Routing Attacks in Ethereum PoS: A Systematic Exploration](http://arxiv.org/pdf/2505.07713v1)

Authors: Constantine Doumanidis, Maria Apostolaki

With the promise of greater decentralization and sustainability, Ethereum
transitioned from a Proof-of-Work (PoW) to a Proof-of-Stake (PoS) consensus
mechanism. The new consensus protocol introduces novel vulnerabilities that
warrant further investigation. The goal of this paper is to investigate the
security of Ethereum's PoS system from an Internet routing perspective.
  To this end, this paper makes two contributions: First, we devise a novel
framework for inferring the distribution of validators on the Internet without
disturbing the real network. Second, we introduce a class of network-level
attacks on Ethereum's PoS system that jointly exploit Internet routing
vulnerabilities with the protocol's reward and penalty mechanisms. We describe
two representative attacks: StakeBleed, where the attacker triggers an
inactivity leak, halting block finality and causing financial losses for all
validators; and KnockBlock, where the attacker increases her expected MEV gains
by preventing targeted blocks from being included in the chain. We find that
both attacks are practical and effective. An attacker executing StakeBleed can
inflict losses of almost 300 ETH in just 2 hours by hijacking as few as 30 IP
prefixes. An attacker implementing KnockBlock could increase their MEV expected
gains by 44.5% while hijacking a single prefix for less than 2 minutes.
  Our paper serves as a call to action for validators to reinforce their
Internet routing infrastructure and for the Ethereum P2P protocol to implement
stronger mechanisms to conceal validator locations.

### 4. [BBR's Sharing Behavior with CUBIC and Reno](http://arxiv.org/pdf/2505.07741v1)

Authors: Fatih Berkay Sarpkaya, Ashutosh Srivastava, Fraida Fund, Shivendra Panwar

TCP BBR's behavior has been explained by various theoretical models, and in
particular those that describe how it co-exists with other types of flows.
However, as new versions of the BBR protocol have emerged, it remains unclear
to what extent the high-level behaviors described by these models apply to the
newer versions. In this paper, we systematically evaluate the most influential
steady-state and fluid models describing BBR's coexistence with loss-based
flows over shared bottleneck links. Our experiments, conducted on a new
experimental platform (FABRIC), extend previous evaluations to additional
network scenarios, enabling comparisons between the two models and include the
recently introduced BBRv3. Our findings confirm that the steady-state model
accurately captures BBRv1 behavior, especially against single loss-based flows.
The fluid model successfully captures several key behaviors of BBRv1 and BBRv2
but shows limitations, in scenarios involving deep buffers, large numbers of
flows, or intra-flow fairness. Importantly, we observe clear discrepancies
between existing model predictions and BBRv3 behavior, suggesting the need for
an updated or entirely new modeling approach for this latest version. We hope
these results validate and strengthen the research community's confidence in
these models and identify scenarios where they do not apply.

### 5. [Assessing the Latency of Network Layer Security in 5G Networks](http://arxiv.org/pdf/2505.07328v1)

Authors: Sotiris Michaelides, Jonathan Mucke, Martin Henze

In contrast to its predecessors, 5G supports a wide range of commercial,
industrial, and critical infrastructure scenarios. One key feature of 5G,
ultra-reliable low latency communication, is particularly appealing to such
scenarios for its real-time capabilities. However, 5G's enhanced security,
mostly realized through optional security controls, imposes additional overhead
on the network performance, potentially hindering its real-time capabilities.
To better assess this impact and guide operators in choosing between different
options, we measure the latency overhead of IPsec when applied over the N3 and
the service-based interfaces to protect user and control plane data,
respectively. Furthermore, we evaluate whether WireGuard constitutes an
alternative to reduce this overhead. Our findings show that IPsec, if
configured correctly, has minimal latency impact and thus is a prime candidate
to secure real-time critical scenarios.

### 6. [Synthesizing Diverse Network Flow Datasets with Scalable Dynamic Multigraph Generation](http://arxiv.org/pdf/2505.07777v1)

Authors: Arya Grayeli, Vipin Swarup, Steven E. Noel

Obtaining real-world network datasets is often challenging because of
privacy, security, and computational constraints. In the absence of such
datasets, graph generative models become essential tools for creating synthetic
datasets. In this paper, we introduce a novel machine learning model for
generating high-fidelity synthetic network flow datasets that are
representative of real-world networks. Our approach involves the generation of
dynamic multigraphs using a stochastic Kronecker graph generator for structure
generation and a tabular generative adversarial network for feature generation.
We further employ an XGBoost (eXtreme Gradient Boosting) model for graph
alignment, ensuring accurate overlay of features onto the generated graph
structure. We evaluate our model using new metrics that assess both the
accuracy and diversity of the synthetic graphs. Our results demonstrate
improvements in accuracy over previous large-scale graph generation methods
while maintaining similar efficiency. We also explore the trade-off between
accuracy and diversity in synthetic graph dataset creation, a topic not
extensively covered in related works. Our contributions include the synthesis
and evaluation of large real-world netflow datasets and the definition of new
metrics for evaluating synthetic graph generative models.

### Robotics

### 1. [A Framework for Joint Grasp and Motion Planning in Confined Spaces](http://arxiv.org/pdf/2505.07259v1)

Authors: Martin Rudorfer, Jiří Hartvich, Vojtěch Vonásek

Robotic grasping is a fundamental skill across all domains of robot
applications. There is a large body of research for grasping objects in
table-top scenarios, where finding suitable grasps is the main challenge. In
this work, we are interested in scenarios where the objects are in confined
spaces and hence particularly difficult to reach. Planning how the robot
approaches the object becomes a major part of the challenge, giving rise to
methods for joint grasp and motion planning. The framework proposed in this
paper provides 20 benchmark scenarios with systematically increasing
difficulty, realistic objects with precomputed grasp annotations, and tools to
create and share more scenarios. We further provide two baseline planners and
evaluate them on the scenarios, demonstrating that the proposed difficulty
levels indeed offer a meaningful progression. We invite the research community
to build upon this framework by making all components publicly available as
open source.

### 2. [BETTY Dataset: A Multi-modal Dataset for Full-Stack Autonomy](http://arxiv.org/pdf/2505.07266v1)

Authors: Micah Nye, Ayoub Raji, Andrew Saba, Eidan Erlich, Robert Exley, Aragya Goyal, Alexander Matros, Ritesh Misra, Matthew Sivaprakasam, Marko Bertogna, Deva Ramanan, Sebastian Scherer

We present the BETTY dataset, a large-scale, multi-modal dataset collected on
several autonomous racing vehicles, targeting supervised and self-supervised
state estimation, dynamics modeling, motion forecasting, perception, and more.
Existing large-scale datasets, especially autonomous vehicle datasets, focus
primarily on supervised perception, planning, and motion forecasting tasks. Our
work enables multi-modal, data-driven methods by including all sensor inputs
and the outputs from the software stack, along with semantic metadata and
ground truth information. The dataset encompasses 4 years of data, currently
comprising over 13 hours and 32TB, collected on autonomous racing vehicle
platforms. This data spans 6 diverse racing environments, including high-speed
oval courses, for single and multi-agent algorithm evaluation in feature-sparse
scenarios, as well as high-speed road courses with high longitudinal and
lateral accelerations and tight, GPS-denied environments. It captures highly
dynamic states, such as 63 m/s crashes, loss of tire traction, and operation at
the limit of stability. By offering a large breadth of cross-modal and dynamic
data, the BETTY dataset enables the training and testing of full autonomy stack
pipelines, pushing the performance of all algorithms to the limits. The current
dataset is available at https://pitt-mit-iac.github.io/betty-dataset/.

### 3. [Autonomous Robotic Pruning in Orchards and Vineyards: a Review](http://arxiv.org/pdf/2505.07318v1)

Authors: Alessandro Navone, Mauro Martini, Marcello Chiaberge

Manual pruning is labor intensive and represents up to 25% of annual labor
costs in fruit production, notably in apple orchards and vineyards where
operational challenges and cost constraints limit the adoption of large-scale
machinery. In response, a growing body of research is investigating compact,
flexible robotic platforms capable of precise pruning in varied terrains,
particularly where traditional mechanization falls short.
  This paper reviews recent advances in autonomous robotic pruning for orchards
and vineyards, addressing a critical need in precision agriculture. Our review
examines literature published between 2014 and 2024, focusing on innovative
contributions across key system components. Special attention is given to
recent developments in machine vision, perception, plant skeletonization, and
control strategies, areas that have experienced significant influence from
advancements in artificial intelligence and machine learning. The analysis
situates these technological trends within broader agricultural challenges,
including rising labor costs, a decline in the number of young farmers, and the
diverse pruning requirements of different fruit species such as apple,
grapevine, and cherry trees.
  By comparing various robotic architectures and methodologies, this survey not
only highlights the progress made toward autonomous pruning but also identifies
critical open challenges and future research directions. The findings
underscore the potential of robotic systems to bridge the gap between manual
and mechanized operations, paving the way for more efficient, sustainable, and
precise agricultural practices.

### 4. [Drive Fast, Learn Faster: On-Board RL for High Performance Autonomous Racing](http://arxiv.org/pdf/2505.07321v1)

Authors: Benedict Hildisch, Edoardo Ghignone, Nicolas Baumann, Cheng Hu, Andrea Carron, Michele Magno

Autonomous racing presents unique challenges due to its non-linear dynamics,
the high speed involved, and the critical need for real-time decision-making
under dynamic and unpredictable conditions. Most traditional Reinforcement
Learning (RL) approaches rely on extensive simulation-based pre-training, which
faces crucial challenges in transfer effectively to real-world environments.
This paper introduces a robust on-board RL framework for autonomous racing,
designed to eliminate the dependency on simulation-based pre-training enabling
direct real-world adaptation. The proposed system introduces a refined Soft
Actor-Critic (SAC) algorithm, leveraging a residual RL structure to enhance
classical controllers in real-time by integrating multi-step
Temporal-Difference (TD) learning, an asynchronous training pipeline, and
Heuristic Delayed Reward Adjustment (HDRA) to improve sample efficiency and
training stability. The framework is validated through extensive experiments on
the F1TENTH racing platform, where the residual RL controller consistently
outperforms the baseline controllers and achieves up to an 11.5 % reduction in
lap times compared to the State-of-the-Art (SotA) with only 20 min of training.
Additionally, an End-to-End (E2E) RL controller trained without a baseline
controller surpasses the previous best results with sustained on-track
learning. These findings position the framework as a robust solution for
high-performance autonomous racing and a promising direction for other
real-time, dynamic autonomous systems.

### 5. [ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning](http://arxiv.org/pdf/2505.07395v1)

Authors: Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang

Vision-Language-Action (VLA) models have shown great potential in general
robotic decision-making tasks via imitation learning. However, the variable
quality of training data often constrains the performance of these models. On
the other hand, offline Reinforcement Learning (RL) excels at learning robust
policy models from mixed-quality data. In this paper, we introduce Reinforced
robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL
principle of maximizing cumulative reward. ReinboT achieves a deeper
understanding of the data quality distribution by predicting dense returns that
capture the nuances of manipulation tasks. The dense return prediction
capability enables the robot to generate more robust decision-making actions,
oriented towards maximizing future benefits. Extensive experiments show that
ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality
dataset and exhibits superior few-shot learning and out-of-distribution
generalization capabilities in real-world tasks.

### 6. [Cooperative Assembly with Autonomous Mobile Manipulators in an Underwater Scenario](http://arxiv.org/pdf/2505.07441v1)

Authors: Davide Torielli

[...] Specifically, the problem addressed is an assembly one known as the
peg-in-hole task. In this case, two autonomous manipulators must carry
cooperatively (at kinematic level) a peg and must insert it into an hole fixed
in the environment. Even if the peg-in-hole is a well-known problem, there are
no specific studies related to the use of two different autonomous
manipulators, especially in underwater scenarios. Among all the possible
investigations towards the problem, this work focuses mainly on the kinematic
control of the robots. The methods used are part of the Task Priority Inverse
Kinematics (TPIK) approach, with a cooperation scheme that permits to exchange
as less information as possible between the agents (that is really important
being water a big impediment for communication). A force-torque sensor is
exploited at kinematic level to help the insertion phase. The results show how
the TPIK and the chosen cooperation scheme can be used for the stated problem.
The simulated experiments done consider little errors in the hole's pose, that
still permit to insert the peg but with a lot of frictions and possible stucks.
It is shown how can be possible to improve (thanks to the data provided by the
force-torque sensor) the insertion phase performed by the two manipulators in
presence of these errors. [...]

### 7. [TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking](http://arxiv.org/pdf/2505.07446v1)

Authors: Hanjing Ye, Yu Zhan, Weixi Situ, Guangcheng Chen, Jingwen Yu, Kuanqi Cai, Hong Zhang

Tracking a target person from robot-egocentric views is crucial for
developing autonomous robots that provide continuous personalized assistance or
collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most
existing target person tracking (TPT) benchmarks are limited to controlled
laboratory environments with few distractions, clean backgrounds, and
short-term occlusions. In this paper, we introduce a large-scale dataset
designed for TPT in crowded and unstructured environments, demonstrated through
a robot-person following task. The dataset is collected by a human pushing a
sensor-equipped cart while following a target person, capturing human-like
following behavior and emphasizing long-term tracking challenges, including
frequent occlusions and the need for re-identification from numerous
pedestrians. It includes multi-modal data streams, including odometry, 3D
LiDAR, IMU, panoptic, and RGB-D images, along with exhaustively annotated 2D
bounding boxes of the target person across 35 sequences, both indoors and
outdoors. Using this dataset and visual annotations, we perform extensive
experiments with existing TPT methods, offering a thorough analysis of their
limitations and suggesting future research directions.

### 8. [GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion](http://arxiv.org/pdf/2505.07455v1)

Authors: Shulong Jiang, Shiqi Zhao, Yuxuan Fan, Peng Yin

Visuotactile sensing offers rich contact information that can help mitigate
performance bottlenecks in imitation learning, particularly under
vision-limited conditions, such as ambiguous visual cues or occlusions.
Effectively fusing visual and visuotactile modalities, however, presents
ongoing challenges. We introduce GelFusion, a framework designed to enhance
policies by integrating visuotactile feedback, specifically from
high-resolution GelSight sensors. GelFusion using a vision-dominated
cross-attention fusion mechanism incorporates visuotactile information into
policy learning. To better provide rich contact information, the framework's
core component is our dual-channel visuotactile feature representation,
simultaneously leveraging both texture-geometric and dynamic interaction
features. We evaluated GelFusion on three contact-rich tasks: surface wiping,
peg insertion, and fragile object pick-and-place. Outperforming baselines,
GelFusion shows the value of its structure in improving the success rate of
policy learning.

### 9. [Average-Reward Maximum Entropy Reinforcement Learning for Global Policy in Double Pendulum Tasks](http://arxiv.org/pdf/2505.07516v1)

Authors: Jean Seong Bjorn Choe, Bumkyu Choi, Jong-kook Kim

This report presents our reinforcement learning-based approach for the
swing-up and stabilisation tasks of the acrobot and pendubot, tailored
specifcially to the updated guidelines of the 3rd AI Olympics at ICRA 2025.
Building upon our previously developed Average-Reward Entropy Advantage Policy
Optimization (AR-EAPO) algorithm, we refined our solution to effectively
address the new competition scenarios and evaluation metrics. Extensive
simulations validate that our controller robustly manages these revised tasks,
demonstrating adaptability and effectiveness within the updated framework.

### 10. [On rapid parallel tuning of controllers of a swarm of MAVs -- distribution strategies of the updated gains](http://arxiv.org/pdf/2505.07523v1)

Authors: Dariusz Horla, Wojciech Giernacki, Vít Krátký, Petr Štibinger, Tomáš Báča, Martin Saska

In this paper, we present a reliable, scalable, time deterministic,
model-free procedure to tune swarms of Micro Aerial Vehicles (MAVs) using basic
sensory data. Two approaches to taking advantage of parallel tuning are
presented. First, the tuning with averaging of the results on the basis of
performance indices reported from the swarm with identical gains to decrease
the negative effect of the noise in the measurements. Second, the tuning with
parallel testing of varying set of gains across the swarm to reduce the tuning
time. The presented methods were evaluated both in simulation and real-world
experiments. The achieved results show the ability of the proposed approach to
improve the results of the tuning while decreasing the tuning time, ensuring at
the same time a reliable tuning mechanism.

### Software Engineering

### 1. [An Empirical Study: MEMS as a Static Performance Metric](http://arxiv.org/pdf/2505.07208v1)

Authors: Liwei Zhang, Baoquan Cui, Xutong Ma, Jian Zhang

Static performance estimation is essential during compile-time analysis, yet
traditional runtime-based methods are costly and platform-dependent. We
investigate mems, the number of memory accesses, as a static and
architecture-independent performance metric. We develop a Clang-based automated
instrumentation tool that rewrites source code to insert path tracing and
\textit{mems} counting logic. This allows us to evaluate mems-based performance
estimation across ten classical algorithm programs. Experimental results show
that within the same program, execution paths with higher mems values
consistently exhibit longer runtime. However, this correlation weakens between
different programs, suggesting that mems is best suited for comparing
performance of different execution paths in a program.

### 2. [Automated Repair of Ambiguous Natural Language Requirements](http://arxiv.org/pdf/2505.07270v1)

Authors: Haoxiang Jia, Robbie Morris, He Ye, Federica Sarro, Sergey Mechtaev

The rise of large language models (LLMs) has amplified the role of natural
language (NL) in software engineering, and its inherent ambiguity and
susceptibility to misinterpretation pose a fundamental challenge for software
quality, because employing ambiguous requirements may result in the generation
of faulty programs. The complexity of ambiguity detection and resolution
motivates us to introduce the problem of automated repair of ambiguous NL
requirements.
  Repairing ambiguity in requirements poses a challenge for LLMs, as it demands
a metacognitive capability - the ability to reflect on how alterations to the
text influence their own interpretation of this text. Indeed, our experiments
show that directly prompting an LLM to detect and resolve ambiguities results
in irrelevant or inconsistent clarifications. Our key novelty is in decomposing
this problem into simpler subproblems which do not require metacognitive
reasoning. First, we analyze and repair LLM's interpretation of requirements
embodied in the distribution of programs they induce using traditional testing
and program repair methods. Second, we repair requirements based on the changes
to the distribution via what we refer to as contractive specification
inference. This decomposition enables targeted, minimal requirement repairs
that yield cross-model performance gains in code generation.
  We implemented this approach in a tool SpecFix, and evaluated it using three
SOTA LLMs, GPT-4o, DeepSeek-V3 and Qwen2.5-Coder-32b-Instruct, across two
widely-used code generation benchmarks: HumanEval+ and MBPP+. Our results show
that SpecFix, operating autonomously without human intervention or external
information, outputs repaired requirements that, when used by LLMs for code
generation, increase the Pass@1 score by 4.3%, and help LLMs to solve 3.4% more
problems via majority vote.

### 3. [BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models](http://arxiv.org/pdf/2505.07360v1)

Authors: Xiuwei Shang, Guoqiang Chen, Shaoyin Cheng, Benlong Wu, Li Hu, Gangyang Li, Weiming Zhang, Nenghai Yu

Binary analysis remains pivotal in software security, offering insights into
compiled programs without source code access. As large language models (LLMs)
continue to excel in diverse language understanding and generation tasks, their
potential in decoding complex binary data structures becomes evident. However,
the lack of standardized benchmarks in this domain limits the assessment and
comparison of LLM's capabilities in binary analysis and hinders the progress of
research and practical applications. To bridge this gap, we introduce
BinMetric, a comprehensive benchmark designed specifically to evaluate the
performance of large language models on binary analysis tasks. BinMetric
comprises 1,000 questions derived from 20 real-world open-source projects
across 6 practical binary analysis tasks, including decompilation, code
summarization, assembly instruction generation, etc., which reflect actual
reverse engineering scenarios. Our empirical study on this benchmark
investigates the binary analysis capabilities of various state-of-the-art LLMs,
revealing their strengths and limitations in this field. The findings indicate
that while LLMs show strong potential, challenges still exist, particularly in
the areas of precise binary lifting and assembly synthesis. In summary,
BinMetric makes a significant step forward in measuring the binary analysis
capabilities of LLMs, establishing a new benchmark leaderboard, and our study
provides valuable insights for the future development of these LLMs in software
security.

### 4. [A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection](http://arxiv.org/pdf/2505.07376v1)

Authors: Junji Yu, Honglin Shu, Michael Fu, Dong Wang, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen

Deep learning-based approaches, particularly those leveraging pre-trained
language models (PLMs), have shown promise in automated software vulnerability
detection. However, existing methods are predominantly limited to specific
programming languages, restricting their applicability in multilingual
settings. Recent advancements in large language models (LLMs) offer
language-agnostic capabilities and enhanced semantic understanding, presenting
a potential solution to this limitation. While existing studies have explored
LLMs for vulnerability detection, their detection performance remains unknown
for multilingual vulnerabilities. To address this gap, we conducted a
preliminary study to evaluate the effectiveness of PLMs and state-of-the-art
LLMs across seven popular programming languages. Our findings reveal that the
PLM CodeT5P achieves the best performance in multilingual vulnerability
detection, particularly in identifying the most critical vulnerabilities. Based
on these results, we further discuss the potential of LLMs in advancing
real-world multilingual vulnerability detection. This work represents an
initial step toward exploring PLMs and LLMs for cross-language vulnerability
detection, offering key insights for future research and practical deployment.

### 5. [A Systematic Literature Review on Neural Code Translation](http://arxiv.org/pdf/2505.07425v1)

Authors: Xiang Chen, Jiacheng Xue, Xiaofei Xie, Caokai Liang, Xiaolin Ju

Code translation aims to convert code from one programming language to
another automatically. It is motivated by the need for multi-language software
development and legacy system migration. In recent years, neural code
translation has gained significant attention, driven by rapid advancements in
deep learning and large language models. Researchers have proposed various
techniques to improve neural code translation quality. However, to the best of
our knowledge, no comprehensive systematic literature review has been conducted
to summarize the key techniques and challenges in this field. To fill this
research gap, we collected 57 primary studies covering the period 2020~2025 on
neural code translation. These studies are analyzed from seven key
perspectives: task characteristics, data preprocessing, code modeling, model
construction, post-processing, evaluation subjects, and evaluation metrics. Our
analysis reveals current research trends, identifies unresolved challenges, and
shows potential directions for future work. These findings can provide valuable
insights for both researchers and practitioners in the field of neural code
translation.

### 6. [Byam: Fixing Breaking Dependency Updates with Large Language Models](http://arxiv.org/pdf/2505.07522v1)

Authors: Frank Reyes, May Mahmoud, Federico Bono, Sarah Nadi, Benoit Baudry, Martin Monperrus

Application Programming Interfaces (APIs) facilitate the integration of
third-party dependencies within the code of client applications. However,
changes to an API, such as deprecation, modification of parameter names or
types, or complete replacement with a new API, can break existing client code.
These changes are called breaking dependency updates; It is often tedious for
API users to identify the cause of these breaks and update their code
accordingly. In this paper, we explore the use of Large Language Models (LLMs)
to automate client code updates in response to breaking dependency updates. We
evaluate our approach on the BUMP dataset, a benchmark for breaking dependency
updates in Java projects. Our approach leverages LLMs with advanced prompts,
including information from the build process and from the breaking dependency
analysis. We assess effectiveness at three granularity levels: at the build
level, the file level, and the individual compilation error level. We
experiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI
o3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that
LLMs can automatically repair breaking updates. Among the considered models,
OpenAI's o3-mini is the best, able to completely fix 27% of the builds when
using prompts that include contextual information such as the buggy line, API
differences, error messages, and step-by-step reasoning instructions. Also, it
fixes 78% of the individual compilation errors. Overall, our findings
demonstrate the potential for LLMs to fix compilation errors due to breaking
dependency updates, supporting developers in their efforts to stay up-to-date
with changes in their dependencies.

### 7. [A Systematic Mapping Study on Contract-based Software Design for Dependable Systems](http://arxiv.org/pdf/2505.07542v1)

Authors: Fazli Faruk Okumus, Amra Ramic, Stefan Kugele

Background: Contract-based Design (CbD) is a valuable methodology for
software design that allows annotation of code and architectural components
with contracts, thereby enhancing clarity and reliability in software
development. It establishes rules that outline the behaviour of software
components and their interfaces and interactions. This modular approach enables
the design process to be segmented into smaller, independently developed,
tested, and verified system components, ultimately leading to more robust and
dependable software. Aim: Despite the significance and well-established
theoretical background of CbD, there is a need for a comprehensive systematic
mapping study for reliable software systems. Our study provides an
evidence-based overview of a method and demonstrates its practical feasibility.
Method: To conduct this study, we systematically searched three different
databases using specially formulated queries, which initially yielded 1,221
primary studies. After voting, we focused on 288 primary studies for more
detailed analysis. Finally, a collaborative review allowed us to gather
relevant evidence and information to address our research questions. Results:
Our findings suggest potential avenues for future research trajectories in CbD,
emphasising its role in improving the dependability of software systems. We
highlight maturity levels across different domains and identify areas that may
benefit from further research. Conclusion: Although CbD is a well-established
software design approach, a more comprehensive literature review is needed to
clarify its theoretical state about dependable systems. Our study addresses
this gap by providing a detailed overview of CbD from various perspectives,
identifying key gaps, and suggesting future research directions.

### 8. [Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using Edge AI](http://arxiv.org/pdf/2505.07583v1)

Authors: Cong Le

This research addresses the growing need for privacy-preserving and
accessible language translation by developing a fully offline Neural Machine
Translation (NMT) system for Vietnamese-English translation on iOS devices.
Given increasing concerns about data privacy and unreliable network
connectivity, on-device translation offers critical advantages. This project
confronts challenges in deploying complex NMT models on resource-limited mobile
devices, prioritizing efficiency, accuracy, and a seamless user experience.
Leveraging advances such as MobileBERT and, specifically, the lightweight
\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \textbf{a} quantized
Transformer-based model is implemented and optimized. The application is
realized as a real-time iOS prototype, tightly integrating modern iOS
frameworks and privacy-by-design principles. Comprehensive documentation covers
model selection, technical architecture, challenges, and final implementation,
including functional Swift code for deployment.

### 9. [PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull Request Outcomes](http://arxiv.org/pdf/2505.07700v1)

Authors: Daniel Ogenrwot, John Businge

The rapid adoption of large language models (LLMs) like ChatGPT in software
development has introduced new ways for developers to interact with AI,
particularly in pull request workflows. While prior research has examined
AI-generated code quality, there is limited understanding of how ChatGPT is
utilized in real-world pull request decision-making and how its suggestions
influence patch integration and rejection. To explore these aspects, we analyze
self-admitted ChatGPT usage (SACU), where developers explicitly disclose their
reliance on ChatGPT within pull request discussions. Our study examines 338
pull requests (285 merged, 53 closed) across 255 GitHub repositories,
containing 645 ChatGPT-generated code snippets and 3,486 patches. We introduce
PatchTrack, a classification tool that determines whether ChatGPT-generated
patches were applied (PA, 115 cases), not applied (PN, 64 cases), or not
suggested (NE, 106 cases). Our findings reveal that full adoption of
ChatGPT-generated code is rare, developers frequently modify or selectively
integrate AI-generated patches to align with project constraints, with a median
integration rate of 25%. Through qualitative analysis, we identify key factors
influencing patch integration and pull request rejection, including scope
misalignment, maintainability concerns, redundant solutions, and procedural
barriers such as incomplete documentation or administrative policies. By
providing empirical insights into ChatGPT's role in pull request workflows,
this study informs developers, maintainers, and educators on the evolving use
of generative AI in collaborative software development. It also lays the
groundwork for future research on optimizing AI-assisted development, improving
transparency in AI adoption, and enhancing patch integration workflows.

### 10. [A Black-box Testing Framework for Oracle Quantum Programs](http://arxiv.org/pdf/2505.07243v1)

Authors: Peixun Long, Jianjun Zhao

Oracle quantum programs are a fundamental class of quantum programs that
serve as a critical bridge between quantum computing and classical computing.
Many important quantum algorithms are built upon oracle quantum programs,
making it essential to ensure their correctness during development. While
software testing is a well-established approach for improving program
reliability, no systematic method has been developed to test oracle quantum
programs. This paper proposes a black-box testing framework designed for
general oracle quantum programs. We define these programs formally, establish
the foundational theory for their testing, and propose a detailed testing
framework. We develop a prototype tool and conduct extensive experimental
evaluations to evaluate the framework's effectiveness. Our results demonstrate
that the proposed framework significantly aids developers in testing oracle
quantum programs, providing insights to enhance the reliability of quantum
software.

### Social and Information Networks

### 1. [The Language of Influence: Sentiment, Emotion, and Hate Speech in State Sponsored Influence Operations](http://arxiv.org/pdf/2505.07212v1)

Authors: Ashfaq Ali Shafin, Khandaker Mamun Ahmed

State-sponsored influence operations (SIOs) on social media have become an
instrumental tool for manipulating public opinion and spreading unverified
information. This study analyzes the sentiment, emotion, and abusive speech in
tweets circulated by influence campaigns originating from three distinct
countries: China, Iran, and Russia. We examined 1.5 million tweets to uncover
patterns in the content of the influence operations using the dataset provided
by Twitter. Our findings reveal distinct patterns of sentiment, emotion, and
abusive nature in different SIOs. Our experimental result shows that Russian
influence operations predominantly employ negative sentiment and toxic language
to polarize audiences, Iranian operations balance negative and positive tones
to provoke hostility while fostering support, and Chinese campaigns focus on
positive messaging to promote favorable narratives.

### 2. [Data Ethics in the Fediverse: Analyzing the Role of Instance Policies in Mastodon Research](http://arxiv.org/pdf/2505.07606v1)

Authors: Mareike Lisker, Helena Mihaljević

This article addresses the disconnect between the individual policy documents
of Mastodon instances--many of which explicitly prohibit data collection for
research purposes--and the actual data handling practices observed in academic
research involving Mastodon. We present a systematic analysis of 29 works that
used Mastodon as a data source, revealing limited adherence to instance--level
policies despite researchers' general awareness of their existence. Our
findings underscore the need for broader discussion about ethical obligations
in research on alternative, decentralized social media platforms.

### 3. ['Congratulations, morons': Dynamics of Toxicity and Interaction Polarization in the Covid Vaccination and Ukraine War Twitter Debates](http://arxiv.org/pdf/2505.07646v1)

Authors: D. S. Axelrod, B. H. Pleasants, J. C. Paolillo

The existence of polarization and echo chambers has been noted in social
media discussions of public concern such as the Covid-19 pandemic, foreign
election interference, and regional conflicts. However, measuring polarization
and assessing the manner in which polarization contributes to partisan behavior
is not always possible to evaluate with static network or affect measurements.
To address this, we conduct an analysis of two large Twitter datasets collected
around Covid-19 vaccination and the Ukraine war to investigate polarization in
terms of the evolution in influencer preferences and toxicity of post contents.
By reducing retweet behavior in each sample to several key dimensions, we
identify clusters that reflect ideological preferences, along with geographic
or linguistic separation for some cases. By tracking the central retweet
tendency of these clusters over time, we observe differences in the relative
position of ideologically unaligned clusters compared to aligned ones, which we
interpret as reflecting polarization dynamics in the information diffusion
space. We then measure the toxicity of posts and test if toxicity in one
cluster can be temporally dependent on its structural closeness to (or toxicity
of) another. We find evidence of ideological opposition among clusters of users
in both samples, and a temporal association between toxicity and structural
divergence for at least two ideologically opposed clusters in our samples.
These observations support the importance of analyzing polarization as a
multifaceted dynamic phenomenon where polarization dynamics may also manifest
in unexpected ways such as within a single ideological camp.

### Systems and Control

### 1. [Economic data-enabled predictive control using machine learning](http://arxiv.org/pdf/2505.07182v1)

Authors: Mingxue Yan, Xuewen Zhang, Kaixiang Zhang, Zhaojian Li, Xunyuan Yin

In this paper, we propose a convex data-based economic predictive control
method within the framework of data-enabled predictive control (DeePC).
Specifically, we use a neural network to transform the system output into a new
state space, where the nonlinear economic cost function of the underlying
nonlinear system is approximated using a quadratic function expressed by the
transformed output in the new state space. Both the neural network parameters
and the coefficients of the quadratic function are learned from open-loop data
of the system. Additionally, we reconstruct constrained output variables from
the transformed output through learning an output reconstruction matrix; this
way, the proposed economic DeePC can handle output constraints explicitly. The
performance of the proposed method is evaluated via a case study in a simulated
chemical process.

### 2. [A Novel Online Pseudospectral Method for Approximation of Nonlinear Systems Dynamics](http://arxiv.org/pdf/2505.07234v1)

Authors: Arian Yousefian, Avimanyu Sahoo, Vignesh Narayanan

This paper presents a novel online system identification approach utilizing
the Chebyshev pseudospectral (PS) method to approximate the dynamics of a
continuous-time nonlinear system. Unlike conventional periodic sampling, the
proposed identification scheme employs aperiodic state sampling, leveraging the
Chebyshev nodes to achieve the desired approximation accuracy. Unlike
traditional off-line PS approaches, the scheme utilizes a moving time-window
strategy to compute the sampling instants (Chebyshev nodes) forward in time for
state measurements. Within each time window, the number of sampling instants is
adaptively determined to meet the specified approximation accuracy. The
Chebyshev basis is also shifted for each time window to accommodate arbitrary
approximation intervals. The least-squares approach is employed to estimate the
coefficients of the shifted Chebyshev basis functions using the measured state
and its derivatives at the end of each window, resulting in a piecewise
approximation of the drift dynamics of the system. In the second step, the
identified drift dynamics are utilized to design an adaptive state estimator to
reconstruct the continuous system states from the aperiodic state measurements.
To address the smoothness of the piecewise approximated system dynamics, the
Chebyshev coefficients are recomputed at the window transition instants,
between two consecutive time windows, by enforcing continuity of the
approximated function and its derivatives. In addition, analytical results are
provided to determine the number of sampling instants within each time window,
which guarantees the desired approximation accuracy. The boundedness of
function approximation and state estimation errors is also proved analytically.
Finally, numerical simulation results are included to validate the proposed
scheme.

### 3. [Towards Autonomous 1/8th Offroad RC Racing -- The TruggySense Educational Platform](http://arxiv.org/pdf/2505.07399v1)

Authors: Robbe Elsermans, Jan Steckel

This paper presents a state-of-the-art Data Acquisition System designed for
off-road conditions, deployed on a Team Corally Kagama 1/8 Remote Controlled
Vehicle. The system is intended to support Advanced Driver Assistance Systems
in an educational context by providing valuable, consistent, and representative
data. Key measurement systems are discussed to enable insights into the Remote
Controlled Vehicles stability during and after off-road races. Furthermore,
four experiments where conducted to evaluate the Data Acquisition Systems
accuracy, stability, and consistency in replicating real-world vehicle
behavior. The proposed Data Acquisition System platform serves as a solid
foundation for use in engineering education, enabling integration with various
Advanced Driver Assistance Systems algorithms to enhance vehicle control and
overall performance, offering a new dimension to off-road racing. Additionally,
realtime telemetry enables verification and validation of Advanced Driver
Assistance Systems algorithms based on the live operating state of the Radio
Controlled Vehicle during races

### 4. [Integrated Localization and Path Planning for an Ocean Exploring Team of Autonomous Underwater Vehicles with Consensus Graph Model Predictive Control](http://arxiv.org/pdf/2505.07484v1)

Authors: Mohsen Eskandari, Andrey V. Savkin, Mohammad Deghat

Navigation of a team of autonomous underwater vehicles (AUVs) coordinated by
an unmanned surface vehicle (USV) is efficient and reliable for deep ocean
exploration. AUVs depart from and return to the USV after collaborative
navigation, data collection, and ocean exploration missions. Efficient path
planning and accurate localization are essential, the latter of which is
critical due to the lack of global localization signals and poor radio
frequency (RF) communication in deep waters. Inertial navigation and acoustic
communication are common solutions for localization. However, the former is
subject to odometry drifts, and the latter is limited to short distances. This
paper proposes a systematic approach for localization-aware energy-efficient
collision-free path planning for a USV-AUVs team. Path planning is formulated
as finite receding horizon model predictive control (MPC) optimization. A
dynamic-aware linear kinodynamic motion equation is developed. The mathematical
formulation for the MPC optimization is effectively developed where
localization is integrated as consensus graph optimization among AUV nodes.
Edges in the optimized AUV-to-USV (A2U) and AUV-to-AUV (A2A) graphs are
constrained to the sonar range of acoustic modems. The time complexity of the
consensus MPC optimization problem is analyzed, revealing a nonconvex NP-hard
problem, which is solved using sequential convex programming. Numerical
simulation results are provided to evaluate the proposed method.

### 5. [Non-Conservative Data-driven Safe Control Design for Nonlinear Systems with Polyhedral Safe Sets](http://arxiv.org/pdf/2505.07733v1)

Authors: Amir Modares, Bosen Lian, Hamidreza Modares

This paper presents a data-driven nonlinear safe control design approach for
discrete-time systems under parametric uncertainties and additive disturbances.
We first characterize a new control structure from which a data-based
representation of closed-loop systems is obtained. This data-based closed-loop
system is composed of two parts: 1) a parametrized linear closed-loop part and
a parametrized nonlinear remainder closed-loop part. We show that using the
standard practice or learning a robust controller to ensure safety while
treating the remaining nonlinearities as disturbances brings about significant
challenges in terms of computational complexity and conservatism. To overcome
these challenges, we develop a novel nonlinear safe control design approach in
which the closed-loop nonlinear remainders are learned, rather than canceled,
in a control-oriented fashion while preserving the computational efficiency. To
this end, a primal-dual optimization framework is leveraged in which the
control gains are learned to enforce the second-order optimality on the
closed-loop nonlinear remainders. This allows us to account for nonlinearities
in the design for the sake of safety rather than treating them as disturbances.
This new controller parameterization and design approach reduces the
computational complexity and the conservatism of designing a safe nonlinear
controller. A simulation example is then provided to show the effectiveness of
the proposed data-driven controller.

### 6. [Continuous-Time Control Synthesis for Multiple Quadrotors under Signal Temporal Logic Specifications](http://arxiv.org/pdf/2505.07240v1)

Authors: Yating Yuan

Ensuring continuous-time control of multiple quadrotors in constrained
environments under signal temporal logic (STL) specifications is challenging
due to nonlinear dynamics, safety constraints, and disturbances. This letter
proposes a two-stage framework to address this challenge. First, exponentially
decaying tracking error bounds are derived with multidimensional geometric
control gains obtained via differential evolution. These bounds are less
conservative, while the resulting tracking errors exhibit smaller oscillations
and improved transient performance. Second, leveraging the time-varying bounds,
a mixed-integer convex programming (MICP) formulation generates piecewise
B\'ezier reference trajectories that satisfy STL and velocity limits, while
ensuring inter-agent safety through convex-hull properties. Simulation results
demonstrate that the proposed approach enables formally verifiable multi-agent
coordination in constrained environments, with provable tracking guarantees
under bounded disturbances.

### 7. [Learning Quasi-LPV Models and Robust Control Invariant Sets with Reduced Conservativeness](http://arxiv.org/pdf/2505.07287v1)

Authors: Sampath Kumar Mulagaleti, Alberto Bemporad

We present an approach to identify a quasi Linear Parameter Varying (qLPV)
model of a plant, with the qLPV model guaranteed to admit a robust control
invariant (RCI) set. It builds upon the concurrent synthesis framework
presented in [1], in which the requirement of existence of an RCI set is
modeled as a control-oriented regularization. Here, we reduce the
conservativeness of the approach by bounding the qLPV system with an uncertain
LTI system, which we derive using bound propagation approaches. The resulting
regularization function is the optimal value of a nonlinear robust optimization
problem that we solve via a differentiable algorithm. We numerically
demonstrate the benefits of the proposed approach over two benchmark
approaches.

### 8. [Interpretable Event Diagnosis in Water Distribution Networks](http://arxiv.org/pdf/2505.07299v1)

Authors: André Artelt, Stelios G. Vrachimis, Demetrios G. Eliades, Ulrike Kuhl, Barbara Hammer, Marios M. Polycarpou

The increasing penetration of information and communication technologies in
the design, monitoring, and control of water systems enables the use of
algorithms for detecting and identifying unanticipated events (such as leakages
or water contamination) using sensor measurements. However, data-driven
methodologies do not always give accurate results and are often not trusted by
operators, who may prefer to use their engineering judgment and experience to
deal with such events.
  In this work, we propose a framework for interpretable event diagnosis -- an
approach that assists the operators in associating the results of algorithmic
event diagnosis methodologies with their own intuition and experience. This is
achieved by providing contrasting (i.e., counterfactual) explanations of the
results provided by fault diagnosis algorithms; their aim is to improve the
understanding of the algorithm's inner workings by the operators, thus enabling
them to take a more informed decision by combining the results with their
personal experiences. Specifically, we propose counterfactual event
fingerprints, a representation of the difference between the current event
diagnosis and the closest alternative explanation, which can be presented in a
graphical way. The proposed methodology is applied and evaluated on a realistic
use case using the L-Town benchmark.

### 9. [Stiffness-based Analytic Centre Method for Cable-Driven Parallel Robots](http://arxiv.org/pdf/2505.07348v1)

Authors: Domenico Dona', Vincenzo Di Paola, Matteo Zoppi, Alberto Trevisani

Nowadays, being fast and precise are key requirements in Robotics. This work
introduces a novel methodology to tune the stiffness of Cable-Driven Parallel
Robots (CDPRs) while simultaneously addressing the tension distribution
problem. In particular, the approach relies on the Analytic-Centre method.
Indeed, weighting the barrier functions makes natural the stiffness adaptation.
The intrinsic ability to adjust the stiffness during the execution of the task
enables the CDPRs to effectively meet above-mentioned requirements. The
capabilities of the method are demonstrated through simulations by comparing it
with the existing approach.

### 10. [High Performance Signal Design for ACO-OFDM Systems using Variational Autoencoder](http://arxiv.org/pdf/2505.07362v1)

Authors: Nam N. Luong, Chuyen T. Nguyen, Thanh V. Pham

This letter proposes a design of low peak-to-average power ratio (PAPR), low
symbol error rate (SER), and high data rate signal for asymmetrically clipped
optical orthogonal frequency division multiplexing (ACO-OFDM) systems. The
proposed design leverages a variational autoencoder (VAE) incorporating gradual
loss learning to jointly optimize the geometry and probability of the
constellation's symbols. This not only enhances mutual information (MI) but
also effectively reduces the PAPR while maintaining a low SER for reliable
transmission. We evaluate the performance of the proposed VAE-based design by
comparing the MI, SER, and PAPR against existing techniques. Simulation results
demonstrate that the proposed method achieves a considerably lower PAPR while
maintaining superior SER and MI performance for a wide range of SNRs.

### Machine Learning (Statistics Category)

### 1. [Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism](http://arxiv.org/pdf/2505.07180v1)

Authors: Ruichu Cai, Kaitao Zheng, Junxian Huang, Zijian Li, Zhengming Chen, Boyan Xu, Zhifeng Hao

Time series imputation is one of the most challenge problems and has broad
applications in various fields like health care and the Internet of Things.
Existing methods mainly aim to model the temporally latent dependencies and the
generation process from the observed time series data. In real-world scenarios,
different types of missing mechanisms, like MAR (Missing At Random), and MNAR
(Missing Not At Random) can occur in time series data. However, existing
methods often overlook the difference among the aforementioned missing
mechanisms and use a single model for time series imputation, which can easily
lead to misleading results due to mechanism mismatching. In this paper, we
propose a framework for time series imputation problem by exploring Different
Missing Mechanisms (DMM in short) and tailoring solutions accordingly.
Specifically, we first analyze the data generation processes with temporal
latent states and missing cause variables for different mechanisms.
Sequentially, we model these generation processes via variational inference and
estimate prior distributions of latent variables via normalizing flow-based
neural architecture. Furthermore, we establish identifiability results under
the nonlinear independent component analysis framework to show that latent
variables are identifiable. Experimental results show that our method surpasses
existing time series imputation techniques across various datasets with
different missing mechanisms, demonstrating its effectiveness in real-world
applications.

### 2. [Adaptive, Robust and Scalable Bayesian Filtering for Online Learning](http://arxiv.org/pdf/2505.07267v1)

Authors: Gerardo Duran-Martin

In this thesis, we introduce Bayesian filtering as a principled framework for
tackling diverse sequential machine learning problems, including online
(continual) learning, prequential (one-step-ahead) forecasting, and contextual
bandits. To this end, this thesis addresses key challenges in applying Bayesian
filtering to these problems: adaptivity to non-stationary environments,
robustness to model misspecification and outliers, and scalability to the
high-dimensional parameter space of deep neural networks. We develop novel
tools within the Bayesian filtering framework to address each of these
challenges, including: (i) a modular framework that enables the development
adaptive approaches for online learning; (ii) a novel, provably robust filter
with similar computational cost to standard filters, that employs Generalised
Bayes; and (iii) a set of tools for sequentially updating model parameters
using approximate second-order optimisation methods that exploit the
overparametrisation of high-dimensional parametric models such as neural
networks. Theoretical analysis and empirical results demonstrate the improved
performance of our methods in dynamic, high-dimensional, and misspecified
models.

### 3. [Adaptive Learning-based Surrogate Method for Stochastic Programs with Implicitly Decision-dependent Uncertainty](http://arxiv.org/pdf/2505.07298v1)

Authors: Boyang Shen, Junyi Liu

We consider a class of stochastic programming problems where the implicitly
decision-dependent random variable follows a nonparametric regression model
with heteroscedastic error. The Clarke subdifferential and surrogate functions
are not readily obtainable due to the latent decision dependency. To deal with
such a computational difficulty, we develop an adaptive learning-based
surrogate method that integrates the simulation scheme and statistical
estimates to construct estimation-based surrogate functions in a way that the
simulation process is adaptively guided by the algorithmic procedure. We
establish the non-asymptotic convergence rate analysis in terms of $(\nu,
\delta)$-near stationarity in expectation under variable proximal parameters
and batch sizes, which exhibits the superior convergence performance and
enhanced stability in both theory and practice. We provide numerical results
with both synthetic and real data which illustrate the benefits of the proposed
algorithm in terms of algorithmic stability and efficiency.

### 4. [Online Episodic Convex Reinforcement Learning](http://arxiv.org/pdf/2505.07303v1)

Authors: Bianca Marin Moreno, Khaled Eldowa, Pierre Gaillard, Margaux Brégère, Nadia Oudjane

We study online learning in episodic finite-horizon Markov decision processes
(MDPs) with convex objective functions, known as the concave utility
reinforcement learning (CURL) problem. This setting generalizes RL from linear
to convex losses on the state-action distribution induced by the agent's
policy. The non-linearity of CURL invalidates classical Bellman equations and
requires new algorithmic approaches. We introduce the first algorithm achieving
near-optimal regret bounds for online CURL without any prior knowledge on the
transition function. To achieve this, we use an online mirror descent algorithm
with varying constraint sets and a carefully designed exploration bonus. We
then address for the first time a bandit version of CURL, where the only
feedback is the value of the objective function on the state-action
distribution induced by the agent's policy. We achieve a sub-linear regret
bound for this more challenging problem by adapting techniques from bandit
convex optimization to the MDP setting.

### 5. [Causal mediation analysis with one or multiple mediators: a comparative study](http://arxiv.org/pdf/2505.07323v1)

Authors: Judith Abécassis, Houssam Zenati, Sami Boumaïza, Julie Josse, Bertrand Thirion

Mediation analysis breaks down the causal effect of a treatment on an outcome
into an indirect effect, acting through a third group of variables called
mediators, and a direct effect, operating through other mechanisms. Mediation
analysis is hard because confounders between treatment, mediators, and outcome
blur effect estimates in observational studies. Many estimators have been
proposed to adjust on those confounders and provide accurate causal estimates.
We consider parametric and non-parametric implementations of classical
estimators and provide a thorough evaluation for the estimation of the direct
and indirect effects in the context of causal mediation analysis for binary,
continuous, and multi-dimensional mediators. We assess several approaches in a
comprehensive benchmark on simulated data. Our results show that advanced
statistical approaches such as the multiply robust and the double machine
learning estimators achieve good performances in most of the simulated settings
and on real data. As an example of application, we propose a thorough analysis
of factors known to influence cognitive functions to assess if the mechanism
involves modifications in brain morphology using the UK Biobank brain imaging
cohort. This analysis shows that for several physiological factors, such as
hypertension and obesity, a substantial part of the effect is mediated by
changes in the brain structure. This work provides guidance to the practitioner
from the formulation of a valid causal mediation problem, including the
verification of the identification assumptions, to the choice of an adequate
estimator.

### 6. [Identifying Causal Direction via Variational Bayesian Compression](http://arxiv.org/pdf/2505.07503v1)

Authors: Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen

Telling apart the cause and effect between two random variables with purely
observational data is a challenging problem that finds applications in various
scientific disciplines. A key principle utilized in this task is the
algorithmic Markov condition, which postulates that the joint distribution,
when factorized according to the causal direction, yields a more succinct
codelength compared to the anti-causal direction. Previous approaches
approximate these codelengths by relying on simple functions or Gaussian
processes (GPs) with easily evaluable complexity, compromising between model
fitness and computational complexity. To overcome these limitations, we propose
leveraging the variational Bayesian learning of neural networks as an
interpretation of the codelengths. Consequently, we can enhance the model
fitness while promoting the succinctness of the codelengths, while avoiding the
significant computational complexity of the GP-based approaches. Extensive
experiments on both synthetic and real-world benchmarks in cause-effect
identification demonstrate the effectiveness of our proposed method, surpassing
the overall performance of related complexity-based and structural causal model
regression-based approaches.

### 7. [Modelling higher education dropouts using sparse and interpretable post-clustering logistic regression](http://arxiv.org/pdf/2505.07582v1)

Authors: Andrea Nigri, Massimo Bilancia, Barbara Cafarelli, Samuele Magro

Higher education dropout constitutes a critical challenge for tertiary
education systems worldwide. While machine learning techniques can achieve high
predictive accuracy on selected datasets, their adoption by policymakers
remains limited and unsatisfactory, particularly when the objective is the
unsupervised identification and characterization of student subgroups at
elevated risk of dropout. The model introduced in this paper is a specialized
form of logistic regression, specifically adapted to the context of university
dropout analysis. Logistic regression continues to serve as a foundational tool
among reliable statistical models, primarily due to the ease with which its
parameters can be interpreted in terms of odds ratios. Our approach
significantly extends this framework by incorporating heterogeneity within the
student population. This is achieved through the application of a preliminary
clustering algorithm that identifies latent subgroups, each characterized by
distinct dropout propensities, which are then modeled via cluster-specific
effects. We provide a detailed interpretation of the model parameters within
this extended framework and enhance interpretability by imposing sparsity
through a tailored variant of the LASSO algorithm. To demonstrate the practical
applicability of the proposed methodology, we present an extensive case study
based on the Italian university system, in which all the developed tools are
systematically applied

### 8. [Certified Data Removal Under High-dimensional Settings](http://arxiv.org/pdf/2505.07640v1)

Authors: Haolin Zou, Arnab Auddy, Yongchan Kwon, Kamiar Rahnama Rad, Arian Maleki

Machine unlearning focuses on the computationally efficient removal of
specific training data from trained models, ensuring that the influence of
forgotten data is effectively eliminated without the need for full retraining.
Despite advances in low-dimensional settings, where the number of parameters \(
p \) is much smaller than the sample size \( n \), extending similar
theoretical guarantees to high-dimensional regimes remains challenging. We
propose an unlearning algorithm that starts from the original model parameters
and performs a theory-guided sequence of Newton steps \( T \in \{ 1,2\}\).
After this update, carefully scaled isotropic Laplacian noise is added to the
estimate to ensure that any (potential) residual influence of forget data is
completely removed. We show that when both \( n, p \to \infty \) with a fixed
ratio \( n/p \), significant theoretical and computational obstacles arise due
to the interplay between the complexity of the model and the finite
signal-to-noise ratio. Finally, we show that, unlike in low-dimensional
settings, a single Newton step is insufficient for effective unlearning in
high-dimensional problems -- however, two steps are enough to achieve the
desired certifiebility. We provide numerical experiments to support the
certifiability and accuracy claims of this approach.

### 9. [Langevin Diffusion Approximation to Same Marginal Schrödinger Bridge](http://arxiv.org/pdf/2505.07647v1)

Authors: Medha Agarwal, Zaid Harchaoui, Garrett Mulcahy, Soumik Pal

We introduce a novel approximation to the same marginal Schr\"{o}dinger
bridge using the Langevin diffusion. As $\varepsilon \downarrow 0$, it is known
that the barycentric projection (also known as the entropic Brenier map) of the
Schr\"{o}dinger bridge converges to the Brenier map, which is the identity. Our
diffusion approximation is leveraged to show that, under suitable assumptions,
the difference between the two is $\varepsilon$ times the gradient of the
marginal log density (i.e., the score function), in $\mathbf{L}^2$. More
generally, we show that the family of Markov operators, indexed by $\varepsilon
> 0$, derived from integrating test functions against the conditional density
of the static Schr\"{o}dinger bridge at temperature $\varepsilon$, admits a
derivative at $\varepsilon=0$ given by the generator of the Langevin semigroup.
Hence, these operators satisfy an approximate semigroup property at low
temperatures.

### 10. [ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data](http://arxiv.org/pdf/2505.07272v1)

Authors: Javier Salazar Cavazos, Jeffrey A. Fessler, Laura Balzano

Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction. However, some applications involve heterogeneous data
that vary in quality due to noise characteristics associated with each data
sample. Heteroscedastic methods aim to deal with such mixed data quality. This
paper develops a subspace learning method, named ALPCAH, that can estimate the
sample-wise noise variances and use this information to improve the estimate of
the subspace basis associated with the low-rank structure of the data. Our
method makes no distributional assumptions of the low-rank component and does
not assume that the noise variances are known. Further, this method uses a soft
rank constraint that does not require subspace dimension to be known.
Additionally, this paper develops a matrix factorized version of ALPCAH, named
LR-ALPCAH, that is much faster and more memory efficient at the cost of
requiring subspace dimension to be known or estimated. Simulations and real
data experiments show the effectiveness of accounting for data
heteroscedasticity compared to existing algorithms. Code available at
https://github.com/javiersc1/ALPCAH.

