# Computer Science arXiv Papers

Collection of top 10 Computer Science research papers pulled daily from arXiv.

---

Pulled on 2025-08-28 17:00:26.100802 PST.

### Artificial Intelligence

### 1. [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](http://arxiv.org/pdf/2508.19502v1)

Authors: Xifeng Yao, Chengyuan Ma, Dongyu Lang, Yinhao Ni, Zhiwei Xu, Huarui Xie, Zihao Chen, Guang Shen, Dandan Tu, Yi Bai, Changzheng Zhang

In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

### 2. [Caught in the Act: a mechanistic approach to detecting deception](http://arxiv.org/pdf/2508.19505v1)

Authors: Gerard Boxo, Ryan Socha, Daniel Yoo, Shivam Raval

Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

### 3. [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](http://arxiv.org/pdf/2508.19562v1)

Authors: Trisanth Srinivasan, Santosh Patapati

This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

### 4. [Skill-based Explanations for Serendipitous Course Recommendation](http://arxiv.org/pdf/2508.19569v1)

Authors: Hung Chau, Run Yu, Zachary Pardos, Peter Brusilovsky

Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

### 5. [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](http://arxiv.org/pdf/2508.19679v1)

Authors: Qihang Ai, Pi Bu, Yue Cao, Yingyao Wang, Jihao Gu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Zhicheng Zheng, Jun Song, Yuning Jiang, Bo Zheng

Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

### 6. [Tracking World States with Language Models: State-Based Evaluation Using Chess](http://arxiv.org/pdf/2508.19851v1)

Authors: Romain Harang, Jason Naradowsky, Yaswitha Gujju, Yusuke Miyao

Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

### 7. [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](http://arxiv.org/pdf/2508.19932v1)

Authors: Nitish Jaipuria, Lorenzo Gatto, Zijun Kan, Shankey Poddar, Bill Cheung, Diksha Bansal, Ramanan Balakrishnan, Aviral Suri, Jose Estevez

The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

### 8. [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](http://arxiv.org/pdf/2508.19963v1)

Authors: M. Umlauft, M. Schranz

Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

### 9. [Data-Efficient Symbolic Regression via Foundation Model Distillation](http://arxiv.org/pdf/2508.19487v1)

Authors: Wangyang Ying, Jinghan Zhang, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Kunpeng Liu, Chandan K. Reddy, Yanjie Fu

Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

### 10. [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](http://arxiv.org/pdf/2508.19499v1)

Authors: Xiangxu Wang, Tianhong Zhao, Wei Tu, Bowen Zhang, Guanzhou Chen, Jinzhou Cao

Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

### Hardware Architecture

### 1. [RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs](http://arxiv.org/pdf/2508.19530v1)

Authors: Yanyun Wang, Dingcui Yu, Yina Lv, Yunpeng Song, Yumiao Zhao, Liang Shi

Quad-level cell (QLC) flash offers significant benefits in cost and capacity,
but its limited reliability leads to frequent read retries, which severely
degrade read performance. A common strategy in high-density flash storage is to
program selected blocks in a low-density mode (SLC), sacrificing some capacity
to achieve higher I/O performance. This hybrid storage architecture has been
widely adopted in consumer-grade storage systems. However, existing hybrid
storage schemes typically focus on write performance and rely solely on data
temperature for migration decisions. This often results in excessive mode
switching, causing substantial capacity overhead.
  In this paper, we present RARO (Reliability-Aware Read performance
Optimization), a hybrid flash management scheme designed to improve read
performance with minimal capacity cost. The key insight behind RARO is that
much of the read slowdown in QLC flash is caused by read retries. RARO triggers
data migration only when hot data resides in QLC blocks experiencing a high
number of read retries, significantly reducing unnecessary conversions and
capacity loss. Moreover, RARO supports fine-grained multi-mode conversions
(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time
read retry statistics and flash characteristics, RARO mitigates over-conversion
and optimizes I/O performance. Experiments on the FEMU platform demonstrate
that RARO significantly improves read performance across diverse workloads,
with negligible impact on usable capacity.

### 2. [Support Vector Machines Classification on Bendable RISC-V](http://arxiv.org/pdf/2508.19656v1)

Authors: Polykarpos Vergos, Theofanis Vergos, Florentia Afentaki, Konstantinos Balaskas, Georgios Zervakis

Flexible Electronics (FE) technology offers uniquecharacteristics in
electronic manufacturing, providing ultra-low-cost, lightweight, and
environmentally-friendly alternatives totraditional rigid electronics. These
characteristics enable a rangeof applications that were previously constrained
by the costand rigidity of conventional silicon technology. Machine learning
(ML) is essential for enabling autonomous, real-time intelligenceon devices
with smart sensing capabilities in everyday objects. However, the large feature
sizes and high power consumption ofthe devices oppose a challenge in the
realization of flexible ML applications. To address the above, we propose an
open-source framework for developing ML co-processors for the Bendable RISC-V
core. In addition, we present a custom ML accelerator architecture for Support
Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)
algorithms. Our ML accelerator adopts a generic, precision-scalable design,
supporting 4-, 8-, and 16-bit weight representations. Experimental results
demonstrate a 21x improvement in both inference execution time and energy
efficiency, on average, highlighting its potential for low-power, flexible
intelligence on the edge.

### 3. [Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites](http://arxiv.org/pdf/2508.19657v1)

Authors: Jorge L. González-Rios, Liz Martínez Marrero, Juan Duncan, Luis M. Garcés-Socarrás, Raudel Cuiman Marquez, Juan A. Vásquez Peralvo, Jevgenij Krivochiza, Symeon Chatzinotas, Björn Ottersten

The use of communication satellites in medium Earth orbit (MEO) is foreseen
to provide quasi-global broadband Internet connectivity in the coming
networking ecosystems. Multi-user multiple-input single-output (MU-MISO)
digital signal processing techniques, such as precoding, emerge as appealing
technological enablers in the forward link of multi-beam satellite systems
operating in full frequency reuse (FFR). However, the orbit dynamics of MEO
satellites pose additional challenges that must be carefully evaluated and
addressed. This work presents the design of an in-lab testbed based on
software-defined radio (SDR) platforms and the corresponding adaptations
required for efficient precoding in a MEO scenario. The setup incorporates a
precise orbit model and the radiation pattern of a custom-designed direct
radiating array (DRA). We analyze the main impairments affecting precoding
performance, including Doppler shifts and payload phase noise, and propose a
synchronization loop to mitigate these effects. Preliminary experimental
results validate the feasibility and effectiveness of the proposed solution.

### 4. [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](http://arxiv.org/pdf/2508.19661v1)

Authors: Florentia Afentaki, Sri Sai Rakesh Nakkilla, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Shiyi Jiang, Georgios Zervakis, Farshad Firouzi, Krishnendu Chakrabarty, Mehdi B. Tahoori

Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

### 5. [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](http://arxiv.org/pdf/2508.19868v1)

Authors: Geraldo F. Oliveira

Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

### 6. [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](http://arxiv.org/pdf/2508.19548v1)

Authors: Madhuvanthi Srivatsav R, Chiranjib Bhattacharyya, Shantanu Chakrabartty, Chetan Singh Thakur

Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

### 7. [Large Language Models (LLMs) for Electronic Design Automation (EDA)](http://arxiv.org/pdf/2508.20030v1)

Authors: Kangwei Xu, Denis Schwachhofer, Jason Blocklove, Ilia Polian, Peter Domanski, Dirk Pflüger, Siddharth Garg, Ramesh Karri, Ozgur Sinanoglu, Johann Knechtel, Zhuorui Zhao, Ulf Schlichtmann, Bing Li

With the growing complexity of modern integrated circuits, hardware engineers
are required to devote more effort to the full design-to-manufacturing
workflow. This workflow involves numerous iterations, making it both
labor-intensive and error-prone. Therefore, there is an urgent demand for more
efficient Electronic Design Automation (EDA) solutions to accelerate hardware
development. Recently, large language models (LLMs) have shown remarkable
advancements in contextual comprehension, logical reasoning, and generative
capabilities. Since hardware designs and intermediate scripts can be
represented as text, integrating LLM for EDA offers a promising opportunity to
simplify and even automate the entire workflow. Accordingly, this paper
provides a comprehensive overview of incorporating LLMs into EDA, with emphasis
on their capabilities, limitations, and future opportunities. Three case
studies, along with their outlook, are introduced to demonstrate the
capabilities of LLMs in hardware design, testing, and optimization. Finally,
future directions and challenges are highlighted to further explore the
potential of LLMs in shaping the next-generation EDA, providing valuable
insights for researchers interested in leveraging advanced AI technologies for
EDA.

### Computational Complexity

### 1. [Parameterised Counting Constraint Satisfaction Problems via Holants on Hypergraphs](http://arxiv.org/pdf/2508.19794v1)

Authors: Panagiotis Aivasiliotis, Andreas Göbel, Marc Roth

We study the complexity of the parameterised counting constraint satisfaction
problem: given a set of constraints over a set of variables and a positive
integer $k$, how many ways are there to assign $k$ variables to 1 (and the
others to 0) such that all constraints are satisfied. Existing work has so far
exclusively focused on restricted settings such as finding and counting
homomorphisms between relational structures due to Grohe (JACM 2007) and Dalmau
and Jonsson (TCS 2004), or the case of finite constraint languages due to
Creignou and Vollmer (SAT 2012), and Bulatov and Marx (SICOMP 2014).
  In this work, we tackle a more general setting of Valued Parameterised
Counting Constraint Satisfaction Problems (VCSPs) with infinite constraint
languages. In this setting we are able to model significantly more general
problems such as (weighted) parameterised factor problems on hypergraphs and
counting weight-$k$ solutions of systems of linear equations, not captured by
existing complexity classifications.
  We express parameterised VCSPs as parameterised \emph{Holant problems} on
uniform hypergraphs, and we establish complete and explicit complexity
dichotomy theorems. For resolving the $\mathrm{P}$ vs.\ $\#\mathrm{P}$
question, we mainly rely on hypergraph gadgets, the existence of which we prove
using properties of degree sequences necessary for realisability in uniform
hypergraphs. For the $\mathrm{FPT}$ vs.\ $\#\mathrm{W}[1]$ question, we mainly
rely on known hardness results for the special case of graphs by Aivasiliotis
et al. (ICALP 2025) and on an extension of the framework of the homomorphism
basis due to Curticapean, Dell and Marx (STOC 17) to uniform hypergraphs. As a
technical highlight, we also employ Curticapean's ``CFI Filters'' (SODA 2024)
to establish polynomial-time algorithms for isolating vectors in the
homomorphism basis.

### Computational Engineering

### 1. [An assessment of estimation models and investment gaps for the deployment of high-speed broadband networks in NUTS3 regions to meet the objectives of the European Gigabit Society](http://arxiv.org/pdf/2508.19921v1)

Authors: Ferrandis Jesus, Ramos Sergio, Feijoo Claudio

This paper analyses the deployment of high speed broadband networks in the
European Union (EU). Its aim is to assess the investment required to meet the
targets set by the European Commission (EC) for 2025, within the framework of
the European Gigabit Society (EGS). This plan aims to ensure the availability
and take up of very high capacity fixed and wireless networks, in both urban
and rural areas, among households and the main socioeconomic drivers. The
estimation model presented here uses a methodology supported by data at the
local (NUTS3) level to give a bottom up estimation of the investment gap for
each of the EGS objectives, using three different scenarios depending on the
mix of wired and wireless technologies offered. The methodology and estimation
model used in the paper are examined against other examples and assumptions
available in the literature. We also offer a dynamic perspective on the
analysis of the evolution of this investment gap over the years 2017 2019,
which includes an assessment of the usefulness of these estimation models.

### 2. [Multi-field decomposed hyper-reduced order modeling of damage-plasticity simulations](http://arxiv.org/pdf/2508.19957v1)

Authors: Jannick Kehls, Stephan Ritzert, Lars Breuer, Qinghua Zhang, Stefanie Reese, Tim Brepols

This paper presents a multi-field decomposed approach for hyper-reduced order
modeling to overcome the limitations of traditional model reduction techniques
for gradient-extended damage-plasticity simulations. The discrete empirical
interpolation method (DEIM) and the energy-conserving sampling and weighting
method (ECSW) are extended to account for the multi-field nature of the
problem. Both methods yield stable reduced order simulations, while
significantly reducing the computational cost compared to full-order
simulations. Two numerical examples are presented to demonstrate the
performance and limitations of the proposed approaches. The decomposed ECSW
method has overall higher accuracy and lower computational cost than the
decomposed DEIM method.

### 3. [From stand-up to start-up: exploring entrepreneurship competences and STEM womens intention](http://arxiv.org/pdf/2508.20091v1)

Authors: Armuna Cristina, Ramos Sergio, Juan Jesus, Feijoo Claudio, Arenal Alberto

This study seeks to explore the relationship between entrepreneurship
competencies and intention (EI) of a sample of potential STEM entrepreneurs in
order to assess the conventional assumption on women exhibiting lower rates of
entrepreneurship intention than men and that the lack of competence perceived
is a higher barrier to be an entrepreneur for them. The model used for the
analysis takes as reference the Entrepreneurship Competences Framework
(EntreComp) proposed by the European Commission (EC) as a common guide to
inspire entrepreneurship education. Data gathering is based on a structured
questionnaire. The conducted analysis uses Students t test means comparison and
factor analysis to define the model of competences, and a multiple regression
model to study the relationship between competences and skill factors in EI.
Findings do not validate the hypothesis that women have fewer entrepreneurship
intentions than men. Also, slight differences on the self-perceived competences
are obtained by gender. In addition, the study confirms the hypothesis of a
positive relationship between competences and EI, but here gender is not a
moderating factor. Results are expected to contribute to the entrepreneurship
competences debate and provide useful insights of application in
entrepreneurship education with orientation towards the business creation.

### Computational Geometry

### 1. [Simpler is Faster: Practical Distance Reporting by Sorting Along a Space-Filling Curve](http://arxiv.org/pdf/2508.19891v1)

Authors: Sarita de Berg, Ivor van der Hoog, Eva Rotenberg, Emil Toftegaard Gæde

Range reporting is a classical problem in computational geometry. A
(rectangular) reporting data structure stores a point set $P$ of $n$ points,
such that, given a (rectangular) query region $\Delta$, it returns all points
in $P \cap \Delta$. A variety of data structures support such queries with
differing asymptotic guarantees such as $k$-d trees, range trees, $R$-trees,
and quadtrees. A common variant of range queries are distance reporting
queries, where the input is a query point $q$ and a radius $\delta$, and the
goal is to report all points in $P$ within distance $\delta$ of $q$. Such
queries frequently arise as subroutines in geometric data structure
construction and in Fr\'echet distance computations. Modern implementations
typically reduce distance queries to rectangular range queries using the data
structures listed above.
  We revisit a simple and practical heuristic for distance reporting. The
approach is straightforward: sort the input point set $P$ along a space-filling
curve. Queries then reduce to scanning at most four contiguous ranges along the
sorted curve. We show extensive experimental evaluation of modern distance and
range reporting data structures. In a static scenario, we show that this simple
technique is competitive with all but the most highly optimised range reporting
data structures. Notably, these involved structures use space-filling curves
themselves to speed up computation. In a dynamic setting, our simpler method
even becomes the preferred technique.
  This leads to a perhaps unexpected insight: while modern data structures
invest heavily in leveraging space-filling curves for optimising their layout
and traversal, it is the curve itself, rather than the surrounding machinery,
that delivers much of the performance.

### 2. [Approximating mixed volumes to arbitrary accuracy](http://arxiv.org/pdf/2508.19582v1)

Authors: Hariharan Narayanan, Sourav Roy

We study the problem of approximating the mixed volume $V(P_1^{(\alpha_1)},
\dots, P_k^{(\alpha_k)})$ of an $k$-tuple of convex polytopes $(P_1, \dots,
P_k)$, each of which is defined as the convex hull of at most $m_0$ points in
$\mathbb{Z}^n$. We design an algorithm that produces an estimate that is within
a multiplicative $1 \pm \epsilon$ factor of the true mixed volume with a
probability greater than $1 - \delta.$ Let the constant $ \prod_{i=2}^{k}
\frac{(\alpha_{i}+1)^{\alpha_{i}+1}}{\alpha_{i}^{\,\alpha_{i}}}$ be denoted by
$\tilde{A}$. When each $P_i \subseteq B_\infty(2^L)$, we show in this paper
that the time complexity of the algorithm is bounded above by a polynomial in
$n, m_0, L, \tilde{A}, \epsilon^{-1}$ and $\log \delta^{-1}$. In fact, a
stronger result is proved in this paper, with slightly more involved
terminology.
  In particular, we provide the first randomized polynomial time algorithm for
computing mixed volumes of such polytopes when $k$ is an absolute constant, but
$\alpha_1, \dots, \alpha_k$ are arbitrary. Our approach synthesizes tools from
convex optimization, the theory of Lorentzian polynomials, and polytope
subdivision.

### 3. [Visualizing Treewidth](http://arxiv.org/pdf/2508.19935v1)

Authors: Alvin Chiu, Thomas Depian, David Eppstein, Michael T. Goodrich, Martin Nöllenburg

A witness drawing of a graph is a visualization that clearly shows a given
property of a graph. We study and implement various drawing paradigms for
witness drawings to clearly show that graphs have bounded pathwidth or
treewidth. Our approach draws the tree decomposition or path decomposition as a
tree of bags, with induced subgraphs shown in each bag, and with ''tracks'' for
each graph vertex connecting its copies in multiple bags. Within bags, we
optimize the vertex layout to avoid crossings of edges and tracks. We implement
a visualization prototype for crossing minimization using dynamic programming
for graphs of small width and heuristic approaches for graphs of larger width.
We introduce a taxonomy of drawing styles, which render the subgraph for each
bag as an arc diagram with one or two pages or as a circular layout with
straight-line edges, and we render tracks either with straight lines or with
orbital-radial paths.

### 4. [An algorithm for accurate and simple-looking metaphorical maps](http://arxiv.org/pdf/2508.19810v1)

Authors: Eleni Katsanou, Tamara Mchedlidze, Antonios Symvonis, Thanos Tolias

"Metaphorical maps" or "contact representations" are visual representations
of vertex-weighted graphs that rely on the geographic map metaphor. The
vertices are represented by countries, the weights by the areas of the
countries, and the edges by contacts/ boundaries among them. The accuracy with
which the weights are mapped to areas and the simplicity of the polygons
representing the countries are the two classical optimization goals for
metaphorical maps. Mchedlidze and Schnorr [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022] presented a force-based algorithm that
creates metaphorical maps that balance between these two optimization goals.
Their maps look visually simple, but the accuracy of the maps is far from
optimal - the countries' areas can vary up to 30% compared to required. In this
paper, we provide a multi-fold extension of the algorithm in [Metaphoric Maps
for Dynamic Vertex-weighted Graphs, EuroVis 2022]. More specifically:
  1. Towards improving accuracy: We introduce the notion of region stiffness
and suggest a technique for varying the stiffness based on the current pressure
of map regions.
  2. Towards maintaining simplicity: We introduce a weight coefficient to the
pressure force exerted on each polygon point based on whether the corresponding
point appears along a narrow passage.
  3. Towards generality: We cover, in contrast to [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022], non-triangulated graphs. This is done by
either generating points where more than three regions meet or by introducing
holes in the metaphorical map.
  We perform an extended experimental evaluation that, among other results,
reveals that our algorithm is able to construct metaphorical maps with nearly
perfect area accuracy with a little sacrifice in their simplicity.

### 5. [Internally-Convex Drawings of Outerplanar Graphs in Small Area](http://arxiv.org/pdf/2508.19913v1)

Authors: Michael A. Bekos, Giordano Da Lozzo, Fabrizio Frati, Giuseppe Liotta, Antonios Symvonis

A well-known result by Kant [Algorithmica, 1996] implies that n-vertex
outerplane graphs admit embedding-preserving planar straight-line grid drawings
where the internal faces are convex polygons in $O(n^2)$ area. In this paper,
we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also
consider outerplanar drawings in which the internal faces are required to be
strictly-convex polygons. In this setting, we consider outerplanar graphs whose
weak dual is a path and give a drawing algorithm that achieves $\Theta(nk^2)$
area, where $k$ is the maximum size of an internal facial cycle.

### Computation and Language

### 1. [Rule Synergy Analysis using LLMs: State of the Art and Implications](http://arxiv.org/pdf/2508.19484v1)

Authors: Bahar Bateni, Benjamin Pratt, Jim Whitehead

Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

### 2. [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](http://arxiv.org/pdf/2508.19529v1)

Authors: Bowen Sun, Yujun Cai, Ming-Hsuan Yang, Yiwei Wang

Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

### 3. [Alignment with Fill-In-the-Middle for Enhancing Code Generation](http://arxiv.org/pdf/2508.19532v1)

Authors: Houxing Ren, Zimu Lu, Weikang Shi, Haotian Hou, Yunqiao Yang, Ke Wang, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li

The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

### 4. [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](http://arxiv.org/pdf/2508.19533v1)

Authors: Kun Peng, Cong Cao, Hao Peng, Guanlin Wu, Zhifeng Hao, Lei Jiang, Yanbing Liu, Philip S. Yu

Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

### 5. [ArgCMV: An Argument Summarization Benchmark for the LLM-era](http://arxiv.org/pdf/2508.19580v1)

Authors: Omkar Gurjar, Agam Goyal, Eshwar Chandrasekharan

Key point extraction is an important task in argument summarization which
involves extracting high-level short summaries from arguments. Existing
approaches for KP extraction have been mostly evaluated on the popular ArgKP21
dataset. In this paper, we highlight some of the major limitations of the
ArgKP21 dataset and demonstrate the need for new benchmarks that are more
representative of actual human conversations. Using SoTA large language models
(LLMs), we curate a new argument key point extraction dataset called ArgCMV
comprising of around 12K arguments from actual online human debates spread
across over 3K topics. Our dataset exhibits higher complexity such as longer,
co-referencing arguments, higher presence of subjective discourse units, and a
larger range of topics over ArgKP21. We show that existing methods do not adapt
well to ArgCMV and provide extensive benchmark results by experimenting with
existing baselines and latest open source models. This work introduces a novel
KP extraction dataset for long-context online discussions, setting the stage
for the next generation of LLM-driven summarization research.

### 6. [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](http://arxiv.org/pdf/2508.19594v1)

Authors: Jun Bai, Minghao Tong, Yang Liu, Zixia Jia, Zilong Zheng

Context faithfulness is essential for reliable reasoning in context-dependent
scenarios. However, large language models often struggle to ground their
outputs in the provided context, resulting in irrelevant responses. Inspired by
the emergent expert specialization observed in mixture-of-experts
architectures, this work investigates whether certain experts exhibit
specialization in context utilization, offering a potential pathway toward
targeted optimization for improved context faithfulness. To explore this, we
propose Router Lens, a method that accurately identifies context-faithful
experts. Our analysis reveals that these experts progressively amplify
attention to relevant contextual information, thereby enhancing context
grounding. Building on this insight, we introduce Context-faithful Expert
Fine-Tuning (CEFT), a lightweight optimization approach that selectively
fine-tunes context-faithful experts. Experiments across a wide range of
benchmarks and models demonstrate that CEFT matches or surpasses the
performance of full fine-tuning while being significantly more efficient.

### 7. [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](http://arxiv.org/pdf/2508.19633v1)

Authors: Chong Tian, Qirong Ho, Xiuying Chen

Rapid LLM advancements heighten fake news risks by enabling the automatic
generation of increasingly sophisticated misinformation. Previous detection
methods, including fine-tuned small models or LLM-based detectors, often
struggle with its dynamically evolving nature. In this work, we propose a novel
framework called the Symbolic Adversarial Learning Framework (SALF), which
implements an adversarial training paradigm by an agent symbolic learning
optimization process, rather than relying on numerical updates. SALF introduces
a paradigm where the generation agent crafts deceptive narratives, and the
detection agent uses structured debates to identify logical and factual flaws
for detection, and they iteratively refine themselves through such adversarial
interactions. Unlike traditional neural updates, we represent agents using
agent symbolic learning, where learnable weights are defined by agent prompts,
and simulate back-propagation and gradient descent by operating on natural
language representations of weights, loss, and gradients. Experiments on two
multilingual benchmark datasets demonstrate SALF's effectiveness, showing it
generates sophisticated fake news that degrades state-of-the-art detection
performance by up to 53.4% in Chinese and 34.2% in English on average. SALF
also refines detectors, improving detection of refined content by up to 7.7%.
We hope our work inspires further exploration into more robust, adaptable fake
news detection systems.

### 8. [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](http://arxiv.org/pdf/2508.19665v1)

Authors: Giovanni Pollo, Andrei Mihai Albu, Alessio Burrello, Daniele Jahier Pagliari, Cristian Tesconi, Loris Panaro, Dario Soldi, Fabio Autieri, Sara Vinco

The recent advancements of the automotive sector demand robust co-simulation
methodologies that enable early validation and seamless integration across
hardware and software domains. However, the lack of standardized interfaces and
the dominance of proprietary simulation platforms pose significant challenges
to collaboration, scalability, and IP protection. To address these limitations,
this paper presents an approach for automatically wrapping SystemC models by
using the Functional Mock-up Interface (FMI) standard. This method combines the
modeling accuracy and fast time-to-market of SystemC with the interoperability
and encapsulation benefits of FMI, enabling secure and portable integration of
embedded components into co-simulation workflows. We validate the proposed
methodology on real-world case studies, demonstrating its effectiveness with
complex designs.

### 9. [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](http://arxiv.org/pdf/2508.19689v1)

Authors: Xiaoying Zhang

Developing adaptable, extensible, and accurate task bots with minimal or zero
human intervention is a significant challenge in dialog research. This thesis
examines the obstacles and potential solutions for creating such bots, focusing
on innovative techniques that enable bots to learn and adapt autonomously in
constantly changing environments.

### 10. [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](http://arxiv.org/pdf/2508.19720v1)

Authors: Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo

In Large Language Models (LLMs) generation, there exist knowledge conflicts
and scenarios where parametric knowledge contradicts knowledge provided in the
context. Previous works studied tuning, decoding algorithms, or locating and
editing context-aware neurons to adapt LLMs to be faithful to new contextual
knowledge. However, they are usually inefficient or ineffective for large
models, not workable for black-box models, or unable to continuously adjust
LLMs' sensitivity to the knowledge provided in the context. To mitigate these
problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a
simple framework that can steer LLMs' sensitivity to contextual knowledge
continuously at a lightweight cost. Specifically, we tune two small LMs (i.e.
proxy models) and use the difference in their output distributions to shift the
original distribution of an LLM without modifying the LLM weights. In the
evaluation process, we not only design synthetic data and fine-grained metrics
to measure models' sensitivity to contextual knowledge but also use a real
conflict dataset to validate CSKS's practical efficacy. Extensive experiments
demonstrate that our framework achieves continuous and precise control over
LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity
and reduced sensitivity, thereby allowing LLMs to prioritize either contextual
or parametric knowledge as needed flexibly. Our data and code are available at
https://github.com/OliveJuiceLin/CSKS.

### Cryptography and Security

### 1. [Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC](http://arxiv.org/pdf/2508.19525v1)

Authors: Tianshi Xu, Wen-jie Lu, Jiangrui Yu, Chen Yi, Chenqi Lin, Runsheng Wang, Meng Li

This paper presents an efficient framework for private Transformer inference
that combines Homomorphic Encryption (HE) and Secure Multi-party Computation
(MPC) to protect data privacy. Existing methods often leverage HE for linear
layers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,
Softmax activation functions), but the conversion between HE and MPC introduces
significant communication costs. The proposed framework, dubbed BLB, overcomes
this by breaking down layers into fine-grained operators and further fusing
adjacent linear operators, reducing the need for HE/MPC conversions. To manage
the increased ciphertext bit width from the fused linear operators, BLB
proposes the first secure conversion protocol between CKKS and MPC and enables
CKKS-based computation of the fused operators. Additionally, BLB proposes an
efficient matrix multiplication protocol for fused computation in Transformers.
Extensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB
achieves a $21\times$ reduction in communication overhead compared to BOLT
(S\&P'24) and a $2\times$ reduction compared to Bumblebee (NDSS'25), along with
latency reductions of $13\times$ and $1.8\times$, respectively, when leveraging
GPU acceleration.

### 2. [The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again](http://arxiv.org/pdf/2508.19774v1)

Authors: Tong Liu, Guozhu Meng, Peng Zhou, Zizhuang Deng, Shuaiyin Yao, Kai Chen

Pickle deserialization vulnerabilities have persisted throughout Python's
history, remaining widely recognized yet unresolved. Due to its ability to
transparently save and restore complex objects into byte streams, many AI/ML
frameworks continue to adopt pickle as the model serialization protocol despite
its inherent risks. As the open-source model ecosystem grows, model-sharing
platforms such as Hugging Face have attracted massive participation,
significantly amplifying the real-world risks of pickle exploitation and
opening new avenues for model supply chain poisoning. Although several
state-of-the-art scanners have been developed to detect poisoned models, their
incomplete understanding of the poisoning surface leaves the detection logic
fragile and allows attackers to bypass them. In this work, we present the first
systematic disclosure of the pickle-based model poisoning surface from both
model loading and risky function perspectives. Our research demonstrates how
pickle-based model poisoning can remain stealthy and highlights critical gaps
in current scanning solutions. On the model loading surface, we identify 22
distinct pickle-based model loading paths across five foundational AI/ML
frameworks, 19 of which are entirely missed by existing scanners. We further
develop a bypass technique named Exception-Oriented Programming (EOP) and
discover 9 EOP instances, 7 of which can bypass all scanners. On the risky
function surface, we discover 133 exploitable gadgets, achieving almost a 100%
bypass rate. Even against the best-performing scanner, these gadgets maintain
an 89% bypass rate. By systematically revealing the pickle-based model
poisoning surface, we achieve practical and robust bypasses against real-world
scanners. We responsibly disclose our findings to corresponding vendors,
receiving acknowledgments and a $6000 bug bounty.

### 3. [Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping](http://arxiv.org/pdf/2508.19825v1)

Authors: Shaoor Munir, Nurullah Demir, Qian Li, Konrad Kollnig, Zubair Shafiq

The privacy community has a long track record of investigating emerging types
of web tracking techniques. Recent work has focused on compliance of web
trackers with new privacy laws such as Europe's GDPR and California's CCPA.
Despite the growing body of research documenting widespread lack of compliance
with new privacy laws, there is a lack of robust enforcement. Different from
prior work, we conduct a tech-law analysis to map decades-old U.S. laws about
interception of electronic communications--so-called wiretapping--to web
tracking. Bridging the tech-law gap for older wiretapping laws is important and
timely because, in cases where legal harm to privacy is proven, they can
provide statutory private right of action, are at the forefront of recent
privacy enforcement, and could ultimately lead to a meaningful change in the
web tracking landscape.
  In this paper, we focus on a particularly invasive tracking technique: the
use of JavaScript event listeners by third-party trackers for real-time
keystroke interception on websites. We use an instrumented web browser to crawl
a sample of the top-million websites to investigate the use of event listeners
that aligns with the criteria for wiretapping, according to U.S. wiretapping
law at the federal level and in California. We find evidence that 38.52%
websites installed third-party event listeners to intercept keystrokes, and
that at least 3.18% websites transmitted intercepted information to a
third-party server, which aligns with the criteria for wiretapping. We further
find evidence that the intercepted information such as email addresses typed
into form fields are used for unsolicited email marketing. Beyond our work that
maps the intersection between technical measurement and U.S. wiretapping law,
additional future legal research is required to determine when the wiretapping
observed in our paper passes the threshold for illegality.

### 4. [SCAMPER -- Synchrophasor Covert chAnnel for Malicious and Protective ERrands](http://arxiv.org/pdf/2508.20051v1)

Authors: Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami

We note that constituent fields (notably the fraction-of-seconds timestamp
field) in the data payload structure of the synchrophasor communication
protocol (IEEE C37.118 standard) are overprovisioned relative to real-world
usage and needs, lending themselves to abuse for embedding of covert channels.
We develop the SCAMPER (Synchrophasor Covert Channel for Malicious and
Protective ERrands) framework to exploit these overprovisioned fields for
covert communication and show that SCAMPER can be applied for both malicious
(attack) and protective (defense) purposes. Through modifications of the
timestamp field, we demonstrate that SCAMPER enables an attacker to accomplish
surreptitious communications between devices in the power system to trigger a
variety of malicious actions. These timestamp modifications can be performed
without having any impact on the operation of the power system. However, having
recognized the potential for this covert channel, we show that SCAMPER can
instead be applied for defensive security purposes as an integrated
cryptographic data integrity mechanism that can facilitate detection of false
data injection (FDI) attacks. We perform experimental studies of the proposed
methods on two Hardware-in-the-Loop (HIL) testbeds to demonstrate the
effectiveness of the proposed SCAMPER framework for both malicious and
protective purposes.

### 5. [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](http://arxiv.org/pdf/2508.19493v1)

Authors: Zhixin Lin, Jungang Li, Shidong Pan, Yibo Shi, Yue Yao, Dongliang Xu

Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

### 6. [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](http://arxiv.org/pdf/2508.19500v1)

Authors: David Noever

This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

### 7. [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](http://arxiv.org/pdf/2508.19641v1)

Authors: Lincan Li, Bolin Shen, Chenxi Zhao, Yuxiang Sun, Kaixiang Zhao, Shirui Pan, Yushun Dong

Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

### 8. [Addressing Deepfake Issue in Selfie banking through camera based authentication](http://arxiv.org/pdf/2508.19714v1)

Authors: Subhrojyoti Mukherjee, Manoranjan Mohanty

Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

### 9. [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](http://arxiv.org/pdf/2508.20083v1)

Authors: Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang

Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

### 10. [Smart Contract Intent Detection with Pre-trained Programming Language Model](http://arxiv.org/pdf/2508.20086v1)

Authors: Youwei Huang, Jianwen Li, Sen Fang, Yao Li, Peng Yang, Bin Hu, Tao Zhang

Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

### Computer Vision and Pattern Recognition

### 1. [JVLGS: Joint Vision-Language Gas Leak Segmentation](http://arxiv.org/pdf/2508.19485v1)

Authors: Xinlong Zhao, Qixiang Pang, Shan Du

Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

### 2. [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](http://arxiv.org/pdf/2508.19511v1)

Authors: Alzayat Saleh, Shunsuke Hatano, Mostafa Rahimi Azghadi

The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

### 3. [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](http://arxiv.org/pdf/2508.19527v1)

Authors: Zhiting Gao, Dan Song, Diqiong Jiang, Chao Xue, An-An Liu

Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

### 4. [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](http://arxiv.org/pdf/2508.19542v1)

Authors: Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, Shuaiwei Jiao

While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

### 5. [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](http://arxiv.org/pdf/2508.19555v1)

Authors: Yu-Wei Zhang, Tongju Han, Lipeng Gao, Mingqiang Wei, Hui Liu, Changbao Li, Caiming Zhang

This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

### 6. [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](http://arxiv.org/pdf/2508.19573v1)

Authors: Luhu Li, Bowen Lin, Mukhtiar Khan, Shujun Fu

Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

### 7. [High-Speed FHD Full-Color Video Computer-Generated Holography](http://arxiv.org/pdf/2508.19579v1)

Authors: Haomiao Zhang, Miao Cao, Xuan Yu, Hui Luo, Yanling Piao, Mengjie Qin, Zhangyuan Li, Ping Wang, Xin Yuan

Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

### 8. [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](http://arxiv.org/pdf/2508.19581v1)

Authors: Dat Nguyen Cong, Hieu Tran Bao, Hoang Thanh-Tung

Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

### 9. [Generalizing Monocular 3D Object Detection](http://arxiv.org/pdf/2508.19593v1)

Authors: Abhinav Kumar

Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

### 10. [Quantization Robustness to Input Degradations for Object Detection](http://arxiv.org/pdf/2508.19600v1)

Authors: Toghrul Karimov, Hassan Imani, Allan Kazakov

Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

### Computers and Society

### 1. [Deep Hype in Artificial General Intelligence: Uncertainty, Sociotechnical Fictions and the Governance of AI Futures](http://arxiv.org/pdf/2508.19749v1)

Authors: Andreu Belsunces Gonçalves

Artificial General Intelligence (AGI) is promoted by technology leaders and
investors as a system capable of performing all human intellectual tasks, and
potentially surpassing them. Despite its vague definition and uncertain
feasibility, AGI has attracted major investment and political attention,
fuelled by promises of civilisational transformation. This paper conceptualises
AGI as sustained by deep hype: a long-term, overpromissory dynamic articulated
through sociotechnical fictions that render not-yet-existing technologies
desirable and urgent. The analysis highlights how uncertainty, fiction, and
venture capital speculation interact to advance a cyberlibertarian and
longtermist programme that sidelines democratic oversight and reframes
regulation as obsolete, with critical implications for the governance of
technological futures.

### 2. [Bridging the Regulatory Divide: Ensuring Safety and Equity in Wearable Health Technologies](http://arxiv.org/pdf/2508.20031v1)

Authors: Akshay Kelshiker, Susan Cheng, Jivan Achar, Jane Bambauer, Leo Anthony Celi, Divya Jain, Thinh Nguyen, Harsh Patel, Nina Prakash, Alice Wong, Barbara Evans

As wearable health technologies have grown more sophisticated, the
distinction between "wellness" and "medical" devices has become increasingly
blurred. While some features undergo formal U.S. Food and Drug Administration
(FDA) review, many over-the-counter tools operate in a regulatory grey zone,
leveraging health-related data and outputs without clinical validation. Further
complicating the issue is the widespread repurposing of wellness devices for
medical uses, which can introduce safety risks beyond the reach of current
oversight. Drawing on legal analysis, case studies, and ethical considerations,
we propose an approach emphasizing distributed risk, patient-centered outcomes,
and iterative reform. Without a more pluralistic and evolving framework, the
promise of wearable health technology risks being undermined by growing
inequities, misuse, and eroded public trust.

### 3. [Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models](http://arxiv.org/pdf/2508.19492v1)

Authors: Mehmet Can Yavuz, Humza Gohar Kabir, Aylin Özkan

Objectivity in journalism has long been contested, oscillating between ideals
of neutral, fact-based reporting and the inevitability of subjective framing.
With the advent of large language models (LLMs), these tensions are now
mediated by algorithmic systems whose training data and design choices may
themselves embed cultural or ideological biases. This study investigates
geopolitical parallax-systematic divergence in news quality and subjectivity
assessments-by comparing article-level embeddings from Chinese-origin (Qwen,
BGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate
both on a human-annotated news quality benchmark spanning fifteen stylistic,
informational, and affective dimensions, and on parallel corpora covering
politically sensitive topics, including Palestine and reciprocal China-United
States coverage. Using logistic regression probes and matched-topic evaluation,
we quantify per-metric differences in predicted positive-class probabilities
between model families. Our findings reveal consistent, non-random divergences
aligned with model origin. In Palestine-related coverage, Western models assign
higher subjectivity and positive emotion scores, while Chinese models emphasize
novelty and descriptiveness. Cross-topic analysis shows asymmetries in
structural quality metrics Chinese-on-US scoring notably lower in fluency,
conciseness, technicality, and overall quality-contrasted by higher negative
emotion scores. These patterns align with media bias theory and our distinction
between semantic, emotional, and relational subjectivity, and extend LLM bias
literature by showing that geopolitical framing effects persist in downstream
quality assessment tasks. We conclude that LLM-based media evaluation pipelines
require cultural calibration to avoid conflating content differences with
model-induced bias.

### 4. [Hallucinating with AI: AI Psychosis as Distributed Delusions](http://arxiv.org/pdf/2508.19588v1)

Authors: Lucy Osler

There is much discussion of the false outputs that generative AI systems such
as ChatGPT, Claude, Gemini, DeepSeek, and Grok create. In popular terminology,
these have been dubbed AI hallucinations. However, deeming these AI outputs
hallucinations is controversial, with many claiming this is a metaphorical
misnomer. Nevertheless, in this paper, I argue that when viewed through the
lens of distributed cognition theory, we can better see the dynamic and
troubling ways in which inaccurate beliefs, distorted memories and
self-narratives, and delusional thinking can emerge through human-AI
interactions; examples of which are popularly being referred to as cases of AI
psychosis. In such cases, I suggest we move away from thinking about how an AI
system might hallucinate at us, by generating false outputs, to thinking about
how, when we routinely rely on generative AI to help us think, remember, and
narrate, we can come to hallucinate with AI. This can happen when AI introduces
errors into the distributed cognitive process, but it can also happen when AI
sustains, affirms, and elaborates on our own delusional thinking and
self-narratives, such as in the case of Jaswant Singh Chail. I also examine how
the conversational style of chatbots can lead them to play a dual-function,
both as a cognitive artefact and a quasi-Other with whom we co-construct our
beliefs, narratives, and our realities. It is this dual function, I suggest,
that makes generative AI an unusual, and particularly seductive, case of
distributed cognition.

### 5. [AI-Powered Detection of Inappropriate Language in Medical School Curricula](http://arxiv.org/pdf/2508.19883v1)

Authors: Chiman Salavati, Shannon Song, Scott A. Hale, Roberto E. Montenegro, Shiri Dori-Hacohen, Fabricio Murai

The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

### Databases

### 1. [Towards a fundamental theory of modeling discrete systems](http://arxiv.org/pdf/2508.19803v1)

Authors: Peter Fettke, Wolfgang Reisig

Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

### 2. [Bootstrapping Learned Cost Models with Synthetic SQL Queries](http://arxiv.org/pdf/2508.19807v1)

Authors: Michael Nidd, Christoph Miksovic, Thomas Gschwind, Francesco Fusco, Andrea Giovannini, Ioana Giurgiu

Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

### Distributed, Parallel, and Cluster Computing

### 1. [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](http://arxiv.org/pdf/2508.19805v1)

Authors: Shota Naito, Tsukasa Ninomiya, Koichi Wada

Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

### 2. [Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents](http://arxiv.org/pdf/2508.19504v1)

Authors: Kevin Song, Anand Jayarajan, Yaoyao Ding, Qidong Su, Zhanda Zhu, Sihang Liu, Gennady Pekhimenko

Large Language Models (LLMs) agents augmented with domain tools promise to
autonomously execute complex tasks requiring human-level intelligence, such as
customer service and digital assistance. However, their practical deployment is
often limited by their low success rates under complex real-world environments.
To tackle this, prior research has primarily focused on improving the agents
themselves, such as developing strong agentic LLMs, while overlooking the role
of the system environment in which the agent operates.
  In this paper, we study a complementary direction: improving agent success
rates by optimizing the system environment in which the agent operates. We
collect 142 agent traces (3,656 turns of agent-environment interactions) across
5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we
propose a taxonomy for agent-environment interaction failures that includes 6
failure modes. Guided by these findings, we design Aegis, a set of targeted
environment optimizations: 1) environment observability enhancement, 2) common
computation offloading, and 3) speculative agentic actions. These techniques
improve agent success rates on average by 6.7-12.5%, without any modifications
to the agent and underlying LLM.

### 3. [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](http://arxiv.org/pdf/2508.19559v1)

Authors: Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu

Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

### 4. [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](http://arxiv.org/pdf/2508.19591v1)

Authors: Jiakui Shen, Yunqi Mi, Guoshuai Zhao, Jialie Shen, Xueming Qian

Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

### 5. [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](http://arxiv.org/pdf/2508.19868v1)

Authors: Geraldo F. Oliveira

Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

### 6. [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](http://arxiv.org/pdf/2508.19495v1)

Authors: Muhammad Ahmed Mohsin, Junaid Ahmad, Muhammad Hamza Nawaz, Muhammad Ali Jamshed

Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

### 7. [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](http://arxiv.org/pdf/2508.19670v1)

Authors: Diogo Costa, Jose Martins, Sandro Pinto

As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

### 8. [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](http://arxiv.org/pdf/2508.20016v1)

Authors: Matthias Maiterth, Wesley H. Brewer, Jaya S. Kuruvella, Arunavo Dey, Tanzima Z. Islam, Kevin Menear, Dmitry Duplyakin, Rashadul Kabir, Tapasya Patki, Terry Jones, Feiyi Wang

Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

### Digital Libraries

### 1. [Interactive Graph Visualization and TeamingRecommendation in an Interdisciplinary Project'sTalent Knowledge Graph](http://arxiv.org/pdf/2508.19489v1)

Authors: Jiawei Xu, Juichien Chen, Yilin Ye, Zhandos Sembay, Swathi Thaker, Pamela Payne-Foster, Jake Chen, Ying Ding

Interactive visualization of large scholarly knowledge graphs combined with
LLM reasoning shows promise butremains under-explored. We address this gap by
developing an interactive visualization system for the Cell Map forAI Talent
Knowledge Graph (28,000 experts and 1,179 biomedical datasets). Our approach
integrates WebGLvisualization with LLM agents to overcome limitations of
traditional tools such as Gephi, particularly for large-scaleinteractive node
handling. Key functionalities include responsive exploration, filtering, and
AI-drivenrecommendations with justifications. This integration can potentially
enable users to effectively identify potentialcollaborators and relevant
dataset users within biomedical and AI research communities. The system
contributes anovel framework that enhances knowledge graph exploration through
intuitive visualization and transparent, LLM-guided recommendations. This
adaptable solution extends beyond the CM4AI community to other large
knowledgegraphs, improving information representation and decision-making.
Demo: https://cm4aikg.vercel.app/

### 2. [The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music](http://arxiv.org/pdf/2508.19876v1)

Authors: Sepideh Shafiei, Shapour Hakam

We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level,
open-access corpus designed for the computational study of Iranian classical
music, with a particular emphasis on the radif, a structured repertoire of
modal-melodic units central to pedagogy and performance. The dataset combines
symbolic MIDI representations, phrase-level audio-MIDI alignment, musicological
transcriptions in PDF format, and comparative tables of theoretical information
curated from a range of performers and scholars. We outline the multi-phase
construction process, including segment annotation, alignment methods, and a
structured system of identifier codes to reference individual musical units.
The current release includes the complete radif of Karimi; MIDI files and
metadata from Mirza Abdollah's radif; selected segments from the vocal radif of
Davami, as transcribed by Payvar and Fereyduni; and a dedicated section
featuring audio-MIDI examples of tahrir ornamentation performed by prominent
20th-century vocalists. While the symbolic and analytical components are
released under an open-access license (CC BY-NC 4.0), some referenced audio
recordings and third-party transcriptions are cited using discographic
information to enable users to locate the original materials independently,
pending copyright permission. Serving both as a scholarly archive and a
resource for computational analysis, this dataset supports applications in
ethnomusicology, pedagogy, symbolic audio research, cultural heritage
preservation, and AI-driven tasks such as automatic transcription and music
generation. We welcome collaboration and feedback to support its ongoing
refinement and broader integration into musicological and machine learning
workflows.

### Discrete Mathematics

### 1. [Fractional domatic number and minimum degree](http://arxiv.org/pdf/2508.19617v1)

Authors: Quentin Chuet, Hugo Demaret, Hoang La, François Pirot

The domatic number of a graph $G$ is the maximum number of pairwise disjoint
dominating sets of $G$. We are interested in the LP-relaxation of this
parameter, which is called the fractional domatic number of $G$. We study its
extremal value in the class of graphs of minimum degree $d$. The fractional
domatic number of a graph of minimum degree $d$ is always at most $d+1$, and at
least $(1-o(1))\, d/\ln d$ as $d\to \infty$. This is asymptotically tight even
within the class of split graphs. Our main result concerns the case $d=2$; we
show that, excluding $8$ exceptional graphs, the fractional domatic number of
every connected graph of minimum degree (at least) $2$ is at least $5/2$. We
also show that this bound cannot be improved if only finitely many graphs are
excluded, even when restricting to bipartite graphs of girth at least $6$. This
proves in a stronger sense a conjecture by Gadouleau, Harms, Mertzios, and
Zamaraev (2024). This also extends and generalises results from McCuaig and
Shepherd (1989), from Fujita, Kameda, and Yamashita (2000), and from Abbas,
Egerstedt, Liu, Thomas, and Whalen (2016). Finally, we show that planar graphs
of minimum degree at least $2$ and girth at least $g$ have fractional domatic
number at least $3 - O(1/g)$ as $g\to\infty$.

### 2. [Growth Forms of Tilings](http://arxiv.org/pdf/2508.19928v1)

Authors: Peter Hilgers, Anton Shutov

The growth form (or corona limit) of a tiling is the limit form of its
coordination shells, i.e. its set of tiles located at a fixed distance from
some tile. We give an overview of current results, conjectures and open
questions about growth forms, including periodic, multigrid, substitution, and
hat tilings.

### 3. [An algorithm for accurate and simple-looking metaphorical maps](http://arxiv.org/pdf/2508.19810v1)

Authors: Eleni Katsanou, Tamara Mchedlidze, Antonios Symvonis, Thanos Tolias

"Metaphorical maps" or "contact representations" are visual representations
of vertex-weighted graphs that rely on the geographic map metaphor. The
vertices are represented by countries, the weights by the areas of the
countries, and the edges by contacts/ boundaries among them. The accuracy with
which the weights are mapped to areas and the simplicity of the polygons
representing the countries are the two classical optimization goals for
metaphorical maps. Mchedlidze and Schnorr [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022] presented a force-based algorithm that
creates metaphorical maps that balance between these two optimization goals.
Their maps look visually simple, but the accuracy of the maps is far from
optimal - the countries' areas can vary up to 30% compared to required. In this
paper, we provide a multi-fold extension of the algorithm in [Metaphoric Maps
for Dynamic Vertex-weighted Graphs, EuroVis 2022]. More specifically:
  1. Towards improving accuracy: We introduce the notion of region stiffness
and suggest a technique for varying the stiffness based on the current pressure
of map regions.
  2. Towards maintaining simplicity: We introduce a weight coefficient to the
pressure force exerted on each polygon point based on whether the corresponding
point appears along a narrow passage.
  3. Towards generality: We cover, in contrast to [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022], non-triangulated graphs. This is done by
either generating points where more than three regions meet or by introducing
holes in the metaphorical map.
  We perform an extended experimental evaluation that, among other results,
reveals that our algorithm is able to construct metaphorical maps with nearly
perfect area accuracy with a little sacrifice in their simplicity.

### 4. [Internally-Convex Drawings of Outerplanar Graphs in Small Area](http://arxiv.org/pdf/2508.19913v1)

Authors: Michael A. Bekos, Giordano Da Lozzo, Fabrizio Frati, Giuseppe Liotta, Antonios Symvonis

A well-known result by Kant [Algorithmica, 1996] implies that n-vertex
outerplane graphs admit embedding-preserving planar straight-line grid drawings
where the internal faces are convex polygons in $O(n^2)$ area. In this paper,
we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also
consider outerplanar drawings in which the internal faces are required to be
strictly-convex polygons. In this setting, we consider outerplanar graphs whose
weak dual is a path and give a drawing algorithm that achieves $\Theta(nk^2)$
area, where $k$ is the maximum size of an internal facial cycle.

### Data Structures and Algorithms

### 1. [An Optimal Sorting Algorithm for Persistent Random Comparison Faults](http://arxiv.org/pdf/2508.19785v1)

Authors: Barbara Geissmann, Stefano Leucci, Chih-Hung Liu, Paolo Penna

We consider the problem of sorting $n$ elements subject to persistent random
comparison errors. In this problem, each comparison between two elements can be
wrong with some fixed (small) probability $p$, and comparing the same pair of
elements multiple times always yields the same result. Sorting perfectly in
this model is impossible, and the objective is to minimize the dislocation of
each element in the output sequence, i.e., the difference between its position
in the sequence and its true rank.
  In this paper, we present the first $O(n\log n)$-time sorting algorithm that
guarantees both $O(\log n)$ maximum dislocation and $O(n)$ total dislocation
with high probability when $p<\frac{1}{4}$. This settles the time complexity
sorting with persistent comparison errors in the given range of $p$ and shows
that comparison errors do not increase its computational difficulty. Indeed,
$\Omega(n\log n)$ time is necessary to archive a maximum dislocation of $O(\log
n)$ even without comparison errors. Moreover, we prove that no algorithm can
guarantee a maximum dislocation of $o(\log n)$ with high probability, nor a
total dislocation of $o(n)$ in expectation.
  To develop our sorting algorithm, we solve two related sub-problems, which
might be of independent interest. More precisely, we show that $O(\log n)$ time
suffices to find a position in which to insert a new element $x$ in an
almost-sorted sequence $S$ of $n$ elements having dislocation at most
$d=\Omega(\log n)$, so that the dislocation of $x$ in the resulting sequence is
$O(d)$ with high probability (which can be equivalently thought as the problem
of estimating the rank of $x$ in $S$). We also show that the maximum (resp.
total) dislocation of an approximately sorted sequence $S$ of $n$ elements can
be lowered to $O(\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is
an upper bound on the maximum dislocation of $S$.

### 2. [Optimizing Wiggle in Storylines](http://arxiv.org/pdf/2508.19802v1)

Authors: Alexander Dobler, Tim Hegemann, Martin Nöllenburg, Alexander Wolff

A storyline visualization shows interactions between characters over time.
Each character is represented by an x-monotone curve. Time is mapped to the
x-axis, and groups of characters that interact at a particular point $t$ in
time must be ordered consecutively in the y-dimension at $x=t$. The predominant
objective in storyline optimization so far has been the minimization of
crossings between (blocks of) characters. Building on this work, we investigate
another important, but less studied quality criterion, namely the minimization
of wiggle, i.e., the amount of vertical movement of the characters over time.
Given a storyline instance together with an ordering of the characters at any
point in time, we show that wiggle count minimization is NP-complete. In
contrast, we provide algorithms based on mathematical programming to solve
linear wiggle height minimization and quadratic wiggle height minimization
efficiently. Finally, we introduce a new method for routing character curves
that focuses on keeping distances between neighboring curves constant as long
as they run in parallel. We have implemented our algorithms, and we conduct a
case study that explores the differences between the three optimization
objectives. We use existing benchmark data, but we also present a new use case
for storylines, namely the visualization of rolling stock schedules in railway
operation.

### 3. [Distributed Sparsest Cut via Eigenvalue Estimation](http://arxiv.org/pdf/2508.19898v1)

Authors: Yannic Maus, Tijn de Vos

We give new, improved bounds for approximating the sparsest cut value or in
other words the conductance $\phi$ of a graph in the CONGEST model. As our main
result, we present an algorithm running in $O(\log^2 n/\phi)$ rounds in which
every vertex outputs a value $\tilde \phi$ satisfying $\phi \le \tilde \phi \le
\sqrt{2.01\phi}$. In most regimes, our algorithm improves significantly over
the previously fastest algorithm for the problem [Chen, Meierhans, Probst
Gutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way
conductance.
  We obtain these results, by approximating the eigenvalues of the normalized
Laplacian matrix $L:=I-\rm{Deg}^{-1/2}A\rm{Deg}^ {-1/2}$, where, $A$ is the
adjacency matrix and $\rm{Deg}$ is the diagonal matrix with the weighted
degrees on the diagonal. The previous state of the art sparsest cut algorithm
is in the technical realm of expander decompositions. Our algorithms, on the
other hand, are relatively simple and easy to implement. At the core, they rely
on the well-known power method, which comes down to repeatedly multiplying the
Laplacian with a vector. This operation can be performed in a single round in
the CONGEST model. All our algorithms apply to weighted, undirected graphs. Our
lower bounds apply even in unweighted graphs.

### 4. [Bipartite Matching with Pair-Dependent Bounds](http://arxiv.org/pdf/2508.20002v1)

Authors: Shaul Rosner, Tami Tamir

Let $G=(U \cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$
represents machines. We study a new variant of the bipartite matching problem
in which each job in $U$ can be matched to at most one machine in $V$, and the
number of jobs that can be assigned to a machine depends on the specific jobs
matched to it. These pair-dependent bounds reflect systems where different jobs
have varying tolerance for congestion, determined by the specific machine they
are assigned to.
  We define a bipartite PD-matching as a set of edges $M \subseteq E$ that
satisfies these job-to-machine tolerance constraints. This variant of matching
extends well-known matching problems, however, despite its relevance to
real-world systems, it has not been studied before. We study bipartite
PD-matchings with the objective of maximizing the matching size. As we show,
the problem exhibits significant differences from previously studied matching
problems. We analyze its computational complexity both in the general case and
for specific restricted instances, presenting hardness results alongside
optimal and approximation algorithms.

### 5. [Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem](http://arxiv.org/pdf/2508.20041v1)

Authors: Thomas Bläsius, Henrik Csöre, Max Göttlicher, Elly Schmidt, Wendy Yi

Motivated by hierarchical networks, we introduce the Flow-weighted Layered
Metric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the
Euclidean Steiner tree with layered structure and capacity constraints per
layer. The goal is to construct a cost-optimal Steiner forest connecting a set
of sources to a set of sinks under load-dependent edge costs. We prove that
FLaMECaST is NP-hard to approximate, even in restricted cases where all sources
lie on a circle. However, assuming few additional constraints for such
instances, we design a dynamic program that achieves a $\left(1 +
\frac{1}{2^n}\right)$-approximation in polynomial time. By generalizing the
structural insights the dynamic program is based on, we extend the approach to
certain settings, where all sources are positioned on a convex polygon.

### 6. [Visualizing Treewidth](http://arxiv.org/pdf/2508.19935v1)

Authors: Alvin Chiu, Thomas Depian, David Eppstein, Michael T. Goodrich, Martin Nöllenburg

A witness drawing of a graph is a visualization that clearly shows a given
property of a graph. We study and implement various drawing paradigms for
witness drawings to clearly show that graphs have bounded pathwidth or
treewidth. Our approach draws the tree decomposition or path decomposition as a
tree of bags, with induced subgraphs shown in each bag, and with ''tracks'' for
each graph vertex connecting its copies in multiple bags. Within bags, we
optimize the vertex layout to avoid crossings of edges and tracks. We implement
a visualization prototype for crossing minimization using dynamic programming
for graphs of small width and heuristic approaches for graphs of larger width.
We introduce a taxonomy of drawing styles, which render the subgraph for each
bag as an arc diagram with one or two pages or as a circular layout with
straight-line edges, and we render tracks either with straight lines or with
orbital-radial paths.

### 7. [An algorithm for accurate and simple-looking metaphorical maps](http://arxiv.org/pdf/2508.19810v1)

Authors: Eleni Katsanou, Tamara Mchedlidze, Antonios Symvonis, Thanos Tolias

"Metaphorical maps" or "contact representations" are visual representations
of vertex-weighted graphs that rely on the geographic map metaphor. The
vertices are represented by countries, the weights by the areas of the
countries, and the edges by contacts/ boundaries among them. The accuracy with
which the weights are mapped to areas and the simplicity of the polygons
representing the countries are the two classical optimization goals for
metaphorical maps. Mchedlidze and Schnorr [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022] presented a force-based algorithm that
creates metaphorical maps that balance between these two optimization goals.
Their maps look visually simple, but the accuracy of the maps is far from
optimal - the countries' areas can vary up to 30% compared to required. In this
paper, we provide a multi-fold extension of the algorithm in [Metaphoric Maps
for Dynamic Vertex-weighted Graphs, EuroVis 2022]. More specifically:
  1. Towards improving accuracy: We introduce the notion of region stiffness
and suggest a technique for varying the stiffness based on the current pressure
of map regions.
  2. Towards maintaining simplicity: We introduce a weight coefficient to the
pressure force exerted on each polygon point based on whether the corresponding
point appears along a narrow passage.
  3. Towards generality: We cover, in contrast to [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022], non-triangulated graphs. This is done by
either generating points where more than three regions meet or by introducing
holes in the metaphorical map.
  We perform an extended experimental evaluation that, among other results,
reveals that our algorithm is able to construct metaphorical maps with nearly
perfect area accuracy with a little sacrifice in their simplicity.

### 8. [Internally-Convex Drawings of Outerplanar Graphs in Small Area](http://arxiv.org/pdf/2508.19913v1)

Authors: Michael A. Bekos, Giordano Da Lozzo, Fabrizio Frati, Giuseppe Liotta, Antonios Symvonis

A well-known result by Kant [Algorithmica, 1996] implies that n-vertex
outerplane graphs admit embedding-preserving planar straight-line grid drawings
where the internal faces are convex polygons in $O(n^2)$ area. In this paper,
we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also
consider outerplanar drawings in which the internal faces are required to be
strictly-convex polygons. In this setting, we consider outerplanar graphs whose
weak dual is a path and give a drawing algorithm that achieves $\Theta(nk^2)$
area, where $k$ is the maximum size of an internal facial cycle.

### Emerging Technologies

### 1. [MC for Gastroretentive Drug Delivery](http://arxiv.org/pdf/2508.19739v1)

Authors: Sebastian Lotter, Marco Seiter, Maryam Pirmoradi, Lukas Brand, Dagmar Fischer, Robert Schober

Recently, bacterial nanocellulose (BNC), a biological material produced by
non-pathogenic bacteria that possesses excellent material properties for
various medical applications, has received increased interest as a carrier
system for drug delivery. However, the vast majority of existing studies on
drug release from BNC are feasibility studies with modeling and design aspects
remaining largely unexplored. To narrow this research gap, this paper proposes
a novel model for the drug release from BNC. Specifically, the drug delivery
system considered in this paper consists of a BNC fleece coated with a polymer.
The polymer coating is used as an additional diffusion barrier, enabling the
controlled release of an active pharmaceutical ingredient. The proposed
physics-based model reflects the geometry of the BNC and incorporates the
impact of the polymer coating on the drug release. Hence, it can be useful for
designing BNC-based drug delivery systems in the future. The accuracy of the
model is validated with experimental data obtained in wet lab experiments.

### 2. [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](http://arxiv.org/pdf/2508.19905v1)

Authors: Imad Ali Shah, Jiarong Li, Roshan George, Tim Brophy, Enda Ward, Martin Glavin, Edward Jones, Brian Deegan

Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

### 3. [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](http://arxiv.org/pdf/2508.20016v1)

Authors: Matthias Maiterth, Wesley H. Brewer, Jaya S. Kuruvella, Arunavo Dey, Tanzima Z. Islam, Kevin Menear, Dmitry Duplyakin, Rashadul Kabir, Tapasya Patki, Terry Jones, Feiyi Wang

Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

### Graphics

### 1. [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](http://arxiv.org/pdf/2508.19518v1)

Authors: Hail Song, Seokhwan Yang, Woontack Woo

We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

### 2. [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](http://arxiv.org/pdf/2508.20080v1)

Authors: Changha Shin, Woong Oh Cho, Seon Joo Kim

360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

### Human-Computer Interaction

### 1. [Haptic Tracing: A new paradigm for spatialized Haptic rendering](http://arxiv.org/pdf/2508.19703v1)

Authors: Tom Roy, Yann Glemarec, Gurvan Lecuyer, Quentin Galvane, Philippe Guillotel, Ferran Argelaguet

Haptic technology enhances interactive experiences by providing force and
tactile feedback, improving user performance and immersion. However, despite
advancements, creating tactile experiences still remains challenging due to
device diversity and complexity. Most available haptic frameworks rely on
trigger-based or event-based systems, and disregard the information of the 3D
scene to render haptic information. This paper introduces Haptic Tracing, a
novel method for spatial haptic rendering that simplifies the creation of
interactive haptic experiences without relying on physical simulations. It uses
concepts from visual and audio rendering to model and propagate haptic
information through a 3D scene. The paper also describes how our proposed
haptic rendering method can be used to create a vibrotactile rendering system,
enabling the creation of perceptually coherent and dynamic haptic interactions.
Finally, the paper discusses a user study that explores the role of the haptic
propagation and multi-actuator rendering on the users' haptic experience. The
results show that our approach significantly enhances the realism and the
expressivity of the haptic feedback, showcasing its potential for developing
more complex and realistic haptic experiences.

### 2. [Burst: Collaborative Curation in Connected Social Media Communities](http://arxiv.org/pdf/2508.19768v1)

Authors: Yutong Zhang, Taeuk Kang, Sydney Yeh, Anavi Baddepudi, Lindsay Popowski, Tiziano Piccardi, Michael S. Bernstein

Positive social interactions can occur in groups of many shapes and sizes,
spanning from small and private to large and open. However, social media tends
to binarize our experiences into either isolated small groups or into large
public squares. In this paper, we introduce Burst, a social media design that
allows users to share and curate content between many spaces of varied size and
composition. Users initially post content to small trusted groups, who can then
burst that content, routing it to the groups that would be the best audience.
We instantiate this approach into a mobile phone application, and demonstrate
through a ten-day field study (N=36) that Burst enabled a participatory
curation culture. With this work, we aim to articulate potential new design
directions for social media sharing.

### 3. [Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices](http://arxiv.org/pdf/2508.19818v1)

Authors: Rania Islmabouli, Marlene Brunner, Devender Kumar, Mahdi Sareban, Gunnar Treff, Michael Neudorfer, Josef Niebauer, Arne Bathke, Jan David Smeddinck

Wearable devices with photoplethysmography (PPG) sensors are widely used to
monitor heart rate (HR), yet often suffer from accuracy issues. However, users
typically do not receive an indication of potential measurement errors. We
present a real-time warning system that detects and communicates inaccuracies
in PPG-derived HR, aiming to enhance transparency and trust. Using data from
Polar and Garmin devices, we trained a deep learning model to classify HR
accuracy using only the derived HR signal. The system detected over 80% of
inaccurate readings. By providing interpretable, real-time feedback directly to
users, our work contributes to HCI by promoting user awareness, informed
decision-making, and trust in wearable health technology.

### 4. [Lessons from Biophilic Design: Rethinking Affective Interaction Design in Built Environments](http://arxiv.org/pdf/2508.19867v1)

Authors: Shruti Rao, Judith Good, Hamed Alavi

The perspectives of affective interaction in built environments are largely
overlooked and instead dominated by affective computing approaches that view
emotions as "static", computable states to be detected and regulated. To
address this limitation, we interviewed architects to explore how biophilic
design -- our deep-rooted emotional connection with nature -- could shape
affective interaction design in smart buildings. Our findings reveal that
natural environments facilitate self-directed emotional experiences through
spatial diversity, embodied friction, and porous sensory exchanges. Based on
this, we introduce three design principles for discussion at the Affective
Interaction workshop: (1) Diversity of Spatial Experiences, (2) Self-Reflection
Through Complexity & Friction, and (3) Permeability & Sensory Exchange with the
Outside World, while also examining the challenges of integrating these
perspectives into built environments.

### 5. [Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations](http://arxiv.org/pdf/2508.19942v1)

Authors: Martin Benderoth, Patrick Gebhard, Christian Keller, C. Benjamin Nakhosteen, Stefan Schaffer, Tanja Schneeberger

This paper introduces a novel approach to tackle the challenges of preserving
and transferring tacit knowledge--deep, experience-based insights that are hard
to articulate but vital for decision-making, innovation, and problem-solving.
Traditional methods rely heavily on human facilitators, which, while effective,
are resource-intensive and lack scalability. A promising alternative is the use
of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer
facilitators. These agents interact autonomously and socially intelligently
with users through multimodal behaviors (verbal, paraverbal, nonverbal),
simulating expert roles in various organizational contexts. SIAs engage
employees in empathic, natural-language dialogues, helping them externalize
insights that might otherwise remain unspoken. Their success hinges on building
trust, as employees are often hesitant to share tacit knowledge without
assurance of confidentiality and appreciation. Key technologies include Large
Language Models (LLMs) for generating context-relevant dialogue,
Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and
Chain-of-Thought (CoT) prompting to guide structured reflection. These enable
SIAs to actively elicit knowledge, uncover implicit assumptions, and connect
insights to broader organizational contexts. Potential applications span
onboarding, where SIAs support personalized guidance and introductions, and
knowledge retention, where they conduct structured interviews with retiring
experts to capture heuristics behind decisions. Success depends on addressing
ethical and operational challenges such as data privacy, algorithmic bias, and
resistance to AI. Transparency, robust validation, and a culture of trust are
essential to mitigate these risks.

### 6. [CapTune: Adapting Non-Speech Captions With Anchored Generative Models](http://arxiv.org/pdf/2508.19971v1)

Authors: Jeremy Zhengqi Huang, Caluã de Lacerda Pataca, Liang-Yuan Wu, Dhruv Jain

Non-speech captions are essential to the video experience of deaf and hard of
hearing (DHH) viewers, yet conventional approaches often overlook the diversity
of their preferences. We present CapTune, a system that enables customization
of non-speech captions based on DHH viewers' needs while preserving creator
intent. CapTune allows caption authors to define safe transformation spaces
using concrete examples and empowers viewers to personalize captions across
four dimensions: level of detail, expressiveness, sound representation method,
and genre alignment. Evaluations with seven caption creators and twelve DHH
participants showed that CapTune supported creators' creative control while
enhancing viewers' emotional engagement with content. Our findings also reveal
trade-offs between information richness and cognitive load, tensions between
interpretive and descriptive representations of sound, and the
context-dependent nature of caption preferences.

### 7. [FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones](http://arxiv.org/pdf/2508.20034v1)

Authors: Xia Su, Ruiqi Chen, Jingwei Ma, Chu Li, Jon E. Froehlich

Indoor mapping data is crucial for routing, navigation, and building
management, yet such data are widely lacking due to the manual labor and
expense of data collection, especially for larger indoor spaces. Leveraging
recent advancements in commodity drones and photogrammetry, we introduce
FlyMeThrough -- a drone-based indoor scanning system that efficiently produces
3D reconstructions of indoor spaces with human-AI collaborative annotations for
key indoor points-of-interest (POI) such as entrances, restrooms, stairs, and
elevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and
functionality. To investigate use cases and solicit feedback from target
stakeholders, we also conducted a qualitative user study with five building
managers and five occupants. Our findings indicate that FlyMeThrough can
efficiently and precisely create indoor 3D maps for strategic space planning,
resource management, and navigation.

### 8. [Orchid: Orchestrating Context Across Creative Workflows with Generative AI](http://arxiv.org/pdf/2508.19517v1)

Authors: Srishti Palani, Gonzalo Ramos

Context is critical for meaningful interactions between people and Generative
AI (GenAI). Yet mainstream tools offer limited means to orchestrate it,
particularly across workflows that span multiple interactions, sessions, and
models, as often occurs in creative projects. Re specifying prior details,
juggling diverse artifacts, and dealing with context drift overwhelm users,
obscure intent, and curtail creativity. To address these challenges, we present
Orchid, a system that gives its users affordances to specify, reference, and
monitor context throughout evolving workflows. Specifically, Orchid enables
users to (1) specify context related to the project, themselves, and different
styles, (2) reference these via explicit mentions, inline selection, or
implicit grounding, and (3) monitor context assigned to different interactions
across the workflow. In a within-subjects study (n=12), participants using
Orchid to execute creative tasks (compared to a baseline toolkit of web search,
LLM-based chat, and digital notebooks) produced more novel and feasible
outcomes, reporting greater alignment between their intent and the AI's
responses, higher perceived control, and increased transparency. By
prioritizing context orchestration, Orchid offers an actionable step toward
next generation GenAI tools that support complex, iterative workflows -
enabling creators and AI to stay aligned and augment their creative potential.

### 9. [PersoNo: Personalised Notification Urgency Classifier in Mixed Reality](http://arxiv.org/pdf/2508.19622v1)

Authors: Jingyao Zheng, Haodi Weng, Xian Wang, Chengbin Cui, Sven Mayer, Chi-lok Tai, Lik-Hang Lee

Mixed Reality (MR) is increasingly integrated into daily life, providing
enhanced capabilities across various domains. However, users face growing
notification streams that disrupt their immersive experience. We present
PersoNo, a personalised notification urgency classifier for MR that
intelligently classifies notifications based on individual user preferences.
Through a user study (N=18), we created the first MR notification dataset
containing both self-labelled and interaction-based data across activities with
varying cognitive demands. Our thematic analysis revealed that, unlike in
mobiles, the activity context is equally important as the content and the
sender in determining notification urgency in MR. Leveraging these insights, we
developed PersoNo using large language models that analyse users replying
behaviour patterns. Our multi-agent approach achieved 81.5% accuracy and
significantly reduced false negative rates (0.381) compared to baseline models.
PersoNo has the potential not only to reduce unnecessary interruptions but also
to offer users understanding and control of the system, adhering to
Human-Centered Artificial Intelligence design principles.

### 10. [Attention is also needed for form design](http://arxiv.org/pdf/2508.19708v1)

Authors: B. Sankar, Dibakar Sen

Conventional product design is a cognitively demanding process, limited by
its time-consuming nature, reliance on subjective expertise, and the opaque
translation of inspiration into tangible concepts. This research introduces a
novel, attention-aware framework that integrates two synergistic systems:
EUPHORIA, an immersive Virtual Reality environment using eye-tracking to
implicitly capture a designer's aesthetic preferences, and RETINA, an agentic
AI pipeline that translates these implicit preferences into concrete design
outputs. The foundational principles were validated in a two-part study. An
initial study correlated user's implicit attention with explicit preference and
the next one correlated mood to attention. A comparative study where 4
designers solved challenging design problems using 4 distinct workflows, from a
manual process to an end-to-end automated pipeline, showed the integrated
EUPHORIA-RETINA workflow was over 4 times more time-efficient than the
conventional method. A panel of 50 design experts evaluated the 16 final
renderings. Designs generated by the fully automated system consistently
received the highest Worthiness (calculated by an inverse Plackett-Luce model
based on gradient descent optimization) and Design Effectiveness scores,
indicating superior quality across 8 criteria: novelty, visual appeal,
emotional resonance, clarity of purpose, distinctiveness of silhouette, implied
materiality, proportional balance, & adherence to the brief. This research
presents a validated paradigm shift from traditional Computer-Assisted Design
(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By
automating logistical and skill-dependent generative tasks, the proposed
framework elevates the designer's role to that of a creative director,
synergizing human intuition with the generative power of agentic AI to produce
higher-quality designs more efficiently.

### Information Retrieval

### 1. [A Hybrid Recommendation Framework for Enhancing User Engagement in Local News](http://arxiv.org/pdf/2508.19539v1)

Authors: Payam Pourashraf, Bamshad Mobasher

Local news organizations face an urgent need to boost reader engagement amid
declining circulation and competition from global media. Personalized news
recommender systems offer a promising solution by tailoring content to user
interests. Yet, conventional approaches often emphasize general preferences and
may overlook nuanced or eclectic interests in local news.
  We propose a hybrid news recommender that integrates local and global
preference models to improve engagement. Building on evidence of the value of
localized models, our method unifies local and non-local predictors in one
framework. The system adaptively combines recommendations from a local model,
specialized in region-specific content, and a global model that captures
broader preferences. Ensemble strategies and multiphase training balance the
two.
  We evaluated the model on two datasets: a synthetic set based on Syracuse
newspaper distributions and a Danish dataset (EB-NeRD) labeled for local and
non-local content with an LLM. Results show our integrated approach outperforms
single-model baselines in accuracy and coverage, suggesting improved
personalization that can drive user engagement.
  The findings have practical implications for publishers, especially local
outlets. By leveraging both community-specific and general user interests, the
hybrid recommender can deliver more relevant content, increasing retention and
subscriptions. In sum, this work introduces a new direction for recommender
systems, bridging local and global models to revitalize local news consumption
through scalable, personalized user experiences.

### 2. [Improving Recommendation Fairness via Graph Structure and Representation Augmentation](http://arxiv.org/pdf/2508.19547v1)

Authors: Tongxin Xu, Wenqiang Liu, Chenzhong Bin, Cihan Xiao, Zhixin Zeng, Tianlong Gu

Graph Convolutional Networks (GCNs) have become increasingly popular in
recommendation systems. However, recent studies have shown that GCN-based
models will cause sensitive information to disseminate widely in the graph
structure, amplifying data bias and raising fairness concerns. While various
fairness methods have been proposed, most of them neglect the impact of biased
data on representation learning, which results in limited fairness improvement.
Moreover, some studies have focused on constructing fair and balanced data
distributions through data augmentation, but these methods significantly reduce
utility due to disruption of user preferences. In this paper, we aim to design
a fair recommendation method from the perspective of data augmentation to
improve fairness while preserving recommendation utility. To achieve
fairness-aware data augmentation with minimal disruption to user preferences,
we propose two prior hypotheses. The first hypothesis identifies sensitive
interactions by comparing outcomes of performance-oriented and fairness-aware
recommendations, while the second one focuses on detecting sensitive features
by analyzing feature similarities between biased and debiased representations.
Then, we propose a dual data augmentation framework for fair recommendation,
which includes two data augmentation strategies to generate fair augmented
graphs and feature representations. Furthermore, we introduce a debiasing
learning method that minimizes the dependence between the learned
representations and sensitive information to eliminate bias. Extensive
experiments on two real-world datasets demonstrate the superiority of our
proposed framework.

### 3. [Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning](http://arxiv.org/pdf/2508.19855v1)

Authors: Junnan Dong, Siyu An, Yifei Yu, Qian-Wen Zhang, Linhao Luo, Xiao Huang, Yunsheng Wu, Di Yin, Xing Sun

Graph retrieval-augmented generation (GraphRAG) has effectively enhanced
large language models in complex reasoning by organizing fragmented knowledge
into explicitly structured graphs. Prior efforts have been made to improve
either graph construction or graph retrieval in isolation, yielding suboptimal
performance, especially when domain shifts occur. In this paper, we propose a
vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the
entire framework as an intricate integration. Specifically, (i) a seed graph
schema is introduced to bound the automatic extraction agent with targeted
entity types, relations and attribute types, also continuously expanded for
scalability over unseen domains; (ii) To obtain higher-level knowledge upon the
schema, we develop novel dually-perceived community detection, fusing
structural topology with subgraph semantics for comprehensive knowledge
organization. This naturally yields a hierarchical knowledge tree that supports
both top-down filtering and bottom-up reasoning with community summaries; (iii)
An agentic retriever is designed to interpret the same graph schema to
transform complex queries into tractable and parallel sub-queries. It
iteratively performs reflection for more advanced reasoning; (iv) To alleviate
the knowledge leaking problem in pre-trained LLM, we propose a tailored
anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures
the real performance of the GraphRAG frameworks. Extensive experiments across
six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,
remarkably moving the Pareto frontier with up to 90.71% saving of token costs
and 16.62% higher accuracy over state-of-the-art baselines. The results
indicate our adaptability, allowing seamless domain transfer with minimal
intervention on schema.

### 4. [Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization](http://arxiv.org/pdf/2508.19918v1)

Authors: Manato Tajiri, Michimasa Inaba

Conversational Recommender Systems (CRSs) aim to elicit user preferences via
natural dialogue to provide suitable item recommendations. However, current
CRSs often deviate from realistic human interactions by rapidly recommending
items in brief sessions. This work addresses this gap by leveraging Large
Language Models (LLMs) to generate dialogue summaries from dialogue history and
item recommendation information from item description. This approach enables
the extraction of both explicit user statements and implicit preferences
inferred from the dialogue context. We introduce a method using Direct
Preference Optimization (DPO) to ensure dialogue summary and item
recommendation information are rich in information crucial for effective
recommendations. Experiments on two public datasets validate our method's
effectiveness in fostering more natural and realistic conversational
recommendation processes.Our implementation is publicly available
at:https://github.com/UEC-InabaLab/Refining-LLM-Text

### 5. [A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation](http://arxiv.org/pdf/2508.19507v1)

Authors: Kyungho Kim, Sunwoo Kim, Geon Lee, Kijung Shin

In e-commerce, where users face a vast array of possible item choices,
recommender systems are vital for helping them discover suitable items they
might otherwise overlook. While many recommender systems primarily rely on a
user's purchase history, recent multi-behavior recommender systems incorporate
various auxiliary user behaviors, such as item clicks and cart additions, to
enhance recommendations. Despite their overall performance gains, their
effectiveness varies considerably between visited items (i.e., those a user has
interacted with through auxiliary behaviors) and unvisited items (i.e., those
with which the user has had no such interactions). Specifically, our analysis
reveals that (1) existing multi-behavior recommender systems exhibit a
significant gap in recommendation quality between the two item types (visited
and unvisited items) and (2) achieving strong performance on both types with a
single model architecture remains challenging. To tackle these issues, we
propose a novel multi-behavior recommender system, MEMBER. It employs a
mixture-of-experts framework, with experts designed to recommend the two item
types, respectively. Each expert is trained using a self-supervised method
specialized for its design goal. In our comprehensive experiments, we show the
effectiveness of MEMBER across both item types, achieving up to 65.46\%
performance gain over the best competitor in terms of Hit Ratio@20.

### 6. [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](http://arxiv.org/pdf/2508.19591v1)

Authors: Jiakui Shen, Yunqi Mi, Guoshuai Zhao, Jialie Shen, Xueming Qian

Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

### 7. [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](http://arxiv.org/pdf/2508.19758v1)

Authors: Yixuan Tang, Yuanyuan Shi, Yiqun Sun, Anthony Kum Hoe Tung

Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

### 8. [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](http://arxiv.org/pdf/2508.19997v1)

Authors: Boheng Mao

Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

### 9. [A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions](http://arxiv.org/pdf/2508.19620v1)

Authors: Yunqi Mi, Jiakui Shen, Guoshuai Zhao, Jialie Shen, Xueming Qian

Extending recommender systems to federated learning (FL) frameworks to
protect the privacy of users or platforms while making recommendations has
recently gained widespread attention in academia. This is due to the natural
coupling of recommender systems and federated learning architectures: the data
originates from distributed clients (mostly mobile devices held by users),
which are highly related to privacy. In a centralized recommender system
(CenRec), the central server collects clients' data, trains the model, and
provides the service. Whereas in federated recommender systems (FedRec), the
step of data collecting is omitted, and the step of model training is offloaded
to each client. The server only aggregates the model and other knowledge, thus
avoiding client privacy leakage. Some surveys of federated recommender systems
discuss and analyze related work from the perspective of designing FL systems.
However, their utility drops by ignoring specific recommendation scenarios'
unique characteristics and practical challenges. For example, the statistical
heterogeneity issue in cross-domain FedRec originates from the label drift of
the data held by different platforms, which is mainly caused by the recommender
itself, but not the federated architecture. Therefore, it should focus more on
solving specific problems in real-world recommendation scenarios to encourage
the deployment FedRec. To this end, this review comprehensively analyzes the
coupling of recommender systems and federated learning from the perspective of
recommendation researchers and practitioners. We establish a clear link between
recommendation scenarios and FL frameworks, systematically analyzing
scenario-specific approaches, practical challenges, and potential
opportunities. We aim to develop guidance for the real-world deployment of
FedRec, bridging the gap between existing research and applications.

### 10. [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](http://arxiv.org/pdf/2508.20013v1)

Authors: Lotte Gross, Rebecca Walter, Nicole Zoppi, Adrien Justus, Alessandro Gambetti, Qiwei Han, Maximilian Kaiser

This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

### Machine Learning

### 1. [Distribution Shift Aware Neural Tabular Learning](http://arxiv.org/pdf/2508.19486v1)

Authors: Wangyang Ying, Nanxu Gong, Dongjie Wang, Xinyuan Wang, Arun Vignesh Malarkkan, Vivek Gupta, Chandan K. Reddy, Yanjie Fu

Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

### 2. [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](http://arxiv.org/pdf/2508.19554v1)

Authors: Haruki Yonekura, Ren Ozeki, Tatsuya Amano, Hamada Rizk, Hirozumi Yamaguchi

Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

### 3. [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](http://arxiv.org/pdf/2508.19567v1)

Authors: Sheryl Mathew, N Harshit

In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

### 4. [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](http://arxiv.org/pdf/2508.19571v1)

Authors: Yunlong Lin, Chao Lu, Tongshuai Wu, Xiaocong Zhao, Guodong Du, Yanwei Sun, Zirui Li, Jianwei Gong

Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

### 5. [Delta-Audit: Explaining What Changes When Models Change](http://arxiv.org/pdf/2508.19589v1)

Authors: Arshia Hemmat, Afsaneh Fatemi

Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

### 6. [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](http://arxiv.org/pdf/2508.19598v1)

Authors: Zhiwei Li, Yong Hu, Wenqing Wang

The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

### 7. [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](http://arxiv.org/pdf/2508.19613v1)

Authors: Chenzhi Liu, Mahsa Baktashmotlagh, Yanran Tang, Zi Huang, Ruihong Qiu

Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

### 8. [SCAR: A Characterization Scheme for Multi-Modal Dataset](http://arxiv.org/pdf/2508.19659v1)

Authors: Ri Su, Zhao Chen, Caleb Chen Cao, Nan Tang, Lei Chen

Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

### 9. [Tune My Adam, Please!](http://arxiv.org/pdf/2508.19733v1)

Authors: Theodoros Athanasiadis, Steven Adriaensen, Samuel Müller, Frank Hutter

The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

### 10. [Fast 3D Diffusion for Scalable Granular Media Synthesis](http://arxiv.org/pdf/2508.19752v1)

Authors: Muhammad Moeeze Hassan, Régis Cottereau, Filippo Gatti, Patryk Dec

Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

### Neural and Evolutionary Computing

### 1. [Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks](http://arxiv.org/pdf/2508.19920v1)

Authors: Matthew Meek, Guy Tallent, Thomas Breimer, James Gaskell, Abhay Kashyap, Atharv Tekurkar, Jonathan Fischman, Luodi Wang, Viet-Dung Nguyen, John Rieffel

Recently, researchers have explored control methods that embrace nonlinear
dynamic coupling instead of suppressing it. Such designs leverage dynamical
coupling for communication between different parts of the robot. Morphological
communication refers to when those dynamics can be used as an emergent data bus
to facilitate coordination among independent controller modules within the same
robot. Previous research with tensegrity-based robot designs has shown that
evolutionary learning models that evolve spiking neural networks (SNN) as robot
control mechanisms are effective for controlling non-rigid robots. Our own
research explores the emergence of morphological communication in an SNN-based
simulated soft robot in theEvoGym environment.

### 2. [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](http://arxiv.org/pdf/2508.19806v1)

Authors: Shenqi Wang, Guangzhi Tang

Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

### 3. [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](http://arxiv.org/pdf/2508.19548v1)

Authors: Madhuvanthi Srivatsav R, Chiranjib Bhattacharyya, Shantanu Chakrabartty, Chetan Singh Thakur

Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

### 4. [Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation](http://arxiv.org/pdf/2508.19660v1)

Authors: Vojtech Mrazek, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Zdenek Vasicek, Mehdi B. Tahoori, Georgios Zervakis

Printed electronics offer a promising alternative for applications beyond
silicon-based systems, requiring properties like flexibility, stretchability,
conformality, and ultra-low fabrication costs. Despite the large feature sizes
in printed electronics, printed neural networks have attracted attention for
meeting target application requirements, though realizing complex circuits
remains challenging. This work bridges the gap between classification accuracy
and area efficiency in printed neural networks, covering the entire
processing-near-sensor system design and co-optimization from the
analog-to-digital interface-a major area and power bottleneck-to the digital
classifier. We propose an automated framework for designing printed Ternary
Neural Networks with arbitrary input precision, utilizing multi-objective
optimization and holistic approximation. Our circuits outperform existing
approximate printed neural networks by 17x in area and 59x in power on average,
being the first to enable printed-battery-powered operation with under 5%
accuracy loss while accounting for analog-to-digital interfacing costs.

### Networking and Internet Architecture

### 1. [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](http://arxiv.org/pdf/2508.19736v1)

Authors: Mohsen Ahadi, Adeel Malik, Omid Esrafilian, Florian Kaltenberger, Cedric Thienot

5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

### 2. [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](http://arxiv.org/pdf/2508.19870v1)

Authors: Yinqiu Liu, Ruichen Zhang, Haoxiang Luo, Yijing Lin, Geng Sun, Dusit Niyato, Hongyang Du, Zehui Xiong, Yonggang Wen, Abbas Jamalipour, Dong In Kim, Ping Zhang

Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

### 3. [2SYN: Congestion-Aware Multihoming](http://arxiv.org/pdf/2508.20044v1)

Authors: Kfir Toledo, Isaac Keslassy

When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

### 4. [A First Look at Inter-Cell Interference in the Wild](http://arxiv.org/pdf/2508.20060v1)

Authors: Daqian Ding, Yibo Pi, Cailian Chen

In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

### 5. [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](http://arxiv.org/pdf/2508.20077v1)

Authors: Tao Xiuyuan, Milena Radenkovic

In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

### 6. [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](http://arxiv.org/pdf/2508.19548v1)

Authors: Madhuvanthi Srivatsav R, Chiranjib Bhattacharyya, Shantanu Chakrabartty, Chetan Singh Thakur

Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

### Robotics

### 1. [Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks](http://arxiv.org/pdf/2508.19607v1)

Authors: Amin Berjaoui Tahmaz, Ravi Prakash, Jens Kober

This paper presents an Impedance Primitive-augmented hierarchical
reinforcement learning framework for efficient robotic manipulation in
sequential contact tasks. We leverage this hierarchical structure to
sequentially execute behavior primitives with variable stiffness control
capabilities for contact tasks. Our proposed approach relies on three key
components: an action space enabling variable stiffness control, an adaptive
stiffness controller for dynamic stiffness adjustments during primitive
execution, and affordance coupling for efficient exploration while encouraging
compliance. Through comprehensive training and evaluation, our framework learns
efficient stiffness control capabilities and demonstrates improvements in
learning efficiency, compositionality in primitive selection, and success rates
compared to the state-of-the-art. The training environments include block
lifting, door opening, object pushing, and surface cleaning. Real world
evaluations further confirm the framework's sim2real capability. This work lays
the foundation for more adaptive and versatile robotic manipulation systems,
with potential applications in more complex contact-based tasks.

### 2. [Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning](http://arxiv.org/pdf/2508.19608v1)

Authors: Dongjae Lee, Byeongjun Kim, H. Jin Kim

Aerial manipulators based on conventional multirotors can conduct
manipulation only in small roll and pitch angles due to the underactuatedness
of the multirotor base. If the multirotor base is capable of hovering at
arbitrary orientation, the robot can freely locate itself at any point in
$\mathsf{SE}(3)$, significantly extending its manipulation workspace and
enabling a manipulation task that was originally not viable. In this work, we
present a geometric robust control and whole-body motion planning framework for
an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,
we first propose a geometric robust controller for a floating base. Since the
motion of the robotic arm and the interaction forces during manipulation affect
the stability of the floating base, the base should be capable of mitigating
these adverse effects while controlling its 6D pose. We then design a two-step
optimization-based whole-body motion planner, jointly considering the pose of
the floating base and the joint angles of the robotic arm to harness the entire
configuration space. The devised two-step approach facilitates real-time
applicability and enhances convergence of the optimization problem with
non-convex and non-Euclidean search space. The proposed approach enables the
base to be stationary at any 6D pose while autonomously carrying out
sophisticated manipulation near obstacles without any collision. We demonstrate
the effectiveness of the proposed framework through experiments in which an OAM
performs grasping and pulling of an object in multiple scenarios, including
near $90^\circ$ and even $180^\circ$ pitch angles.

### 3. [Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments](http://arxiv.org/pdf/2508.19731v1)

Authors: Maryam Kazemi Eskeri, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner

Multi-robot systems are increasingly deployed in applications, such as
intralogistics or autonomous delivery, where multiple robots collaborate to
complete tasks efficiently. One of the key factors enabling their efficient
cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this
problem optimize task distribution among robots to minimize the overall
execution time. In shared environments, apart from the relative distance
between the robots and the tasks, the execution time is also significantly
impacted by the delay caused by navigating around moving people. However, most
existing MRTA approaches are dynamics-agnostic, relying on static maps and
neglecting human motion patterns, leading to inefficiencies and delays. In this
paper, we introduce \acrfull{method name}. This method leverages Maps of
Dynamics (MoDs), spatio-temporal queryable models designed to capture
historical human movement patterns, to estimate the impact of humans on the
task execution time during deployment. \acrshort{method name} utilizes a
stochastic cost function that includes MoDs. Experimental results show that
integrating MoDs enhances task allocation performance, resulting in reduced
mission completion times by up to $26\%$ compared to the dynamics-agnostic
method and up to $19\%$ compared to the baseline. This work underscores the
importance of considering human dynamics in MRTA within shared environments and
presents an efficient framework for deploying multi-robot systems in
environments populated by humans.

### 4. [Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles](http://arxiv.org/pdf/2508.19771v1)

Authors: Liding Zhang, Zhenshan Bing, Yu Zhang, Kuanqi Cai, Lingyun Chen, Fan Wu, Sami Haddadin, Alois Knoll

Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.

### 5. [Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization](http://arxiv.org/pdf/2508.19776v1)

Authors: Liding Zhang, Yao Ling, Zhenshan Bing, Fan Wu, Sami Haddadin, Alois Knoll

Bidirectional motion planning often reduces planning time compared to its
unidirectional counterparts. It requires connecting the forward and reverse
search trees to form a continuous path. However, this process could fail and
restart the asymmetric bidirectional search due to the limitations of
lazy-reverse search. To address this challenge, we propose Greedy GuILD
Grafting Trees (G3T*), a novel path planner that grafts invalid edge
connections at both ends to re-establish tree-based connectivity, enabling
rapid path convergence. G3T* employs a greedy approach using the minimum
Lebesgue measure of guided incremental local densification (GuILD) subsets to
optimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling
distribution between the informed set and GuILD subsets based on historical and
current cost improvements, ensuring asymptotic optimality. These features
enhance the forward search's growth towards the reverse tree, achieving faster
convergence and lower solution costs. Benchmark experiments across dimensions
from R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior
performance compared to existing single-query sampling-based planners. A video
showcasing our experimental results is available at:
https://youtu.be/3mfCRL5SQIU

### 6. [APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors](http://arxiv.org/pdf/2508.19790v1)

Authors: Liding Zhang, Sicheng Wang, Kuanqi Cai, Zhenshan Bing, Fan Wu, Chaoqun Wang, Sami Haddadin, Alois Knoll

Optimal path planning aims to determine a sequence of states from a start to
a goal while accounting for planning objectives. Popular methods often
integrate fixed batch sizes and neglect information on obstacles, which is not
problem-specific. This study introduces Adaptively Prolated Trees (APT*), a
novel sampling-based motion planner that extends based on Force Direction
Informed Trees (FDIT*), integrating adaptive batch-sizing and elliptical
$r$-nearest neighbor modules to dynamically modulate the path searching process
based on environmental feedback. APT* adjusts batch sizes based on the
hypervolume of the informed sets and considers vertices as electric charges
that obey Coulomb's law to define virtual forces via neighbor samples, thereby
refining the prolate nearest neighbor selection. These modules employ
non-linear prolate methods to adaptively adjust the electric charges of
vertices for force definition, thereby improving the convergence rate with
lower solution costs. Comparative analyses show that APT* outperforms existing
single-query sampling-based planners in dimensions from $\mathbb{R}^4$ to
$\mathbb{R}^{16}$, and it was further validated through a real-world robot
manipulation task. A video showcasing our experimental results is available at:
https://youtu.be/gCcUr8LiEw4

### 7. [A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living](http://arxiv.org/pdf/2508.19816v1)

Authors: Ricardo J. Manríquez-Cisterna, Ankit A. Ravankar, Jose V. Salazar Luces, Takuro Hatsukari, Yasuhisa Hirata

This paper presents a standing support mobility robot "Moby" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.

### 8. [FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control](http://arxiv.org/pdf/2508.19926v1)

Authors: Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu

Unified physics-based humanoid controllers are pivotal for robotics and
character animation, yet models that excel on gentle, everyday motions still
stumble on explosive actions, hampering real-world deployment. We bridge this
gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),
an end-to-end framework composed of frame-accelerated augmentation, a robust
base controller, and a residual mixture-of-experts (MoE). Frame-accelerated
augmentation exposes the model to high-velocity pose changes by widening
inter-frame gaps. The base controller reliably tracks everyday low-dynamic
motions, while the residual MoE adaptively allocates additional network
capacity to handle challenging high-dynamic actions, significantly enhancing
tracking accuracy. In the absence of a public benchmark, we curate the
High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically
plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and
lowers global mean per-joint position error by 14.6\% relative to the baseline,
while preserving near-perfect accuracy on low-dynamic motions. These results
establish FARM as a new baseline for high-dynamic humanoid control and
introduce the first open benchmark dedicated to this challenge. The code and
dataset will be released at https://github.com/Colin-Jing/FARM.

### 9. [Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors](http://arxiv.org/pdf/2508.19953v1)

Authors: Rafael Cathomen, Mayank Mittal, Marin Vlastelica, Marco Hutter

Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.

### 10. [Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](http://arxiv.org/pdf/2508.19958v1)

Authors: Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang

Vision-Language-Action (VLA) models have become a cornerstone in robotic
policy learning, leveraging large-scale multimodal data for robust and scalable
control. However, existing VLA frameworks primarily address short-horizon
tasks, and their effectiveness on long-horizon, multi-step robotic manipulation
remains limited due to challenges in skill chaining and subtask dependencies.
In this work, we introduce Long-VLA, the first end-to-end VLA model
specifically designed for long-horizon robotic tasks. Our approach features a
novel phase-aware input masking strategy that adaptively segments each subtask
into moving and interaction phases, enabling the model to focus on
phase-relevant sensory cues and enhancing subtask compatibility. This unified
strategy preserves the scalability and data efficiency of VLA training, and our
architecture-agnostic module can be seamlessly integrated into existing VLA
models. We further propose the L-CALVIN benchmark to systematically evaluate
long-horizon manipulation. Extensive experiments on both simulated and
real-world tasks demonstrate that Long-VLA significantly outperforms prior
state-of-the-art methods, establishing a new baseline for long-horizon robotic
control.

### Software Engineering

### 1. [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](http://arxiv.org/pdf/2508.19610v1)

Authors: Kathrin Figl, Maria Kirchner, Sebastian Baltes, Michael Felderer

Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

### 2. [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](http://arxiv.org/pdf/2508.19663v1)

Authors: Lola Solovyeva, Eduardo Carneiro Oliveira, Shiyu Fan, Alper Tuncay, Shamil Gareev, Andrea Capiluppi

The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

### 3. [Enabling Content Management Systems as an Information Source in Model-driven Projects](http://arxiv.org/pdf/2508.19797v1)

Authors: Joan Giner-Miguelez, Abel Gómez, Jordi Cabot

Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

### 4. [On the Future of Software Reuse in the Era of AI Native Software Engineering](http://arxiv.org/pdf/2508.19834v1)

Authors: Antero Taivalsaari, Tommi Mikkonen, Cesare Pautasso

Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

### 5. [Towards a fundamental theory of modeling discrete systems](http://arxiv.org/pdf/2508.19803v1)

Authors: Peter Fettke, Wolfgang Reisig

Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

### 6. [Generative AI for Testing of Autonomous Driving Systems: A Survey](http://arxiv.org/pdf/2508.19882v1)

Authors: Qunying Song, He Ye, Mark Harman, Federica Sarro

Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

### 7. [Smart Contract Intent Detection with Pre-trained Programming Language Model](http://arxiv.org/pdf/2508.20086v1)

Authors: Youwei Huang, Jianwen Li, Sen Fang, Yao Li, Peng Yang, Bin Hu, Tao Zhang

Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

### 8. [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](http://arxiv.org/pdf/2508.19558v1)

Authors: Zhuohao Li, Wenqing Chen, Jianxing Yu, Zhichao Lu

Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

### Social and Information Networks

### 1. [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](http://arxiv.org/pdf/2508.19737v1)

Authors: Meng Qin, Weihua Li, Jinqiang Cui, Sen Pei

Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

### 2. [The Economic Complexity of the Roman Empire](http://arxiv.org/pdf/2508.19892v1)

Authors: Matteo Mazzamurro, Petra Hermankova, Michele Coscia, Tom Brughmans

Economic complexity is a powerful tool to estimate the productive
capabilities and future growth of modern economies. Little is known of how
economic complexity evolves over long periods in history. In this paper, we use
archaeological evidence from the Roman Empire in the form of short texts
preserved on a durable material (i.e. inscriptions) to estimate the economic
complexity of the various provinces of the empire. By connecting the
occupations listed in the text of inscriptions with the location in which the
inscribed objects were found we can estimate that the most complex areas during
the first four centuries of the Roman Empire have a remarkable and
statistically significant overlap with the most complex countries today. While
we lack an explanation for the reason of the preservation of economic
complexity through the ages, this evidence provides a suggestion about how
difficult the development of economic capabilities might be.

### 3. [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](http://arxiv.org/pdf/2508.19907v1)

Authors: Hewen Wang, Renchi Yang, Xiaokui Xiao

Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

### Systems and Control

### 1. [Hybrid ML-RL Approach for Smart Grid Stability Prediction and Optimized Control Strategy](http://arxiv.org/pdf/2508.19541v1)

Authors: Kazi Sifatul Islam, Anandi Dutta, Shivani Mruthyunjaya

Electrical grids are now much more complex due to the rapid integration of
distributed generation and alternative energy sources, which makes forecasting
grid stability with optimized control a crucial task for operators. Traditional
statistical, physics-based, and ML models can learn the pattern of the grid
features, but have limitations in optimal strategy control with instability
prediction. This work proposes a hybrid ML-RL framework that leverages ML for
rapid stability prediction and RL for dynamic control and optimization. The
first stage of this study created a baseline that explored the potential of
various ML models for stability prediction. Out of them, the stacking
classifiers of several fundamental models show a significant performance in
classifying the instability, leading to the second stage, where reinforcement
learning algorithms (PPO, A2C, and DQN) optimize power control actions.
Experimental results demonstrate that the hybrid ML-RL model effectively
stabilizes the grid, achieves rapid convergence, and significantly reduces
training time. The integration of ML-based stability classification with
RL-based dynamic control enhances decision-making efficiency while lowering
computational complexity, making it well-suited for real-time smart grid
applications.

### 2. [Symbolic Equation Modeling of Composite Loads: A Kolmogorov-Arnold Network based Learning Approach](http://arxiv.org/pdf/2508.19612v1)

Authors: Sonam Dorji, Yongkang Sun, Yuchen Zhang, Ghavameddin Nourbakhsh, Yateendra Mishra, Yan Xu

With increasing penetration of distributed energy resources installed behind
the meter, there is a growing need for adequate modelling of composite loads to
enable accurate power system simulation analysis. Existing measurement based
load modeling methods either fit fixed-structure physical models, which limits
adaptability to evolving load mixes, or employ flexible machine learning
methods which are however black boxes and offer limited interpretability. This
paper presents a new learning based load modelling method based on Kolmogorov
Arnold Networks towards modelling flexibility and interpretability. By actively
learning activation functions on edges, KANs automatically derive free form
symbolic equations that capture nonlinear relationships among measured
variables without prior assumptions about load structure. Case studies
demonstrate that the proposed approach outperforms other methods in both
accuracy and generalization ability, while uniquely representing composite
loads into transparent, interpretable mathematical equations.

### 3. [Low-Cost Architecture and Efficient Pattern Synthesis for Polarimetric Phased Array Based on Polarization Coding Reconfigurable Elements](http://arxiv.org/pdf/2508.19644v1)

Authors: Yiqing Wang, Jian Zhou, Chen Pang, Wenyang Man, Zixiang Xiong, Ke Meng, Zhanling Wang, Yongzhen Li

Polarimetric phased arrays (PPAs) enhance radar target detection and
anti-jamming capabilities. However, the dual transmit/receive (T/R) channel
requirement leads to high costs and system complexity. To address this, this
paper introduces a polarization-coding reconfigurable phased array (PCRPA) and
associated pattern synthesis techniques to reduce PPA costs while minimizing
performance degradation. Each PCRPA element connects to a single T/R channel
and incorporates two-level RF switches for real-time control of polarization
states and waveforms. By adjusting element codes and excitation weights, the
PCRPA can generate arbitrarily polarized and dual-polarized beams. Efficient
beam pattern synthesis methods are also proposed, featuring novel optimization
constraints derived from theoretical and analytical analysis of PCRPAs.
Simulations demonstrate that the approach achieves low cross-polarization and
sidelobe levels comparable to conventional architectures within the scan range,
particularly for large arrays. However, the channel reduction inevitably incurs
power and directivity loss. Experiments conducted on an $8\times 8$ X-band
array antenna validate the effectiveness of the proposed system. The PCRPA and
synthesis methods are well-suited for large-scale PPA systems, offering
significant cost-effectiveness while maintaining good sidelobe suppression and
polarization control performance.

### 4. [Distributed Safety-Critical MPC for Multi-Agent Formation Control and Obstacle Avoidance](http://arxiv.org/pdf/2508.19678v1)

Authors: Chao Wang, Shuyuan Zhang, Lei Wang

For nonlinear multi-agent systems with high relative degrees, achieving
formation control and obstacle avoidance in a distributed manner remains a
significant challenge. To address this issue, we propose a novel distributed
safety-critical model predictive control (DSMPC) algorithm that incorporates
discrete-time high-order control barrier functions (DHCBFs) to enforce safety
constraints, alongside discrete-time control Lyapunov functions (DCLFs) to
establish terminal constraints. To facilitate distributed implementation, we
develop estimated neighbor states for formulating DHCBFs and DCLFs, while also
devising a bound constraint to limit estimation errors and ensure convergence.
Additionally, we provide theoretical guarantees regarding the feasibility and
stability of the proposed DSMPC algorithm based on a mild assumption. The
effectiveness of the proposed method is evidenced by the simulation results,
demonstrating improved performance and reduced computation time compared to
existing approaches.

### 5. [Uncertainty-Based Perturb and Observe for Fast Optimization of Unknown, Time-Varying Processes](http://arxiv.org/pdf/2508.19756v1)

Authors: Leontine Aarnoudse, Mark Haring, Nathan van de Wouw, Alexey Pavlov

Model-free adaptive optimization methods are capable of optimizing unknown,
time-varying processes even when other optimization methods are not. However,
their practical application is often limited by perturbations that are used to
gather information on the unknown cost and its gradient. The aim of this paper
is to develop a perturb-and-observe (P&O) method that reduces the need for such
perturbations while still achieving fast and accurate tracking of time-varying
optima. To this end, a (time-varying) model of the cost is constructed in an
online fashion, taking into account the uncertainty on the measured performance
cost as well as the decreasing reliability of older measurements. Perturbations
are only used when this is expected to lead to improved performance over a
certain time horizon. Convergence conditions are provided under which the
strategy converges to a neighborhood of the optimum. Finally, simulation
results demonstrate that uncertainty-based P\&O can reduce the number of
perturbations significantly while still tracking a time-varying optimum
accurately.

### 6. [Limited Preemption of the 3-Phase Task Model using Preemption Thresholds](http://arxiv.org/pdf/2508.19760v1)

Authors: Thilanka Thilakasiri, Matthias Becker

Phased execution models are a well-known solution to tackle the
unpredictability of today's complex COTS multi-core platforms. The semantics of
these models dedicate phases for a task's execution and shared memory accesses.
Memory phases are solely dedicated to load all necessary instructions and data
to private local memory, and to write back the results of the computation.
During execution phases, only the private local memory is accessed. While
non-preemptive execution phases utilize the local memory well, schedulability
is reduced due to blocking. On the other hand, fully preemptive execution
phases allow for better schedulability, but require local memory to be large
enough to hold all tasks involved in preemption simultaneously. Limited
preemption is a promising approach that provides moderation between
non-preemptive and fully preemptive scheduling.
  In this paper, we propose using preemption thresholds to limit the number of
preemptions to minimize local memory usage while maintaining schedulability. We
propose a worst-case response time and a worst-case memory requirement analysis
for sporadic 3-phase tasks under partitioned fixed-priority scheduling with
preemption thresholds. We further show how the state-of-the-art algorithm to
assign preemption thresholds can be applied to the considered task model.
Evaluations demonstrate that preemption thresholds can significantly reduce the
memory usage (by $2.5\times$) compared to fully preemptive scheduling, while
maintaining high schedulability ratios ($13\times$) compared to non-preemptive
scheduling.

### 7. [Combined Stochastic and Robust Optimization for Electric Autonomous Mobility-on-Demand with Nested Benders Decomposition](http://arxiv.org/pdf/2508.19933v1)

Authors: Sten Elling Tingstad Jacobsen, Balázs Kulcsár, Anders Lindman

The electrification and automation of mobility are reshaping how cities
operate on-demand transport systems. Managing Electric Autonomous
Mobility-on-Demand (EAMoD) fleets effectively requires coordinating dispatch,
rebalancing, and charging decisions under multiple uncertainties, including
travel demand, travel time, energy consumption, and charger availability. We
address this challenge with a combined stochastic and robust model predictive
control (MPC) framework. The framework integrates spatio-temporal Bayesian
neural network forecasts with a multi-stage stochastic optimization model,
formulated as a large-scale mixed-integer linear program. To ensure real-time
applicability, we develop a tailored Nested Benders Decomposition that exploits
the scenario tree structure and enables efficient parallelized solution.
Stochastic optimization is employed to anticipate demand and infrastructure
variability, while robust constraints on energy consumption and travel times
safeguard feasibility under worst-case realizations. We evaluate the framework
using high-fidelity simulations of San Francisco and Chicago. Compared with
deterministic, reactive, and robust baselines, the combined stochastic and
robust approach reduces median passenger waiting times by up to 36% and
95th-percentile delays by nearly 20%, while also lowering rebalancing distance
by 27% and electricity costs by more than 35%. We also conduct a sensitivity
analysis of battery size and vehicle efficiency, finding that energy-efficient
vehicles maintain stable performance even with small batteries, whereas less
efficient vehicles require larger batteries and greater infrastructure support.
Our results emphasize the importance of jointly optimizing predictive control,
vehicle capabilities, and infrastructure planning to enable scalable,
cost-efficient EAMoD operations.

### 8. [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](http://arxiv.org/pdf/2508.19670v1)

Authors: Diogo Costa, Jose Martins, Sandro Pinto

As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

### 9. [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](http://arxiv.org/pdf/2508.19945v1)

Authors: Zhouyu Zhang, Chih-Yuan Chiu, Glen Chou

We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

### 10. [The Coherent Multiplex: Scalable Real-Time Wavelet Coherence Architecture](http://arxiv.org/pdf/2508.19994v1)

Authors: Noah Shore

The Coherent Multiplex is formalized and validated as a scalable, real-time
system for identifying, analyzing, and visualizing coherence among multiple
time series. Its architecture comprises a fast spectral similarity layer based
on cosine similarity metrics of Fourier-transformed signals, and a sparse
time-frequency layer for wavelet coherence. The system constructs and evolves a
multilayer graph representing inter-signal relationships, enabling low-latency
inference and monitoring. A simulation prototype demonstrates functionality
across 8 synthetic channels with a high similarity threshold for further
computation, with additional opportunities for scaling the architecture up to
support thousands of input signals with constrained hardware. Applications
discussed include neuroscience, finance, and biomedical signal analysis.

### Machine Learning (Statistics Category)

### 1. [Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy](http://arxiv.org/pdf/2508.19750v1)

Authors: Binhui Zhang, Jianwei Ma

Normalizing Flows provide a principled framework for high-dimensional density
estimation and generative modeling by constructing invertible transformations
with tractable Jacobian determinants. We propose Fractal Flow, a novel
normalizing flow architecture that enhances both expressiveness and
interpretability through two key innovations. First, we integrate
Kolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into
normalizing flows to construct a structured, interpretable latent space and
model hierarchical semantic clusters. Second, inspired by Fractal Generative
Models, we introduce a recursive modular design into normalizing flows to
improve transformation interpretability and estimation accuracy. Experiments on
MNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the
Fractal Flow achieves latent clustering, controllable generation, and superior
estimation accuracy.

### 2. [Interestingness First Classifiers](http://arxiv.org/pdf/2508.19780v1)

Authors: Ryoma Sato

Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

### 3. [Eigenvalue distribution of the Neural Tangent Kernel in the quadratic scaling](http://arxiv.org/pdf/2508.20036v1)

Authors: Lucas Benigni, Elliot Paquette

We compute the asymptotic eigenvalue distribution of the neural tangent
kernel of a two-layer neural network under a specific scaling of dimension.
Namely, if $X\in\mathbb{R}^{n\times d}$ is an i.i.d random matrix,
$W\in\mathbb{R}^{d\times p}$ is an i.i.d $\mathcal{N}(0,1)$ matrix and
$D\in\mathbb{R}^{p\times p}$ is a diagonal matrix with i.i.d bounded entries,
we consider the matrix
  \[
  \mathrm{NTK}
  =
  \frac{1}{d}XX^\top
  \odot
  \frac{1}{p}
  \sigma'\left(
  \frac{1}{\sqrt{d}}XW
  \right)D^2
  \sigma'\left(
  \frac{1}{\sqrt{d}}XW
  \right)^\top
  \]
  where $\sigma'$ is a pseudo-Lipschitz function applied entrywise and under
the scaling $\frac{n}{dp}\to \gamma_1$ and $\frac{p}{d}\to \gamma_2$. We
describe the asymptotic distribution as the free multiplicative convolution of
the Marchenko--Pastur distribution with a deterministic distribution depending
on $\sigma$ and $D$.

### 4. [Neural Conditional Simulation for Complex Spatial Processes](http://arxiv.org/pdf/2508.20067v1)

Authors: Julia Walchessen, Andrew Zammit-Mangion, Raphaël Huser, Mikael Kuusela

A key objective in spatial statistics is to simulate from the distribution of
a spatial process at a selection of unobserved locations conditional on
observations (i.e., a predictive distribution) to enable spatial prediction and
uncertainty quantification. However, exact conditional simulation from this
predictive distribution is intractable or inefficient for many spatial process
models. In this paper, we propose neural conditional simulation (NCS), a
general method for spatial conditional simulation that is based on neural
diffusion models. Specifically, using spatial masks, we implement a conditional
score-based diffusion model that evolves Gaussian noise into samples from a
predictive distribution when given a partially observed spatial field and
spatial process parameters as inputs. The diffusion model relies on a neural
network that only requires unconditional samples from the spatial process for
training. Once trained, the diffusion model is amortized with respect to the
observations in the partially observed field, the number and locations of those
observations, and the spatial process parameters, and can therefore be used to
conditionally simulate from a broad class of predictive distributions without
retraining the neural network. We assess the NCS-generated simulations against
simulations from the true conditional distribution of a Gaussian process model,
and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick
process model for spatial extremes. In the latter case, we show that it is more
efficient and accurate to conditionally simulate using NCS than classical MCMC
techniques implemented in standard software. We conclude that NCS enables
efficient and accurate conditional simulation from spatial predictive
distributions that are challenging to sample from using traditional methods.

### 5. [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](http://arxiv.org/pdf/2508.19563v1)

Authors: Hejia Liu, Mochen Yang, Gediminas Adomavicius

Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

### 6. [Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers](http://arxiv.org/pdf/2508.19841v1)

Authors: Fahime Seyedheydari, Kevin Conley, Simo Särkkä

We present a probabilistic, data-driven surrogate model for predicting the
radiative properties of nanoparticle embedded scattering media. The model uses
conditional normalizing flows, which learn the conditional distribution of
optical outputs, including reflectance, absorbance, and transmittance, given
input parameters such as the absorption coefficient, scattering coefficient,
anisotropy factor, and particle size distribution. We generate training data
using Monte Carlo radiative transfer simulations, with optical properties
derived from Mie theory. Unlike conventional neural networks, the conditional
normalizing flow model yields full posterior predictive distributions, enabling
both accurate forecasts and principled uncertainty quantification. Our results
demonstrate that this model achieves high predictive accuracy and reliable
uncertainty estimates, establishing it as a powerful and efficient surrogate
for radiative transfer simulations.

### 7. [The Information Dynamics of Generative Diffusion](http://arxiv.org/pdf/2508.19897v1)

Authors: Luca Ambrogioni

Generative diffusion models have emerged as a powerful class of models in
machine learning, yet a unified theoretical understanding of their operation is
still developing. This perspective paper provides an integrated perspective on
generative diffusion by connecting their dynamic, information-theoretic, and
thermodynamic properties under a unified mathematical framework. We demonstrate
that the rate of conditional entropy production during generation (i.e. the
generative bandwidth) is directly governed by the expected divergence of the
score function's vector field. This divergence, in turn, is linked to the
branching of trajectories and generative bifurcations, which we characterize as
symmetry-breaking phase transitions in the energy landscape. This synthesis
offers a powerful insight: the process of generation is fundamentally driven by
the controlled, noise-induced breaking of (approximate) symmetries, where peaks
in information transfer correspond to critical transitions between possible
outcomes. The score function acts as a dynamic non-linear filter that regulates
the bandwidth of the noise by suppressing fluctuations that are incompatible
with the data.

### 8. [The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology](http://arxiv.org/pdf/2508.19914v1)

Authors: Muhammad Waqas, Rukhmini Bandyopadhyay, Eman Showkatian, Amgad Muneer, Anas Zafar, Frank Rojas Alvarez, Maricel Corredor Marin, Wentao Li, David Jaffray, Cara Haymaker, John Heymach, Natalie I Vokes, Luisa Maren Solis Soto, Jianjun Zhang, Jia Wu

Foundation models have recently emerged as powerful feature extractors in
computational pathology, yet they typically omit mechanisms for leveraging the
global spatial structure of tissues and the local contextual relationships
among diagnostically relevant regions - key elements for understanding the
tumor microenvironment. Multiple instance learning (MIL) remains an essential
next step following foundation model, designing a framework to aggregate
patch-level features into slide-level predictions. We present EAGLE-Net, a
structure-preserving, attention-guided MIL architecture designed to augment
prediction and interpretability. EAGLE-Net integrates multi-scale absolute
spatial encoding to capture global tissue architecture, a top-K
neighborhood-aware loss to focus attention on local microenvironments, and
background suppression loss to minimize false positives. We benchmarked
EAGLE-Net on large pan-cancer datasets, including three cancer types for
classification (10,260 slides) and seven cancer types for survival prediction
(4,172 slides), using three distinct histology foundation backbones (REMEDIES,
Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher
classification accuracy and the top concordance indices in 6 of 7 cancer types,
producing smooth, biologically coherent attention maps that aligned with expert
annotations and highlighted invasive fronts, necrosis, and immune infiltration.
These results position EAGLE-Net as a generalizable, interpretable framework
that complements foundation models, enabling improved biomarker discovery,
prognostic modeling, and clinical decision support



---

# Nature Computer Science Reports

Collection of today's Computer Science research papers pulled from Nature Open Access Reports.

---

Pulled on 2025-08-28 PST.

### 1. [Fusion of automatically learned rhythm and morphology features matches diagnostic criteria and enhances AI explainability](https://www.nature.com/articles/s44387-025-00022-w)

Authors: Alexander Hammer et al.

### 2. [Aggregate based light incremental sharding for efficient embedding table management for recommender systems](https://www.nature.com/articles/s41598-025-15351-8)

Authors: Chao Kong et al.

### 3. [A simple and effective approach for body part recognition on CT scans based on projection estimation](https://www.nature.com/articles/s41598-025-17174-z)

Authors: Franko Hrzic et al.

### 4. [Leveraging blockchain for cybersecurity detection using hybridization of prairie dog optimization with differential evolution on internet of things environment](https://www.nature.com/articles/s41598-025-10410-6)

Authors: Fahad F. Alruwaili

### 5. [Integrating physiological signals for enhanced sleep apnea diagnosis with SleepNet](https://www.nature.com/articles/s41598-025-16154-7)

Authors: Prashant Hemrajani et al.

### 6. [Hybrid quantum-classical-quantum convolutional neural networks](https://www.nature.com/articles/s41598-025-13417-1)

Authors: Changzhou Long et al.

### 7. [AI-Driven Tai Chi mastery using deep learning framework for movement assessment and personalized training](https://www.nature.com/articles/s41598-025-17187-8)

Authors: Xun Zhao

### 8. [Collaborative filtering models an experimental and detailed comparative study](https://www.nature.com/articles/s41598-025-15096-4)

Authors: Devangam Bangaru Rajesh et al.

### 9. [Enhanced PCB defect detection via HSA-RTDETR on RT-DETR](https://www.nature.com/articles/s41598-025-11394-z)

Authors: Yesong Wang et al.

### 10. [A cost effective real time rail track monitoring system leveraging multi sensor fusion and multi objective optimization](https://www.nature.com/articles/s41598-025-99404-y)

Authors: Saiful Islam Salim et al.

### 11. [Generalizing machine learning models from clinical free text](https://www.nature.com/articles/s41598-025-17197-6)

Authors: Balaji Pandian et al.

### 12. [Deep transfer learning and attention based P2.5 forecasting in Delhi using a decade of winter season data](https://www.nature.com/articles/s41598-025-16664-4)

Authors: S. Lakshmi et al.

### 13. [How large language models encode theory-of-mind: a study on sparse parameter patterns](https://www.nature.com/articles/s44387-025-00031-9)

Authors: Yuheng Wu et al.

