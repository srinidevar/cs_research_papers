# Computer Science arXiv Papers

Collection of top 10 Computer Science research papers pulled daily from arXiv.

---

Pulled on 2025-07-03 17:00:25.571207 PST.

### Artificial Intelligence

### 1. [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](http://arxiv.org/pdf/2507.01376v1)

Authors: Yinwang Ren, Yangyang Liu, Tang Ji, Xun Xu

AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

### 2. [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](http://arxiv.org/pdf/2507.01410v1)

Authors: Abeer Dyoub, Francesca A. Lisi

The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

### 3. [Using multi-agent architecture to mitigate the risk of LLM hallucinations](http://arxiv.org/pdf/2507.01446v1)

Authors: Abd Elrahman Amer, Magdi Amer

Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

### 4. [Joint Matching and Pricing for Crowd-shipping with In-store Customers](http://arxiv.org/pdf/2507.01749v1)

Authors: Arash Dehghan, Mucahit Cevik, Merve Bodur, Bissan Ghaddar

This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

### 5. [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](http://arxiv.org/pdf/2507.01833v1)

Authors: Yi-Dong Shen, Thomas Eiter

Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

### 6. [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](http://arxiv.org/pdf/2507.01259v1)

Authors: Michał Matak, Jarosław A. Chudziak

In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

### 7. [LLM-based Realistic Safety-Critical Driving Video Generation](http://arxiv.org/pdf/2507.01264v1)

Authors: Yongjie Fu, Ruijian Zha, Pei Tian, Xuan Di

Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

### 8. [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](http://arxiv.org/pdf/2507.01271v1)

Authors: Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa

In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

### 9. [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](http://arxiv.org/pdf/2507.01274v1)

Authors: Vishakha Lall, Yisi Liu

Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

### 10. [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](http://arxiv.org/pdf/2507.01281v1)

Authors: Juan Chen, Baolong Bi, Wei Zhang, Jingyan Sui, Xiaofei Zhu, Yuanzhuo Wang, Lingrui Mei, Shenghua Liu

Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

### Hardware Architecture

### 1. [SD-Acc: Accelerating Stable Diffusion through Phase-aware Sampling and Hardware Co-Optimizations](http://arxiv.org/pdf/2507.01309v1)

Authors: Zhican Wang, Guanghui He, Hongxiang Fan

The emergence of diffusion models has significantly advanced generative AI,
improving the quality, realism, and creativity of image and video generation.
Among them, Stable Diffusion (StableDiff) stands out as a key model for
text-to-image generation and a foundation for next-generation multi-modal
algorithms. However, its high computational and memory demands hinder inference
speed and energy efficiency. To address these challenges, we identify three
core issues: (1) intensive and often redundant computations, (2) heterogeneous
operations involving convolutions and attention mechanisms, and (3) diverse
weight and activation sizes.
  We present SD-Acc, a novel algorithm and hardware co-optimization framework.
At the algorithm level, we observe that high-level features in certain
denoising phases show significant similarity, enabling approximate computation.
Leveraging this, we propose an adaptive, phase-aware sampling strategy that
reduces compute and memory loads. This framework automatically balances image
quality and complexity based on the StableDiff model and user requirements. At
the hardware level, we design an address-centric dataflow to efficiently handle
heterogeneous operations within a simple systolic array. We address the
bottleneck of nonlinear functions via a two-stage streaming architecture and a
reconfigurable vector processing unit. Additionally, we implement adaptive
dataflow optimizations by combining dynamic reuse and operator fusion tailored
to StableDiff workloads, significantly reducing memory access. Across multiple
StableDiff models, our method achieves up to a 3x reduction in computational
demand without compromising image quality. Combined with our optimized hardware
accelerator, SD-Acc delivers higher speed and energy efficiency than
traditional CPU and GPU implementations.

### 2. [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](http://arxiv.org/pdf/2507.01429v1)

Authors: Benjamin Chen Ming Choong, Tao Luo, Cheng Liu, Bingsheng He, Wei Zhang, Joey Tianyi Zhou

Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

### 3. [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](http://arxiv.org/pdf/2507.01676v1)

Authors: Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli

Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

### Computational Complexity

### 1. [Symport/Antiport P Systems with Membrane Separation Characterize P^(#P)](http://arxiv.org/pdf/2507.01657v1)

Authors: Vivien Ducros, Claudio Zandron

Membrane systems represent a computational model that operates in a
distributed and parallel manner, inspired by the behavior of biological cells.
These systems feature objects that transform within a nested membrane
structure. This research concentrates on a specific type of these systems,
based on cellular symport/antiport communication of chemicals.
  Results in the literature show that systems of this type that also allow cell
division can solve PSPACE problems. In our study, we investigate systems that
use membrane separation instead of cell division, for which only limited
results are available. Notably, it has been shown that any problem solvable by
such systems in polynomial time falls within the complexity class P^(#P).
  By implementing a system solving MIDSAT, a P^(#P)-complete problem, we
demonstrate that the reverse inclusion is true as well, thus providing an exact
characterization of the problem class solvable by P systems with
symport/antiport and membrane separation.
  Moreover, our implementation uses rules of length at most three. With this
limit, systems were known to be able to solve NP-complete problems, whereas
limiting the rules by length two, they characterize P.

### 2. [Hardness of Quantum Distribution Learning and Quantum Cryptography](http://arxiv.org/pdf/2507.01292v1)

Authors: Taiga Hiroka, Min-Hsiu Hsieh, Tomoyuki Morimae

The existence of one-way functions (OWFs) forms the minimal assumption in
classical cryptography. However, this is not necessarily the case in quantum
cryptography. One-way puzzles (OWPuzzs), introduced by Khurana and Tomer,
provide a natural quantum analogue of OWFs. The existence of OWPuzzs implies
$PP\neq BQP$, while the converse remains open. In classical cryptography, the
analogous problem-whether OWFs can be constructed from $P \neq NP$-has long
been studied from the viewpoint of hardness of learning. Hardness of learning
in various frameworks (including PAC learning) has been connected to OWFs or to
$P \neq NP$. In contrast, no such characterization previously existed for
OWPuzzs. In this paper, we establish the first complete characterization of
OWPuzzs based on the hardness of a well-studied learning model: distribution
learning. Specifically, we prove that OWPuzzs exist if and only if proper
quantum distribution learning is hard on average. A natural question that
follows is whether the worst-case hardness of proper quantum distribution
learning can be derived from $PP \neq BQP$. If so, and a worst-case to
average-case hardness reduction is achieved, it would imply OWPuzzs solely from
$PP \neq BQP$. However, we show that this would be extremely difficult: if
worst-case hardness is PP-hard (in a black-box reduction), then $SampBQP \neq
SampBPP$ follows from the infiniteness of the polynomial hierarchy. Despite
that, we show that $PP \neq BQP$ is equivalent to another standard notion of
hardness of learning: agnostic. We prove that $PP \neq BQP$ if and only if
agnostic quantum distribution learning with respect to KL divergence is hard.
As a byproduct, we show that hardness of agnostic quantum distribution learning
with respect to statistical distance against $PPT^{\Sigma_3^P}$ learners
implies $SampBQP \neq SampBPP$.

### Computational Engineering

### 1. [Spatially Distributed Wettability Characterization in Porous Media](http://arxiv.org/pdf/2507.01617v1)

Authors: Faisal Aljaberi, Hadi Belhaj, Sajjad Foroughi, Mohammed Al-Kobaisi, Martin Blunt

An enhanced geometric algorithm for automated pore-by-pore contact angle
measurement from micro-CT images, is presented that achieves superior accuracy
compared to existing methods through robust fluid-fluid and solid-fluid
interface extrapolation. Using this high resolution data, we generate spatially
distributed contact angle maps that reveal previously hidden wettability
heterogeneity. Our analysis of mixed-wet systems demonstrates the severe
limitations of averaged metrics: a sample with a mean contact angle of 64.7
degrees, conventionally classified as uniformly weakly water-wet, exhibits 40%
of its pore space in the intermediate-wetting regime (70-110 degrees). This
heterogeneity explains the presence of minimal surface interfaces and
fundamentally different pore-filling mechanisms operating within the same
sample. By providing open-source tools for spatially-resolved wettability
characterization, this work enables more accurate predictions of multiphase
flow behavior in heterogeneous porous materials, essential for optimizing
subsurface energy storage and recovery processes.

### 2. [A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses](http://arxiv.org/pdf/2507.01706v1)

Authors: Dominik Itner, Dmitrij Dreiling, Hauke Gravenkamp, Bernd Henning, Carolin Birk

In this contribution, we address the estimation of the frequency-dependent
elastic parameters of polymers in the ultrasound range, which is formulated as
an inverse problem. This inverse problem is implemented as a nonlinear
regression-type optimization problem, in which the simulation signals are
fitted to the measurement signals. These signals consist of displacement
responses in waveguides, focusing on hollow cylindrical geometries to enhance
the simulation efficiency. To accelerate the optimization and reduce the number
of model evaluations and wait times, we propose two novel methods. First, we
introduce an adaptation of the Levenberg-Marquardt method derived from a
geometrical interpretation of the least-squares optimization problem. Second,
we introduce an improved objective function based on the autocorrelated
envelopes of the measurement and simulation signals. Given that this study
primarily relies on simulation data to quantify optimization convergence, we
aggregate the expected ranges of realistic material parameters and derive their
distributions to ensure the reproducibility of optimizations with proper
measurements. We demonstrate the effectiveness of our objective function
modification and step adaptation for various materials with isotropic material
symmetry by comparing them with a state-of-the-art optimization method. In all
cases, our method reduces the total number of model evaluations, thereby
shortening the time to identify the material parameters.

### Computational Geometry

### 1. [Multiple Watchman Routes in Staircase Polygons](http://arxiv.org/pdf/2507.01940v1)

Authors: Anna Brötzner, Bengt J. Nilsson, Christiane Schmidt

We consider the watchman route problem for multiple watchmen in staircase
polygons, which are rectilinear $x$- and $y$-monotone polygons. For two
watchmen, we propose an algorithm to find an optimal solution that takes
quadratic time, improving on the cubic time of a trivial solution. For $m \geq
3$ watchmen, we explain where this approach fails, and present an approximation
algorithm for the min-max criterion with only an additive error.

### 2. [A Deterministic Partition Tree and Applications](http://arxiv.org/pdf/2507.01775v1)

Authors: Haitao Wang

In this paper, we present a deterministic variant of Chan's randomized
partition tree [Discret. Comput. Geom., 2012]. This result leads to numerous
applications. In particular, for $d$-dimensional simplex range counting (for
any constant $d \ge 2$), we construct a data structure using $O(n)$ space and
$O(n^{1+\epsilon})$ preprocessing time, such that each query can be answered in
$o(n^{1-1/d})$ time (specifically, $O(n^{1-1/d} / \log^{\Omega(1)} n)$ time),
thereby breaking an $\Omega(n^{1-1/d})$ lower bound known for the semigroup
setting. Notably, our approach does not rely on any bit-packing techniques. We
also obtain deterministic improvements for several other classical problems,
including simplex range stabbing counting and reporting, segment intersection
detection, counting and reporting, ray-shooting among segments, and more.
Similar to Chan's original randomized partition tree, we expect that additional
applications will emerge in the future, especially in situations where
deterministic results are preferred.

### Computation and Language

### 1. [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](http://arxiv.org/pdf/2507.01278v1)

Authors: Cindy Lie Tabuse, David Restepo, Carolina Gracitelli, Fernando Korn Malerbi, Caio Regatieri, Luis Filipe Nakayama

Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

### 2. [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](http://arxiv.org/pdf/2507.01299v1)

Authors: Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu

Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

### 3. [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](http://arxiv.org/pdf/2507.01334v1)

Authors: Nifu Dan, Yujun Cai, Yiwei Wang

Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

### 4. [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](http://arxiv.org/pdf/2507.01437v1)

Authors: Ting Xu, Xiaoxiao Deng, Xiandong Meng, Haifeng Yang, Yan Wu

This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

### 5. [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](http://arxiv.org/pdf/2507.01449v1)

Authors: Tianyu Liu, Qitan Lv, Hao Li, Xing Gao, Xiao Sun

Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

### 6. [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](http://arxiv.org/pdf/2507.01541v1)

Authors: Álvaro Zaera, Diana Nicoleta Popa, Ivan Sekulic, Paolo Rosso

Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

### 7. [Is External Information Useful for Stance Detection with LLMs?](http://arxiv.org/pdf/2507.01543v1)

Authors: Quang Minh Nguyen, Taegyoon Kim

In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

### 8. [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](http://arxiv.org/pdf/2507.01594v1)

Authors: Shutong Feng, Hsien-chin Lin, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Renato Vukovic, Milica Gašić

Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

### 9. [Chart Question Answering from Real-World Analytical Narratives](http://arxiv.org/pdf/2507.01627v1)

Authors: Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha

We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

### 10. [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](http://arxiv.org/pdf/2507.01645v1)

Authors: Rifki Afina Putri

In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

### Cryptography and Security

### 1. [A Compact 16-bit S-box over Tower Field $\F_{(((2^2)^2)^2)^2}$ with High Security](http://arxiv.org/pdf/2507.01423v1)

Authors: Bahram Rashidi, Behrooz Khadem

This paper introduces a compact and secure 16-bit substitution box (S-box)
designed over the composite field $\F_{(((2^2)^2)^2)^2}$, optimized for both
hardware efficiency and cryptographic robustness. The proposed S-box decomposes
operations into subfields, leveraging a tower field architecture. This enables
significant hardware reduction through optimized field inversion and a low-cost
affine transformation. Security evaluations confirm resilience against linear,
differential, algebraic and DPA attacks, validated via metrics including
Nonlinearity (32512), Differential Uniformity (4), Algebraic Degree (15),
Transparency order (15.9875) and SNR (0.34e-08). The hardware results, in 65 nm
CMOS technology, show the proposed 16-bit S-box has lower hardware resources
consumption and lower critical path delay (CPD) than those of other 16-bit
S-boxes. By integrating high algebraic complexity with resource-efficient
structures, this work addresses the growing demand for scalable cryptographic
primitives in data-sensitive applications, demonstrating that larger S-boxes
can enhance security without proportional hardware costs. The results
underscore the viability of composite field-based architectures in balancing
security and efficiency for modern block ciphers.

### 2. [A new efficient RPKI Design](http://arxiv.org/pdf/2507.01465v1)

Authors: Haya Schulmann, Niklas Vogel

Resource Public Key Infrastructure (RPKI) is a critical security mechanism
for BGP, but the complexity of its architecture is a growing concern as its
adoption scales. Current RPKI design heavily reuses legacy PKI components, such
as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,
all these introduce excessive cryptographic validation, redundant metadata, and
inefficiencies in both storage and processing. We show that these design
choices, although based on established standards, create significant
performance bottlenecks, increase the vulnerability surface, and hinder
scalability for wide-scale Internet deployment.
  In this paper, we perform the first systematic analysis of the root causes of
complexity in RPKI's design and experimentally quantify their real-world
impact. We show that over 70% of validation time in RPKI relying parties is
spent on certificate parsing and signature verification, much of it
unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),
a backwards-compatible redesign that preserves all security guarantees while
substantially reducing protocol overhead. iRPKI eliminates EE-certificates and
ROA signatures, merges revocation and integrity objects, replaces verbose
encodings with Protobuf, and restructures repository metadata for more
efficient access. We experimentally demonstrate that our implementation of
iRPKI in the Routinator validator achieves a 20x speed-up of processing time,
18x improvement of bandwidth requirements and 8x reduction in cache memory
footprint, while also eliminating classes of vulnerabilities that have led to
at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the
feasibility of deploying RPKI at scale in the Internet, and especially in
constrained environments. Our design may be deployed incrementally without
impacting existing operations.

### 3. [EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity Leakage in Blockchain](http://arxiv.org/pdf/2507.01635v1)

Authors: Chenyu Li, Xueping Liang, Xiaorui Gong, Xiu Zhang

While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust
cryptographic designs to protect user privacy, real-world deployment reveals
critical vulnerabilities when users deviate from security guidelines. In this
paper, we design a system called EGNInfoLeaker. Our study is the first work
that uncovers widespread public key reuse across Ethereum's peer-to-peer
networks - a practice that fundamentally undermines the protocol's privacy
guarantees. Through systematic analysis of 300 real-world network snapshots, we
identify 83 users controlling 483 service nodes via public key reuse, enabling
precise de-anonymization through IP correlation. Using evidence collected by
EGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to
network entities and generates comprehensive user profiles. For User27, it
exposes the public key, IP, network ID, location (country/region/city), and
ISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic
misuse transforms theoretical anonymity into practical identity leakage,
exposing users to surveillance and targeted attacks. These findings establish
that protocol security depends not only on sound design but also on strict user
compliance. Going forward, our detection framework provides a foundation for
enhancing real-world privacy preservation in decentralized networks.

### 4. [Towards Better Attribute Inference Vulnerability Measures](http://arxiv.org/pdf/2507.01710v1)

Authors: Paul Francis, David Wagner

The purpose of anonymizing structured data is to protect the privacy of
individuals in the data while retaining the statistical properties of the data.
An important class of attack on anonymized data is attribute inference, where
an attacker infers the value of an unknown attribute of a target individual
given knowledge of one or more known attributes. A major limitation of recent
attribute inference measures is that they do not take recall into account, only
precision. It is often the case that attacks target only a fraction of
individuals, for instance data outliers. Incorporating recall, however,
substantially complicates the measure, because one must determine how to
combine recall and precision in a composite measure for both the attack and
baseline. This paper presents the design and implementation of an attribute
inference measure that incorporates both precision and recall. Our design also
improves on how the baseline attribute inference is computed. In experiments
using a generic best row match attack on moderately-anonymized microdata, we
show that in over 25\% of the attacks, our approach correctly labeled the
attack to be at risk while the prior approach incorrectly labeled the attack to
be safe.

### 5. [Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range](http://arxiv.org/pdf/2507.01768v1)

Authors: Anis Yusof, Yuancheng Liu, Niklaus Kang, Choon Meng Seah, Zhenkai Liang, Ee-Chien Chang

The prevalence of cyberattacks on Industrial Control Systems (ICS) has
highlighted the necessity for robust security measures and incident response to
protect critical infrastructure. This is prominent when Operational Technology
(OT) systems undergo digital transformation by integrating with Information
Technology (IT) systems to enhance operational efficiency, adaptability, and
safety. To support analysts in staying abreast of emerging attack patterns,
there is a need for ICS datasets that reflect indicators representative of
contemporary cyber threats. To address this, we conduct two ICS cyberattack
simulations to showcase the impact of trending ICS cyberattacks on a railway
cyber range that resembles the railway infrastructure. The attack scenario is
designed to blend trending attack trends with attack patterns observed from
historical ICS incidents. The resulting evidence is collected as datasets,
serving as an essential resource for cyberattack analysis. This captures key
indicators that are relevant to the current threat landscape, augmenting the
effectiveness of security systems and analysts to protect against ICS cyber
threats.

### 6. [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](http://arxiv.org/pdf/2507.01487v1)

Authors: Marc Damie, Florian Hahn, Andreas Peter, Jan Ramon

Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

### 7. [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](http://arxiv.org/pdf/2507.01513v1)

Authors: Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, Heng Tao Shen

By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

### 8. [Hardness of Quantum Distribution Learning and Quantum Cryptography](http://arxiv.org/pdf/2507.01292v1)

Authors: Taiga Hiroka, Min-Hsiu Hsieh, Tomoyuki Morimae

The existence of one-way functions (OWFs) forms the minimal assumption in
classical cryptography. However, this is not necessarily the case in quantum
cryptography. One-way puzzles (OWPuzzs), introduced by Khurana and Tomer,
provide a natural quantum analogue of OWFs. The existence of OWPuzzs implies
$PP\neq BQP$, while the converse remains open. In classical cryptography, the
analogous problem-whether OWFs can be constructed from $P \neq NP$-has long
been studied from the viewpoint of hardness of learning. Hardness of learning
in various frameworks (including PAC learning) has been connected to OWFs or to
$P \neq NP$. In contrast, no such characterization previously existed for
OWPuzzs. In this paper, we establish the first complete characterization of
OWPuzzs based on the hardness of a well-studied learning model: distribution
learning. Specifically, we prove that OWPuzzs exist if and only if proper
quantum distribution learning is hard on average. A natural question that
follows is whether the worst-case hardness of proper quantum distribution
learning can be derived from $PP \neq BQP$. If so, and a worst-case to
average-case hardness reduction is achieved, it would imply OWPuzzs solely from
$PP \neq BQP$. However, we show that this would be extremely difficult: if
worst-case hardness is PP-hard (in a black-box reduction), then $SampBQP \neq
SampBPP$ follows from the infiniteness of the polynomial hierarchy. Despite
that, we show that $PP \neq BQP$ is equivalent to another standard notion of
hardness of learning: agnostic. We prove that $PP \neq BQP$ if and only if
agnostic quantum distribution learning with respect to KL divergence is hard.
As a byproduct, we show that hardness of agnostic quantum distribution learning
with respect to statistical distance against $PPT^{\Sigma_3^P}$ learners
implies $SampBQP \neq SampBPP$.

### 9. [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](http://arxiv.org/pdf/2507.01321v1)

Authors: Zhiyao Ren, Siyuan Liang, Aishan Liu, Dacheng Tao

In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

### 10. [Rational Censorship Attack: Breaking Blockchain with a Blackboard](http://arxiv.org/pdf/2507.01453v1)

Authors: Michelle Yeo, Haoqian Zhang

Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

### Computer Vision and Pattern Recognition

### 1. [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](http://arxiv.org/pdf/2507.01254v1)

Authors: Runze Cheng, Xihang Qiu, Ming Li, Ye Zhang, Chun Li, Fei Yu

Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

### 2. [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](http://arxiv.org/pdf/2507.01255v1)

Authors: Xiao Liu, Jiawei Zhang

The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

### 3. [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](http://arxiv.org/pdf/2507.01275v1)

Authors: Chengxu Liu, Lu Qi, Jinshan Pan, Xueming Qian, Ming-Hsuan Yang

Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

### 4. [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](http://arxiv.org/pdf/2507.01290v1)

Authors: Sunyong Seo, Semin Kim, Jongha Lee

Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

### 5. [Physics-informed Ground Reaction Dynamics from Human Motion Capture](http://arxiv.org/pdf/2507.01340v1)

Authors: Cuong Le, Huy-Phuong Le, Duc Le, Minh-Thien Duong, Van-Binh Nguyen, My-Ha Le

Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

### 6. [Learning Camera-Agnostic White-Balance Preferences](http://arxiv.org/pdf/2507.01342v1)

Authors: Luxi Zhao, Mahmoud Afifi, Michael S. Brown

The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

### 7. [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](http://arxiv.org/pdf/2507.01347v1)

Authors: Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu

We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

### 8. [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](http://arxiv.org/pdf/2507.01351v1)

Authors: Chaoxiang Cai, Longrong Yang, Kaibing Chen, Fan Yang, Xi Li

The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

### 9. [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](http://arxiv.org/pdf/2507.01367v1)

Authors: Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao

Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

### 10. [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](http://arxiv.org/pdf/2507.01384v1)

Authors: Langyu Wang, Bingke Zhu, Yingying Chen, Yiyuan Zhang, Ming Tang, Jinqiao Wang

The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

### Computers and Society

### 1. [A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies](http://arxiv.org/pdf/2507.01304v1)

Authors: Ion Nemteanu, Adir Mancebo Jr., Leslie Joe, Ryan Lopez, Patricia Lopez, Warren Woodrich Pettine

Artificial intelligence (AI) offers incredible possibilities for patient
care, but raises significant ethical issues, such as the potential for bias.
Powerful ethical frameworks exist to minimize these issues, but are often
developed for academic or regulatory environments and tend to be comprehensive
but overly prescriptive, making them difficult to operationalize within
fast-paced, resource-constrained environments. We introduce the Scalable Agile
Framework for Execution in AI (SAFE-AI) designed to balance ethical rigor with
business priorities by embedding ethical oversight into standard Agile-based
product development workflows. The framework emphasizes the early establishment
of testable acceptance criteria, fairness metrics, and transparency metrics to
manage model uncertainty, while also promoting continuous monitoring and
re-evaluation of these metrics across the AI lifecycle. A core component of
this framework are responsibility metrics using scenario-based probability
analogy mapping designed to enhance transparency and stakeholder trust. This
ensures that retraining or tuning activities are subject to lightweight but
meaningful ethical review. By focusing on the minimum necessary requirements
for responsible development, our framework offers a scalable, business-aligned
approach to ethical AI suitable for organizations without dedicated ethics
teams.

### 2. [From Reports to Reality: Testing Consistency in Instagram's Digital Services Act Compliance Data](http://arxiv.org/pdf/2507.01787v1)

Authors: Marie-Therese Sekwenz, Ben Wagner, Hans De Bruijn

The Digital Services Act (DSA) introduces harmonized rules for content
moderation and platform governance in the European Union, mandating robust
compliance mechanisms, particularly for very large online platforms and search
engines. This study examined compliance with DSA requirements, focusing on
Instagram as a case study. We develop and apply a multi-level consistency
framework to evaluate DSA compliance. Our findings contribute to the broader
discussion on empirically-based regulation, providing insight into how
researchers, regulators, auditors and platforms can better utilize DSA
mechanisms to improve reporting and enforcement quality and accountability.
This work underscores that consistency can help detect potential compliance
failures. It also demonstrates that platforms should be evaluated as part of an
interconnected ecosystem rather than through isolated processes, which is
crucial for effective compliance evaluation under the DSA.

### 3. [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](http://arxiv.org/pdf/2507.01418v1)

Authors: Inyoung Cheong, Alicia Guo, Mina Lee, Zhehui Liao, Kowe Kadoma, Dongyoung Go, Joseph Chee Chang, Peter Henderson, Mor Naaman, Amy X. Zhang

As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

### 4. [The Thin Line Between Comprehension and Persuasion in LLMs](http://arxiv.org/pdf/2507.01936v1)

Authors: Adrian de Wynter, Tangming Yuan

Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

### 5. [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](http://arxiv.org/pdf/2507.01547v1)

Authors: Ubada El Joulani, Tatiana Kalganova, Stergios-Aristoteles Mitoulis, Sotirios Argyroudis

Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

### 6. [Epistemic Scarcity: The Economics of Unresolvable Unknowns](http://arxiv.org/pdf/2507.01483v1)

Authors: Craig S Wright

This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

### Databases

### 1. [Handling out-of-order input arrival in CEP engines on the edge combining optimistic, pessimistic and lazy evaluation](http://arxiv.org/pdf/2507.01461v1)

Authors: Styliani Kyrama, Anastasios Gounaris

In Complex Event Processing, handling out-of-order, late, and duplicate
events is critical for real-time analytics, especially on resource-constrained
devices that process heterogeneous data from multiple sources. We present
LimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and
speculative processing to efficiently handle data inconsistencies while
supporting multi-pattern detection under relaxed semantics. LimeCEP integrates
Kafka for efficient message ordering, retention, and duplicate elimination, and
offers configurable strategies to trade off between accuracy, latency, and
resource consumption. Compared to state-of-the-art systems like SASE and
FlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up
to 10 times lower memory usage and 6 times lower CPU utilization, while
maintaining near-perfect precision and recall under high-disorder input
streams, making it well-suited for non-cloud deployments.

### 2. [PathDB: A system for evaluating regular path queries](http://arxiv.org/pdf/2507.01755v1)

Authors: Roberto García, Renzo Angles, Vicente Rojas, Sebastián Ferrada

PathDB is a Java-based graph database designed for in-memory data loading and
querying. By utilizing Regular Path Queries (RPQ) and a closed path algebra,
PathDB processes paths through its three main components: the parser, the
logical plan, and the physical plan. This modular design allows for targeted
optimizations and modifications without impacting overall functionality.
Benchmark experiments illustrate PathDB's execution times and flexibility in
handling dynamic and complex path queries, compared to baseline methods like
Depth-First Search (DFS) and Breadth-First Search (BFS) guided by an automaton,
highlighting its optimizations that contribute to its performance.

### 3. [Counterfactual Explanation of Shapley Value in Data Coalitions](http://arxiv.org/pdf/2507.01267v1)

Authors: Michelle Si, Jian Pei

The Shapley value is widely used for data valuation in data markets. However,
explaining the Shapley value of an owner in a data coalition is an unexplored
and challenging task. To tackle this, we formulate the problem of finding the
counterfactual explanation of Shapley value in data coalitions. Essentially,
given two data owners $A$ and $B$ such that $A$ has a higher Shapley value than
$B$, a counterfactual explanation is a smallest subset of data entries in $A$
such that transferring the subset from $A$ to $B$ makes the Shapley value of
$A$ less than that of $B$. We show that counterfactual explanations always
exist, but finding an exact counterfactual explanation is NP-hard. Using Monte
Carlo estimation to approximate counterfactual explanations directly according
to the definition is still very costly, since we have to estimate the Shapley
values of owners $A$ and $B$ after each possible subset shift. We develop a
series of heuristic techniques to speed up computation by estimating
differential Shapley values, computing the power of singular data entries, and
shifting subsets greedily, culminating in the SV-Exp algorithm. Our
experimental results on real datasets clearly demonstrate the efficiency of our
method and the effectiveness of counterfactuals in interpreting the Shapley
value of an owner.

### 4. [A bibliometric analysis on the current situation and hot trends of the impact of microplastics on soil based on CiteSpace](http://arxiv.org/pdf/2507.01520v1)

Authors: Yiran Zheng, Yue Quan, Su Yan, Xinting Lv, Yuguanmin Cao, Minjie Fu, Mingji Jin

This paper aims to comprehensively grasp the research status and development
trends of soil microplastics (MPs). It collects studies from the Web of Science
Core Collection covering the period from 2013 to 2024. Employing CiteSpace and
VOSviewer, the paper conducts in - depth analyses of literature regarding the
environmental impacts of microplastics. These analyses involve keyword co -
occurrence, clustering, burst term identification, as well as co - occurrence
analysis of authors and institutions. Microplastics can accumulate in soil,
transfer through food chains, and ultimately affect human health, making the
research on them essential for effective pollution control. Focusing on the
international research on the impacts of microplastics on soil and ecosystems,
the study reveals a steadily increasing trend in the number of publications
each year, reaching a peak of 956 articles in 2024. A small number of highly
productive authors contribute significantly to the overall research output. The
keyword clustering analysis results in ten major clusters, including topics
such as plastic pollution and microbial communities. The research on soil
microplastics has evolved through three distinct stages: the preliminary
exploration phase from 2013 to 2016, the expansion phase from 2017 to 2020, and
the integration phase from 2021 to 2024. For future research, multi - level
assessments of the impacts of microplastics on soil ecosystems and organisms
should be emphasized, in order to fully uncover the associated hazards and
develop practical solutions.

### 5. [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](http://arxiv.org/pdf/2507.01599v1)

Authors: Zhaoyan Sun, Jiayi Wang, Xinyang Zhao, Jiachi Wang, Guoliang Li

Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

### 6. [Enhanced Influence-aware Group Recommendation for Online Media Propagation](http://arxiv.org/pdf/2507.01616v1)

Authors: Chengkun He, Xiangmin Zhou, Chen Wang, Longbing Cao, Jie Shao, Xiaodong Li, Guang Xu, Carrie Jinqiu Hu, Zahir Tari

Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

### Distributed, Parallel, and Cluster Computing

### 1. [EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data](http://arxiv.org/pdf/2507.01615v1)

Authors: Alper Alimoglu, Kamil Erdayandi, Mustafa A. Mustafa, Ümit Cali

This paper proposes a new decentralized framework, named EDGChain-E
(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,
encrypted energy data using blockchain and the InterPlanetary File System. The
framework incorporates a Decentralized Autonomous Organization (DAO) to
orchestrate collaborative data governance across the lifecycle of energy
research and operations, such as smart grid monitoring, demand forecasting, and
peer-to-peer energy trading. In EDGChain-E, initial commits capture the full
encrypted datasets-such as smart meter readings or grid telemetry-while
subsequent updates are tracked as encrypted Git patches, ensuring integrity,
traceability, and privacy. This versioning mechanism supports secure
collaboration across multiple stakeholders (e.g., utilities, researchers,
regulators) without compromising sensitive or regulated information. We
highlight the framework's capability to maintain FAIR-compliant (Findable,
Accessible, Interoperable, Reusable) provenance of encrypted data. By embedding
hash-based content identifiers in Merkle trees, the system enables transparent,
auditable, and immutable tracking of data changes, thereby supporting
reproducibility and trust in decentralized energy applications.

### 2. [Evolving HPC services to enable ML workloads on HPE Cray EX](http://arxiv.org/pdf/2507.01880v1)

Authors: Stefano Schuppli, Fawzi Mohamed, Henrique Mendonça, Nina Mujkanovic, Elia Palme, Dino Conciatore, Lukas Drescher, Miguel Gila, Pim Witlox, Joost VandeVondele, Maxime Martinasso, Thomas C. Schulthess, Torsten Hoefler

The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

### 3. [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](http://arxiv.org/pdf/2507.01285v1)

Authors: Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal

Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

### 4. [Optimal Dispersion Under Asynchrony](http://arxiv.org/pdf/2507.01298v1)

Authors: Debasish Pattanayak, Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma

We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$
mobile agents, each with a unique ID and initially located arbitrarily on the
nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously
relocate so that no node hosts more than one agent. Dispersion serves as a
fundamental task in distributed computing of mobile agents, and its complexity
stems from key challenges in local coordination under anonymity and limited
memory.
  The goal is to minimize both the time to achieve dispersion and the memory
required per agent. It is known that any algorithm requires $\Omega(k)$ time in
the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result
[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and
an $O(k \log k)$-time algorithm in the asynchronous setting, both using
$O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by
presenting the first dispersion algorithm that runs in optimal $O(k)$ time
using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a
novel technique we develop in this paper that constructs a port-one tree in
anonymous graphs, which may be of independent interest.

### 5. [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](http://arxiv.org/pdf/2507.01438v1)

Authors: Zheyu Shen, Yexiao He, Ziyao Wang, Yuning Zhang, Guoheng Sun, Wanghao Ye, Ang Li

Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

### 6. [Rational Censorship Attack: Breaking Blockchain with a Blackboard](http://arxiv.org/pdf/2507.01453v1)

Authors: Michelle Yeo, Haoqian Zhang

Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

### 7. [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](http://arxiv.org/pdf/2507.01676v1)

Authors: Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli

Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

### 8. [Analyzing Common Electronic Structure Theory Algorithms for Distributed Quantum Computing](http://arxiv.org/pdf/2507.01902v1)

Authors: Grier M. Jones, Hans-Arno Jacobsen

To move towards the utility era of quantum computing, many corporations have
posed distributed quantum computing (DQC) as a framework for scaling the
current generation of devices for practical applications. One of these
applications is quantum chemistry, also known as electronic structure theory,
which has been poised as a "killer application" of quantum computing, To this
end, we analyze five electronic structure methods, found in common packages
such as Tequila and ffsim, which can be easily interfaced with the Qiskit
Circuit Cutting addon. Herein, we provide insights into cutting these
algorithms using local operations (LO) to determine their aptitude for
distribution. The key findings of our work are that many of these algorithms
cannot be efficiently parallelized using LO, and new methods must be developed
to apply electronic structure theory within a DQC framework.

### 9. [GPU-based complete search for nonlinear minimization subject to bounds](http://arxiv.org/pdf/2507.01770v1)

Authors: Guanglu Zhang, Qihang Shan, Jonathan Cagan

This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

### Digital Libraries

### 1. [A bibliometric analysis on the current situation and hot trends of the impact of microplastics on soil based on CiteSpace](http://arxiv.org/pdf/2507.01520v1)

Authors: Yiran Zheng, Yue Quan, Su Yan, Xinting Lv, Yuguanmin Cao, Minjie Fu, Mingji Jin

This paper aims to comprehensively grasp the research status and development
trends of soil microplastics (MPs). It collects studies from the Web of Science
Core Collection covering the period from 2013 to 2024. Employing CiteSpace and
VOSviewer, the paper conducts in - depth analyses of literature regarding the
environmental impacts of microplastics. These analyses involve keyword co -
occurrence, clustering, burst term identification, as well as co - occurrence
analysis of authors and institutions. Microplastics can accumulate in soil,
transfer through food chains, and ultimately affect human health, making the
research on them essential for effective pollution control. Focusing on the
international research on the impacts of microplastics on soil and ecosystems,
the study reveals a steadily increasing trend in the number of publications
each year, reaching a peak of 956 articles in 2024. A small number of highly
productive authors contribute significantly to the overall research output. The
keyword clustering analysis results in ten major clusters, including topics
such as plastic pollution and microbial communities. The research on soil
microplastics has evolved through three distinct stages: the preliminary
exploration phase from 2013 to 2016, the expansion phase from 2017 to 2020, and
the integration phase from 2021 to 2024. For future research, multi - level
assessments of the impacts of microplastics on soil ecosystems and organisms
should be emphasized, in order to fully uncover the associated hazards and
develop practical solutions.

### 2. [A Dynamical Cartography of the Epistemic Diffusion of Artificial Intelligence in Neuroscience](http://arxiv.org/pdf/2507.01651v1)

Authors: Sylvain Fontaine

Neuroscience and AI have an intertwined history, largely relayed in the
literature of both fields. In recent years, due to the engineering orientations
of AI research and the monopoly of industry for its large-scale applications,
the mutual expansion of neuroscience and AI in fundamental research seems
challenged. In this paper, we bring some empirical evidences that, on the
contrary, AI and neuroscience are continuing to grow together, but with a
pronounced interest in the fields of study related to neurodegenerative
diseases since the 1990s. With a temporal knowledge cartography of neuroscience
drawn with advanced document embedding techniques, we draw the dynamical
shaping of the discipline since the 1970s and identified the conceptual
articulation of AI with this particular subfield mentioned before. However, a
further analysis of the underlying citation network of the studied corpus shows
that the produced AI technologies remain confined in the different subfields
and are not transferred from one subfield to another. This invites us to
discuss the genericity capability of AI in the context of an intradisciplinary
development, especially in the diffusion of its associated metrology.

### Discrete Mathematics

### 1. [Multipacking in Hypercubes](http://arxiv.org/pdf/2507.01565v1)

Authors: Deepak Rajendraprasad, Varun Sani, Birenjith Sasidharan, Jishnu Sen

For an undirected graph $G$, a dominating broadcast on $G$ is a function $f :
V(G) \rightarrow \mathbb{N}$ such that for any vertex $u \in V(G)$, there
exists a vertex $v \in V(G)$ with $f(v) \geqslant 1$ and $d(u,v) \leqslant
f(v)$. The cost of $f$ is $\sum_{v \in V} f(v)$. The minimum cost over all the
dominating broadcasts on $G$ is defined as the broadcast domination number
$\gamma_b(G)$ of $G$. A multipacking in $G$ is a subset $M \subseteq V(G)$ such
that, for every vertex $v \in V(G)$ and every positive integer $r$, the number
of vertices in $M$ within distance $r$ of $v$ is at most $r$. The multipacking
number of $G$, denoted $\operatorname{mp}(G)$, is the maximum cardinality of a
multipacking in $G$. These two optimisation problems are duals of each other,
and it easily follows that $\operatorname{mp}(G) \leqslant \gamma_b(G)$. It is
known that $\gamma_b(G) \leqslant 2\operatorname{mp}(G)+3$ and conjectured that
$\gamma_b(G) \leqslant 2\operatorname{mp}(G)$.
  In this paper, we show that for the $n$-dimensional hypercube $Q_n$ $$
\left\lfloor\frac{n}{2} \right\rfloor
  \leqslant \operatorname{mp}(Q_n)
  \leqslant \frac{n}{2} + 6\sqrt{2n}. $$
  Since $\gamma_b(Q_n) = n-1$ for all $n \geqslant 3$, this verifies the above
conjecture on hypercubes and, more interestingly, gives a sequence of connected
graphs for which the ratio $\frac{\gamma_b(G)}{\operatorname{mp}(G)}$
approaches $2$, a search for which was initiated by Beaudou, Brewster and
Foucaud in 2018. It follows that, for connected graphs $G$ $$
  \limsup_{\operatorname{mp}(G) \rightarrow \infty}
\left\{\frac{\gamma_b(G)}{\operatorname{mp}(G)}\right\} = 2.$$
  The lower bound on $\operatorname{mp}(Q_n)$ is established by a recursive
construction, and the upper bound is established using a classic result from
discrepancy theory.

### 2. [Scheduling on identical machines with conflicts to minimize the mean flow time](http://arxiv.org/pdf/2507.01759v1)

Authors: Nour ElHouda Tellache, Lydia Aoudia, Mourad Boudhar

This paper addresses the problem of scheduling jobs on identical machines
with conflict constraints, where certain jobs cannot be scheduled
simultaneously on different machines. We focus on the case where conflicts can
be represented by a simple undirected graph, and the objective is to minimize
the mean flow time. We show that the problem is NP-hard even on two machines
and two distinct processing times. For unit-time jobs, the problem becomes
NP-hard when the number of machines increases to three. We also identify
polynomial-time solvable cases for specific classes of conflict graphs. For the
general problem, we propose mathematical models, lower bounds, and a genetic
algorithm. We evaluate their performance through computational experiments on a
wide range of instances derived from well-known benchmark instances in the
literature.

### 3. [Some remarks on the uncolored versions of the original CFI-graphs](http://arxiv.org/pdf/2507.01459v1)

Authors: Yijia Chen, Jörg Flum, Mingjun Liu

The CFI-graphs, named after Cai, F\"urer, and Immerman, are central to the
study of the graph isomorphism testing and of first-order logic with counting.
They are colored graphs, and the coloring plays a role in many of their
applications. As usual, it is not hard to remove the coloring by some extra
graph gadgets, but at the cost of blowing up the size of the graphs and
changing some parameters of them as well. This might lead to suboptimal
combinatorial bounds important to their applications. Since then for some
uncolored variants of the CFI-graphs it has been shown that they serve the same
purposes. We show that this already applies to the graphs obtained from the
original CFI-graphs by forgetting the colors. Moreover, we will see that there
is a first-order formula $\varphi(x,y)$ expressing in almost all uncolored
CFI-graphs that $x$ and $y$ have the same color in the corresponding colored
graphs.

### Data Structures and Algorithms

### 1. [Faster Algorithm for Second (s,t)-mincut and Breaking Quadratic barrier for Dual Edge Sensitivity for (s,t)-mincut](http://arxiv.org/pdf/2507.01366v1)

Authors: Surender Baswana, Koustav Bhanja, Anupam Roy

We study (s,t)-cuts of second minimum capacity and present the following
algorithmic and graph-theoretic results.
  1. Vazirani and Yannakakis [ICALP 1992] designed the first algorithm for
computing an (s,t)-cut of second minimum capacity using $O(n^2)$ maximum
(s,t)-flow computations. For directed integer-weighted graphs, we significantly
improve this bound by designing an algorithm that computes an $(s,t)$-cut of
second minimum capacity using $O(\sqrt{n})$ maximum (s,t)-flow computations
w.h.p. To achieve this result, a close relationship of independent interest is
established between $(s,t)$-cuts of second minimum capacity and global mincuts
in directed weighted graphs.
  2. Minimum+1 (s,t)-cuts have been studied quite well recently [Baswana,
Bhanja, and Pandey, ICALP 2022], which is a special case of second
(s,t)-mincut.
  (a) For directed multi-graphs, we design an algorithm that, given any maximum
(s,t)-flow, computes a minimum+1 (s,t)-cut, if it exists, in $O(m)$ time.
  (b) The existing structures for storing and characterizing all minimum+1
(s,t)-cuts occupy $O(mn)$ space. For undirected multi-graphs, we design a DAG
occupying only $O(m)$ space that stores and characterizes all minimum+1
(s,t)-cuts.
  3. The study of minimum+1 (s,t)-cuts often turns out to be useful in
designing dual edge sensitivity oracles -- a compact data structure for
efficiently reporting an (s,t)-mincut after insertion/failure of any given pair
of query edges. It has been shown recently [Bhanja, ICALP 2025] that any dual
edge sensitivity oracle for (s,t)-mincut in undirected multi-graphs must occupy
${\Omega}(n^2)$ space in the worst-case, irrespective of the query time. For
simple graphs, we break this quadratic barrier while achieving a non-trivial
query time.

### 2. [SPARSE-PIVOT: Dynamic correlation clustering for node insertions](http://arxiv.org/pdf/2507.01830v1)

Authors: Mina Dalirrooyfard, Konstantin Makarychev, Slobodan Mitrović

We present a new Correlation Clustering algorithm for a dynamic setting where
nodes are added one at a time. In this model, proposed by Cohen-Addad,
Lattanzi, Maggiori, and Parotsidis (ICML 2024), the algorithm uses database
queries to access the input graph and updates the clustering as each new node
is added. Our algorithm has the amortized update time of
$O_{\epsilon}(\log^{O(1)}(n))$. Its approximation factor is $20+\varepsilon$,
which is a substantial improvement over the approximation factor of the
algorithm by Cohen-Addad et al. We complement our theoretical findings by
empirically evaluating the approximation guarantee of our algorithm. The
results show that it outperforms the algorithm by Cohen-Addad et al.~in
practice.

### 3. [Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition](http://arxiv.org/pdf/2507.01873v1)

Authors: Anders Aamand, Justin Y. Chen, Mina Dalirrooyfard, Slobodan Mitrović, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu

We study differentially private algorithms for graph cut sparsification, a
fundamental problem in algorithms, privacy, and machine learning. While
significant progress has been made, the best-known private and efficient cut
sparsifiers on $n$-node graphs approximate each cut within
$\widetilde{O}(n^{1.5})$ additive error and $1+\gamma$ multiplicative error for
any $\gamma > 0$ [Gupta, Roth, Ullman TCC'12]. In contrast, "inefficient"
algorithms, i.e., those requiring exponential time, can achieve an
$\widetilde{O}(n)$ additive error and $1+\gamma$ multiplicative error
[Eli{\'a}{\v{s}}, Kapralov, Kulkarni, Lee SODA'20]. In this work, we break the
$n^{1.5}$ additive error barrier for private and efficient cut sparsification.
We present an $(\varepsilon,\delta)$-DP polynomial time algorithm that, given a
non-negative weighted graph, outputs a private synthetic graph approximating
all cuts with multiplicative error $1+\gamma$ and additive error $n^{1.25 +
o(1)}$ (ignoring dependencies on $\varepsilon, \delta, \gamma$).
  At the heart of our approach lies a private algorithm for expander
decomposition, a popular and powerful technique in (non-private) graph
algorithms.

### 4. [Dynamic Similarity Graph Construction with Kernel Density Estimation](http://arxiv.org/pdf/2507.01696v1)

Authors: Steinar Laenen, Peter Macgregor, He Sun

In the kernel density estimation (KDE) problem, we are given a set $X$ of
data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in
\mathbb{R}^d$, and the objective is to quickly output an estimate of
$\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$. In this paper, we consider
$\textsf{KDE}$ in the dynamic setting, and introduce a data structure that
efficiently maintains the estimates for a set of query points as data points
are added to $X$ over time. Based on this, we design a dynamic data structure
that maintains a sparse approximation of the fully connected similarity graph
on $X$, and develop a fast dynamic spectral clustering algorithm. We further
evaluate the effectiveness of our algorithms on both synthetic and real-world
datasets.

### 5. [A Deterministic Partition Tree and Applications](http://arxiv.org/pdf/2507.01775v1)

Authors: Haitao Wang

In this paper, we present a deterministic variant of Chan's randomized
partition tree [Discret. Comput. Geom., 2012]. This result leads to numerous
applications. In particular, for $d$-dimensional simplex range counting (for
any constant $d \ge 2$), we construct a data structure using $O(n)$ space and
$O(n^{1+\epsilon})$ preprocessing time, such that each query can be answered in
$o(n^{1-1/d})$ time (specifically, $O(n^{1-1/d} / \log^{\Omega(1)} n)$ time),
thereby breaking an $\Omega(n^{1-1/d})$ lower bound known for the semigroup
setting. Notably, our approach does not rely on any bit-packing techniques. We
also obtain deterministic improvements for several other classical problems,
including simplex range stabbing counting and reporting, segment intersection
detection, counting and reporting, ray-shooting among segments, and more.
Similar to Chan's original randomized partition tree, we expect that additional
applications will emerge in the future, especially in situations where
deterministic results are preferred.

### 6. [Optimal Dispersion Under Asynchrony](http://arxiv.org/pdf/2507.01298v1)

Authors: Debasish Pattanayak, Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma

We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$
mobile agents, each with a unique ID and initially located arbitrarily on the
nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously
relocate so that no node hosts more than one agent. Dispersion serves as a
fundamental task in distributed computing of mobile agents, and its complexity
stems from key challenges in local coordination under anonymity and limited
memory.
  The goal is to minimize both the time to achieve dispersion and the memory
required per agent. It is known that any algorithm requires $\Omega(k)$ time in
the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result
[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and
an $O(k \log k)$-time algorithm in the asynchronous setting, both using
$O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by
presenting the first dispersion algorithm that runs in optimal $O(k)$ time
using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a
novel technique we develop in this paper that constructs a port-one tree in
anonymous graphs, which may be of independent interest.

### Emerging Technologies

### 1. [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](http://arxiv.org/pdf/2507.01429v1)

Authors: Benjamin Chen Ming Choong, Tao Luo, Cheng Liu, Bingsheng He, Wei Zhang, Joey Tianyi Zhou

Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

### 2. [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](http://arxiv.org/pdf/2507.01462v1)

Authors: Eneko Osaba, Estibaliz Garrote, Pablo Miranda-Rodriguez, Alessia Ciacco, Itziar Cabanes, Aitziber Mancisidor

This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

### 3. [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](http://arxiv.org/pdf/2507.01808v1)

Authors: Xiaoyu Ji, Jessica Shorland, Joshua Shank, Pascal Delpe-Brice, Latanya Sweeney, Jan Allebach, Ali Shakouri

Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

### 4. [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](http://arxiv.org/pdf/2507.01284v1)

Authors: Cristian Gariboldi, Hayato Tokida, Ken Kinjo, Yuki Asada, Alexander Carballo

Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

### Graphics

### 1. [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](http://arxiv.org/pdf/2507.01305v1)

Authors: Worameth Chinchuthakun, Pakkapon Phongthawee, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn

We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

### 2. [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](http://arxiv.org/pdf/2507.01631v1)

Authors: Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet

Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

### Computer Science and Game Theory

### 1. [Counterfactual Explanation of Shapley Value in Data Coalitions](http://arxiv.org/pdf/2507.01267v1)

Authors: Michelle Si, Jian Pei

The Shapley value is widely used for data valuation in data markets. However,
explaining the Shapley value of an owner in a data coalition is an unexplored
and challenging task. To tackle this, we formulate the problem of finding the
counterfactual explanation of Shapley value in data coalitions. Essentially,
given two data owners $A$ and $B$ such that $A$ has a higher Shapley value than
$B$, a counterfactual explanation is a smallest subset of data entries in $A$
such that transferring the subset from $A$ to $B$ makes the Shapley value of
$A$ less than that of $B$. We show that counterfactual explanations always
exist, but finding an exact counterfactual explanation is NP-hard. Using Monte
Carlo estimation to approximate counterfactual explanations directly according
to the definition is still very costly, since we have to estimate the Shapley
values of owners $A$ and $B$ after each possible subset shift. We develop a
series of heuristic techniques to speed up computation by estimating
differential Shapley values, computing the power of singular data entries, and
shifting subsets greedily, culminating in the SV-Exp algorithm. Our
experimental results on real datasets clearly demonstrate the efficiency of our
method and the effectiveness of counterfactuals in interpreting the Shapley
value of an owner.

### 2. [Evaluating LLM Agent Collusion in Double Auctions](http://arxiv.org/pdf/2507.01413v1)

Authors: Kushal Agrawal, Verona Teo, Juan J. Vazquez, Sudarsh Kunnavakkam, Vishak Srikanth, Andy Liu

Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

### 3. [Rational Censorship Attack: Breaking Blockchain with a Blackboard](http://arxiv.org/pdf/2507.01453v1)

Authors: Michelle Yeo, Haoqian Zhang

Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

### 4. [Enriching the Felsenthal index with a priori unions for decision-making processes](http://arxiv.org/pdf/2507.01621v1)

Authors: Alicia Mascareñas-Pazos, Silvia Lorenzo-Freire, Jose Maria Alonso-Meijide

Within the domain of game theory, power indexes are defined as functions that
quantify the influence of individual participants in collective decision-making
processes. Felsenthal [D. Felsenthal. A Well-Behaved Index of a Priori P-Power
for Simple N-Person Games. Homo Oeconomicus, 33, 2016] proposed a power index
with a focus on least size winning coalitions, i.e., those coalitions capable
of determining the final outcome and with the smallest number of players among
all winning coalitions. However, the Felsenthal index overlooks pre-existing
affinities between the players, a common and impactful factor in real-world
political and economic contexts. This paper introduces the Felsenthal Owen
power index, a novel index based on Felsenthal's approach that integrates
player affinities using Owen's a priori unions framework. The new index is
rigorously characterised by two distinct sets of axiomatic properties. We
demonstrate its practical utility by applying it to the International Monetary
Fund's voting system, revealing how strategic alliances significantly reshape
power distributions. The index thus offers policymakers a more sophisticated
tool for measuring influence in complex decision-making scenarios.

### Human-Computer Interaction

### 1. [Challenges & Opportunities with LLM-Assisted Visualization Retargeting](http://arxiv.org/pdf/2507.01436v1)

Authors: Luke S. Snyder, Chenglong Wang, Steven Drucker

Despite the ubiquity of visualization examples published on the web,
retargeting existing custom chart implementations to new datasets remains
difficult, time-intensive, and tedious. The adaptation process assumes author
familiarity with both the implementation of the example as well as how the new
dataset might need to be transformed to fit into the example code. With recent
advances in Large Language Models (LLMs), automatic adaptation of code can be
achieved from high-level user prompts, reducing the barrier for visualization
retargeting. To better understand how LLMs can assist retargeting and its
potential limitations, we characterize and evaluate the performance of LLM
assistance across multiple datasets and charts of varying complexity,
categorizing failures according to type and severity. In our evaluation, we
compare two approaches: (1) directly instructing the LLM model to fully
generate and adapt code by treating code as text inputs and (2) a more
constrained program synthesis pipeline where the LLM guides the code
construction process by providing structural information (e.g., visual
encodings) based on properties of the example code and data. We find that both
approaches struggle when new data has not been appropriately transformed, and
discuss important design recommendations for future retargeting systems.

### 2. [Analysis of Drone-Assisted Building Inspection Training in VR vs 2D Monitor Display: an EEG Study](http://arxiv.org/pdf/2507.01471v1)

Authors: Pengkun Liu, Jackson Greene, Jiali Huang, Pingbo Tang, Yu Hou

Researchers have been using simulation-based methods for drone-assisted
inspection training. Multiple brain regions are associated with information
processes and decision-making, and the connectivity of these regions may
further influence inspectors' performance. However, researchers do not
understand the pathways of the information flows when drone pilots process the
maintenance and manipulation of information, which may affect the efficiency of
tacit knowledge transfer. This study aims to reveal the causal connection
between participants' brain regions using an electroencephalogram and dynamic
causal modeling when processing drone-assisted building energy audit tasks
using different display modalities. The results showed similar single-direction
connectivity patterns for the different simulation groups. The results also
showed similar patterns between brain regions related to visual inspection
performance before and after training. These findings highlight the nature of
brain asymmetries and may be utilized in measuring cognitive states and
designing adaptive automation in the knowledge transfer of drone-based
inspection.

### 3. [Designing for Community Care: Reimagining Support for Equity & Well-being in Academia](http://arxiv.org/pdf/2507.01690v1)

Authors: Beatriz Severes, Ana O. Henriques, Rory Clark, Paulo Bala, Anna Carter, Rua Mae Williams, Geraldine Fitzpatrick

Academic well-being is deeply influenced by peer-support networks, yet they
remain informal, inequitable, and unsustainable, often relying on personal
connections and social capital rather than structured, inclusive systems.
Additionally, institutional well-being responses frequently focus on student
populations, neglecting the emotional labour of faculty and staff, reinforcing
an exclusionary academic culture. Drawing on HCI methodologies, participatory
design, and care ethics, this workshop will provide a space for rethinking how
academic communities can support inclusive networks. Through pre-workshop
engagement, co-design activities, and reflection, participants will examine
systemic gaps in networks and explore ways to embed care, equity, and
sustainability into academic peer-support frameworks -- from informal,
exclusionary models to structured, inclusive care-based ecosystems. At the end
of the workshop, participants will co-develop design strategies for integrating
care and resilience in academic ecosystems, resources for designing equitable
support systems, and a peer network invested and committed to fostering a
supportive academic community.

### 4. [Spatial tangible user interfaces for cognitive assessment and training](http://arxiv.org/pdf/2507.01944v1)

Authors: Ehud Sharlin, Yuichi Itoh, Benjamin Watson, Yoshifumi Kitamura, Steve Sutphen, Lili Liu, Fumio Kishino

This paper discusses Tangible User Interfaces (TUIs) and their potential
impact on cognitive assessment and cognitive training. We believe that TUIs,
and particularly a subset that we dub spatial TUIs, can extend human computer
interaction beyond some of its current limitations. Spatial TUIs exploit human
innate spatial and tactile ability in an intuitive and direct manner, affording
interaction paradigms that are practically impossible using current interface
technology. As proof-of-concept we examine implementations in the field of
cognitive assessment and training. In this paper we use Cognitive Cubes, a
novel TUI we developed, as an applied test bed for our beliefs, presenting
promising experimental results for cognitive assessment of spatial ability, and
possibly for training purposes.

### 5. [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](http://arxiv.org/pdf/2507.01274v1)

Authors: Vishakha Lall, Yisi Liu

Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

### 6. [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](http://arxiv.org/pdf/2507.01282v1)

Authors: Matthew JY Kang, Wenli Yang, Monica R Roberts, Byeong Ho Kang, Charles B Malpas

The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

### 7. [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](http://arxiv.org/pdf/2507.01719v1)

Authors: Dorian Peters, Fernanda Espinoza, Marco da Re, Guido Ivetta, Luciana Benotti, Rafael A. Calvo

There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

### 8. [Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts](http://arxiv.org/pdf/2507.01776v1)

Authors: Yuxuan Yang

The integration of machine learning (ML) into spatial design holds immense
potential for optimizing space utilization, enhancing functionality, and
streamlining design processes. ML can automate tasks, predict performance
outcomes, and tailor spaces to user preferences. However, the emotional,
cultural, and aesthetic dimensions of design remain crucial for creating spaces
that truly resonate with users-elements that ML alone cannot address. The key
challenge lies in harmonizing data-driven efficiency with the nuanced,
subjective aspects of design. This paper proposes a human-machine collaboration
framework to bridge this gap. An effective framework should recognize that
while ML enhances design efficiency through automation and prediction, it must
be paired with human creativity to ensure spaces are emotionally engaging and
culturally relevant. Human designers contribute intuition, empathy, and
cultural insight, guiding ML-generated solutions to align with users' emotional
and cultural needs. Additionally, we explore how various ML models can be
integrated with human-centered design principles. These models can automate
design generation and optimization, while human designers refine the outputs to
ensure emotional resonance and aesthetic appeal. Through case studies in office
and residential design, we illustrate how this framework fosters both
creativity and cultural relevance. By merging ML with human creativity, spatial
design can achieve a balance of efficiency and emotional impact, resulting in
environments that are both functional and deeply human.

### 9. [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](http://arxiv.org/pdf/2507.01862v1)

Authors: Sanjay Krishna Anbalagan, Xinrui Nie, Umesh Mohan, Vijay Kumar Kanamarlapudi, Anughna Kommalapati, Xiaodan Zhao

Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

### 10. [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](http://arxiv.org/pdf/2507.01431v1)

Authors: Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao

Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

### Information Retrieval

### 1. [DARTS: A Dual-View Attack Framework for Targeted Manipulation in Federated Sequential Recommendation](http://arxiv.org/pdf/2507.01383v1)

Authors: Qitao Qin, Yucong Luo, Zhibo Chu

Federated recommendation (FedRec) preserves user privacy by enabling
decentralized training of personalized models, but this architecture is
inherently vulnerable to adversarial attacks. Significant research has been
conducted on targeted attacks in FedRec systems, motivated by commercial and
social influence considerations. However, much of this work has largely
overlooked the differential robustness of recommendation models. Moreover, our
empirical findings indicate that existing targeted attack methods achieve only
limited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven
by these observations, we focus on investigating targeted attacks in FSR and
propose a novel dualview attack framework, named DV-FSR. This attack method
uniquely combines a sampling-based explicit strategy with a contrastive
learning-based implicit gradient strategy to orchestrate a coordinated attack.
Additionally, we introduce a specific defense mechanism tailored for targeted
attacks in FSR, aiming to evaluate the mitigation effects of the attack method
we proposed. Extensive experiments validate the effectiveness of our proposed
approach on representative sequential models. Our codes are publicly available.

### 2. [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](http://arxiv.org/pdf/2507.01297v1)

Authors: Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, Sewon Min

Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

### 3. [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](http://arxiv.org/pdf/2507.01633v1)

Authors: Georgii Levtsov, Dmitry Ustalov

With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

### 4. [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](http://arxiv.org/pdf/2507.01810v1)

Authors: Nikita Neveditsin, Pawan Lingras, Vijay Mago

We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

### 5. [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](http://arxiv.org/pdf/2507.01285v1)

Authors: Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal

Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

### 6. [Enhanced Influence-aware Group Recommendation for Online Media Propagation](http://arxiv.org/pdf/2507.01616v1)

Authors: Chengkun He, Xiangmin Zhou, Chen Wang, Longbing Cao, Jie Shao, Xiaodong Li, Guang Xu, Carrie Jinqiu Hu, Zahir Tari

Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

### 7. [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](http://arxiv.org/pdf/2507.01676v1)

Authors: Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli

Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

### 8. [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](http://arxiv.org/pdf/2507.01717v1)

Authors: Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati

Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

### Machine Learning

### 1. [Decomposing Prediction Mechanisms for In-Context Recall](http://arxiv.org/pdf/2507.01414v1)

Authors: Sultan Daniels, Dylan Davis, Dhruv Gautam, Wentinn Liao, Gireeja Ranade, Anant Sahai

We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

### 2. [Loss Functions in Diffusion Models: A Comparative Study](http://arxiv.org/pdf/2507.01516v1)

Authors: Dibyanshu Kumar, Philipp Vaeth, Magda Gregorová

Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

### 3. [MARVIS: Modality Adaptive Reasoning over VISualizations](http://arxiv.org/pdf/2507.01544v1)

Authors: Benjamin Feuer, Lennart Purucker, Oussama Elachqar, Chinmay Hegde

Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

### 4. [Analysis of Muon's Convergence and Critical Batch Size](http://arxiv.org/pdf/2507.01598v1)

Authors: Naoki Sato, Hiroki Naganuma, Hideaki Iiduka

This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

### 5. [Dance Dance ConvLSTM](http://arxiv.org/pdf/2507.01644v1)

Authors: Miguel O'Malley

\textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

### 6. [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](http://arxiv.org/pdf/2507.01695v1)

Authors: Omkar Shende, Gayathri Ananthanarayanan, Marcello Traiola

Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

### 7. [Variational Graph Convolutional Neural Networks](http://arxiv.org/pdf/2507.01699v1)

Authors: Illia Oleksiienko, Juho Kanniainen, Alexandros Iosifidis

Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

### 8. [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](http://arxiv.org/pdf/2507.01714v1)

Authors: Kevin Innerebner, Franz M. Rohrhofer, Bernhard C. Geiger

Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

### 9. [Revisiting Learning Rate Control](http://arxiv.org/pdf/2507.01724v1)

Authors: Micha Henheik, Theresa Eimer, Marius Lindauer

The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

### 10. [Towards Decentralized and Sustainable Foundation Model Training with the Edge](http://arxiv.org/pdf/2507.01803v1)

Authors: Leyang Xue, Meghana Madhyastha, Randal Burns, Myungjin Lee, Mahesh K. Marina

Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

### Neural and Evolutionary Computing

### 1. [Diversity-Preserving Exploitation of Crossover](http://arxiv.org/pdf/2507.01524v1)

Authors: Johannes Lengler, Tom Offermann

Crossover is a powerful mechanism for generating new solutions from a given
population of solutions. Crossover comes with a discrepancy in itself: on the
one hand, crossover usually works best if there is enough diversity in the
population; on the other hand, exploiting the benefits of crossover reduces
diversity. This antagonism often makes crossover reduce its own effectiveness.
  We introduce a new paradigm for utilizing crossover that reduces this
antagonism, which we call diversity-preserving exploitation of crossover
(DiPEC). The resulting Diversity Exploitation Genetic Algorithm (DEGA) is able
to still exploit the benefits of crossover, but preserves a much higher
diversity than conventional approaches.
  We demonstrate the benefits by proving that the (2+1)-DEGA finds the optimum
of LeadingOnes with $O(n^{5/3}\log^{2/3} n)$ fitness evaluations. This is
remarkable since standard genetic algorithms need $\Theta(n^2)$ evaluations,
and among genetic algorithms only some artificial and specifically tailored
algorithms were known to break this runtime barrier. We confirm the theoretical
results by simulations. Finally, we show that the approach is not overfitted to
Leadingones by testing it empirically on other benchmarks and showing that it
is also competitive in other settings. We believe that our findings justify
further systematic investigations of the DiPEC paradigm.

### 2. [Adaptive Estimation of the Number of Algorithm Runs in Stochastic Optimization](http://arxiv.org/pdf/2507.01629v1)

Authors: Tome Eftimov, Peter Korošec

Determining the number of algorithm runs is a critical aspect of experimental
design, as it directly influences the experiment's duration and the reliability
of its outcomes. This paper introduces an empirical approach to estimating the
required number of runs per problem instance for accurate estimation of the
performance of the continuous single-objective stochastic optimization
algorithm. The method leverages probability theory, incorporating a robustness
check to identify significant imbalances in the data distribution relative to
the mean, and dynamically adjusts the number of runs during execution as an
online approach. The proposed methodology was extensively tested across two
algorithm portfolios (104 Differential Evolution configurations and the
Nevergrad portfolio) and the COCO benchmark suite, totaling 5748000 runs. The
results demonstrate 82% - 95% accuracy in estimations across different
algorithms, allowing a reduction of approximately 50% in the number of runs
without compromising optimization outcomes. This online calculation of required
runs not only improves benchmarking efficiency, but also contributes to energy
reduction, fostering a more environmentally sustainable computing ecosystem.

### 3. [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](http://arxiv.org/pdf/2507.01638v1)

Authors: Ana Nikolikj, Gabriela Ochoa, Tome Eftimov

We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

### 4. [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](http://arxiv.org/pdf/2507.01668v1)

Authors: Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov

The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

### Networking and Internet Architecture

### 1. [Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions](http://arxiv.org/pdf/2507.01773v1)

Authors: Bo Yang, Ruihuai Liang, Weixin Li, Han Wang, Xuelin Cao, Zhiwen Yu, Samson Lasaulce, Mérouane Debbah, Mohamed-Slim Alouini, H. Vincent Poor, Chau Yuen

While interest in the application of generative AI (GenAI) in network
optimization has surged in recent years, its rapid progress has often
overshadowed critical limitations intrinsic to generative models that remain
insufficiently examined in existing literature. This survey provides a
comprehensive review and critical analysis of GenAI in network optimization. We
focus on the two dominant paradigms of GenAI including generative diffusion
models (GDMs) and large pre-trained models (LPTMs), and organize our discussion
around a categorization we introduce, dividing network optimization problems
into two primary formulations: one-shot optimization and Markov decision
process (MDP). We first trace key works, including foundational contributions
from the AI community, and categorize current efforts in network optimization.
We also review frontier applications of GDMs and LPTMs in other networking
tasks, providing additional context. Furthermore, we present theoretical
generalization bounds for GDMs in both one-shot and MDP settings, offering
insights into the fundamental factors affecting model performance. Most
importantly, we reflect on the overestimated perception of GenAI's general
capabilities and caution against the all-in-one illusion it may convey. We
highlight critical limitations, including difficulties in constraint
satisfying, limited concept understanding, and the inherent probabilistic
nature of outputs. We also propose key future directions, such as bridging the
gap between generation and optimization. Although they are increasingly
integrated in implementations, they differ fundamentally in both objectives and
underlying mechanisms, necessitating a deeper understanding of their
theoretical connections. Ultimately, this survey aims to provide a structured
overview and a deeper insight into the strengths, limitations, and potential of
GenAI in network optimization.

### 2. [Fluid Aerial Networks: UAV Rotation for Inter-Cell Interference Mitigation](http://arxiv.org/pdf/2507.01289v1)

Authors: Enzhi Zhou, Yue Xiao, Ziyue Liu, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, George K. Karagiannidis

With the rapid development of aerial infrastructure, unmanned aerial vehicles
(UAVs) that function as aerial base stations (ABSs) extend terrestrial network
services into the sky, enabling on-demand connectivity and enhancing emergency
communication capabilities in cellular networks by leveraging the flexibility
and mobility of UAVs. In such a UAV-assisted network, this paper investigates
position-based beamforming between ABSs and ground users (GUs). To mitigate
inter-cell interference, we propose a novel fluid aerial network that leverages
ABS rotation to increase multi-cell capacity and overall network efficiency.
Specifically, considering the line-of-sight channel model, the spatial
beamforming weights are determined by the orientation angles of the GUs. In
this direction, we examine the beamforming gain of a two-dimensional
multiple-input multiple-output (MIMO) array at various ground positions,
revealing that ABS rotation significantly affects multi-user channel
correlation and inter-cell interference. Based on these findings, we propose an
alternative low-complexity algorithm to design the optimal rotation angle for
ABSs, aiming to reduce inter-cell interference and thus maximize the sum rate
of multi-cell systems. In simulations, exhaustive search serves as a benchmark
to validate the optimization performance of the proposed sequential ABS
rotation scheme. Moreover, simulation results demonstrate that, in
interference-limited regions, the proposed ABS rotation paradigm can
significantly reduce inter-cell interference in terrestrial networks and
improve the multi-cell sum rate by approximately 10\% compared to
fixed-direction ABSs without rotation.

### 3. [MmBack: Clock-free Multi-Sensor Backscatter with Synchronous Acquisition and Multiplexing](http://arxiv.org/pdf/2507.01360v1)

Authors: Yijie Li, Weichong Ling, Taiting Lu, Yi-Chao Chen, Vaishnavi Ranganathan, Lili Qiu, Jingxian Wang

Backscatter tags provide a low-power solution for sensor applications, yet
many real-world scenarios require multiple sensors-often of different types-for
complex sensing tasks. However, existing designs support only a single sensor
per tag, increasing spatial overhead. State-of-the-art approaches to
multiplexing multiple sensor streams on a single tag rely on onboard clocks or
multiple modulation chains, which add cost, enlarge form factor, and remain
prone to timing drift-disrupting synchronization across sensors.
  We present mmBack, a low-power, clock-free backscatter tag that enables
synchronous multi-sensor data acquisition and multiplexing over a single
modulation chain. mmBack synchronizes sensor inputs in parallel using a shared
reference signal extracted from ambient RF excitation, eliminating the need for
an onboard timing source. To efficiently multiplex sensor data, mmBack designs
a voltage-division scheme to multiplex multiple sensor inputs as backscatter
frequency shifts through a single oscillator and RF switch. At the receiver,
mmBack develops a frequency tracking algorithm and a finite-state machine for
accurate demultiplexing. mmBack's ASIC design consumes 25.56uW, while its
prototype supports 5 concurrent sensor streams with bandwidths of up to 5kHz
and 3 concurrent sensor streams with bandwidth of up to 18kHz. Evaluation shows
that mmBack achieves an average SNR surpassing 15dB in signal reconstruction.

### 4. [Multi-User Generative Semantic Communication with Intent-Aware Semantic-Splitting Multiple Access](http://arxiv.org/pdf/2507.01333v1)

Authors: Jiayi Lu, Wanting Yang, Zehui Xiong, Rahim Tafazolli, Tony Q. S. Quek, Mérouane Debbah, Dong In Kim

With the booming development of generative artificial intelligence (GAI),
semantic communication (SemCom) has emerged as a new paradigm for reliable and
efficient communication. This paper considers a multi-user downlink SemCom
system, using vehicular networks as the representative scenario for multi-user
content dissemination. To address diverse yet overlapping user demands, we
propose a multi-user Generative SemCom-enhanced intent-aware semantic-splitting
multiple access (SS-MGSC) framework. In the framework, we construct an
intent-aware shared knowledge base (SKB) that incorporates prior knowledge of
semantic information (SI) and user-specific preferences. Then, we designate the
common SI as a one-hot semantic map that is broadcast to all users, while the
private SI is delivered as personalized text for each user. On the receiver
side, a diffusion model enhanced with ControlNet is adopted to generate
high-quality personalized images. To capture both semantic relevance and
perceptual similarity, we design a novel semantic efficiency score (SES) metric
as the optimization objective. Building on this, we formulate a joint
optimization problem for multi-user semantic extraction and beamforming, solved
using a reinforcement learning-based algorithm due to its robustness in
high-dimensional settings. Simulation results demonstrate the effectiveness of
the proposed scheme.

### 5. [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](http://arxiv.org/pdf/2507.01571v1)

Authors: Koen T. W. Teuwen, Sam Baggen, Emmanuele Zambon, Luca Allodi

Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

### Robotics

### 1. [TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control](http://arxiv.org/pdf/2507.01424v1)

Authors: Zhenyang Liu, Yongchong Gu, Sixiao Zheng, Xiangyang Xue, Yanwei Fu

Recent advancements in vision-language models (VLMs) for common-sense
reasoning have led to the development of vision-language-action (VLA) models,
enabling robots to perform generalized manipulation. Although existing
autoregressive VLA methods design a specific architecture like dual-system to
leverage large-scale pretrained knowledge, they tend to capture static
information, often neglecting the dynamic aspects vital for embodied tasks. To
this end, we propose TriVLA, a unified Vision-Language-Action model with a
triple-system architecture for general robot control. The vision-language
module (System 2) interprets the environment through vision and language
instructions. The dynamics perception module (System 3) inherently produces
visual representations that encompass both current static information and
predicted future dynamics, thereby providing valuable guidance for policy
learning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained
video foundation model on robot datasets along with internet human manipulation
data. The subsequent policy learning module (System 1) generates fluid motor
actions in real time. Experimental evaluation demonstrates that TriVLA operates
at approximately 36 Hz and surpasses state-of-the-art imitation learning
baselines on standard simulation benchmarks as well as challenging real-world
manipulation tasks.

### 2. [Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems](http://arxiv.org/pdf/2507.01550v1)

Authors: Johannes Kohl, Georg Muck, Georg Jäger, Sebastian Zug

With the rapid development of more complex robots, Fault Detection and
Diagnosis (FDD) becomes increasingly harder. Especially the need for
predetermined models and historic data is problematic because they do not
encompass the dynamic and fast-changing nature of such systems. To this end, we
propose a concept that actively generates a dynamic system model at runtime and
utilizes it to locate root causes. The goal is to be applicable to all kinds of
robotic systems that share a similar software design. Additionally, it should
exhibit minimal overhead and enhance independence from expert attention.

### 3. [Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design](http://arxiv.org/pdf/2507.01561v1)

Authors: Huijiang Wang, Holger Kunz, Timon Adler, Fumiya Iida

Shape-morphing robots have shown benefits in industrial grasping. We propose
form-flexible grippers for adaptive grasping. The design is based on the hybrid
jamming and suction mechanism, which deforms to handle objects that vary
significantly in size from the aperture, including both larger and smaller
parts. Compared with traditional grippers, the gripper achieves self-closing to
form an airtight seal. Under a vacuum, a wide range of grasping is realized
through the passive morphing mechanism at the interface that harmonizes
pressure and flow rate. This hybrid gripper showcases the capability to
securely grasp an egg, as small as 54.5% of its aperture, while achieving a
maximum load-to-mass ratio of 94.3.

### 4. [Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane](http://arxiv.org/pdf/2507.01705v1)

Authors: Marc-Philip Ecker, Bernhard Bischof, Minh Nhat Vu, Christoph Fröhlich, Tobias Glück, Wolfgang Kemmetmüller

Collision-free motion planning in complex outdoor environments relies heavily
on perceiving the surroundings through exteroceptive sensors. A widely used
approach represents the environment as a voxelized Euclidean distance field,
where robots are typically approximated by spheres. However, for large-scale
manipulators such as forestry cranes, which feature long and slender links,
this conventional spherical approximation becomes inefficient and inaccurate.
This work presents a novel collision detection algorithm specifically designed
to exploit the elongated structure of such manipulators, significantly
enhancing the computational efficiency of motion planning algorithms. Unlike
traditional sphere decomposition methods, our approach not only improves
computational efficiency but also naturally eliminates the need to fine-tune
the approximation accuracy as an additional parameter. We validate the
algorithm's effectiveness using real-world LiDAR data from a forestry crane
application, as well as simulated environment data.

### 5. [SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space](http://arxiv.org/pdf/2507.01723v1)

Authors: Xupeng Zhu, Fan Wang, Robin Walters, Jane Shi

Diffusion Policies are effective at learning closed-loop manipulation
policies from human demonstrations but generalize poorly to novel arrangements
of objects in 3D space, hurting real-world performance. To address this issue,
we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion
policy that adapts trajectories according to 3D transformations of the scene.
Such equivariance is achieved by embedding the states, actions, and the
denoising process in spherical Fourier space. Additionally, we employ novel
spherical FiLM layers to condition the action denoising process equivariantly
on the scene embeddings. Lastly, we propose a spherical denoising temporal
U-net that achieves spatiotemporal equivariance with computational efficiency.
In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization
across transformed 3D scenes. SDP demonstrates a large performance improvement
over strong baselines in 20 simulation tasks and 5 physical robot tasks
including single-arm and bi-manual embodiments. Code is available at
https://github.com/amazon-science/Spherical_Diffusion_Policy.

### 6. [Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws](http://arxiv.org/pdf/2507.01753v1)

Authors: Yash Kulkarni, Susheela Sharma, Omid Rezayof, Siddhartha Kapuria, Jordan P. Amadio, Mohsen Khadem, Maryam Tilton, Farshid Alambeigi

To address the screw loosening and pullout limitations of rigid pedicle
screws in spinal fixation procedures, and to leverage our recently developed
Concentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw
(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal
Fixation (AB-SF). In this concept, two connecting J-shape tunnels are first
drilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are
passed through this tunnel and bone cement is then injected through the
cannulated region of the FPS to form an augmented bridge between two pedicles
and reinforce strength of the fixated spine. To experimentally analyze and
study the feasibility of AB-SF technique, we first used our robotic system
(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation
scenarios in which two J-shape tunnels, forming a bridge, were drilled at
different depth of a vertebral phantom. Next, we implanted two FPSs within the
drilled tunnels and then successfully simulated the bone cement augmentation
process.

### 7. [S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures](http://arxiv.org/pdf/2507.01779v1)

Authors: Daniyal Maroufi, Xinyuan Huang, Yash Kulkarni, Omid Rezayof, Susheela Sharma, Vaibhav Goggela, Jordan P. Amadio, Mohsen Khadem, Farshid Alambeigi

In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling
Framework for Robotic Spinal Fixation Procedures. S3D is designed to enable
realistic steerable drilling while accounting for the anatomical constraints
associated with vertebral access in spinal fixation (SF) procedures. To achieve
this, we first enhanced our previously designed concentric tube Steerable
Drilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral
levels of the spinal column. Additionally, we propose a four-Phase calibration,
registration, and navigation procedure to perform realistic SF procedures on a
spine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom
robotic manipulator. The functionality of this framework is validated through
planar and out-of-plane steerable drilling experiments in vertebral phantoms.

### 8. [Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures](http://arxiv.org/pdf/2507.01811v1)

Authors: Yash Kulkarni, Susheela Sharma, Sarah Go, Jordan P. Amadio, Mohsen Khadem, Farshid Alambeigi

Current pelvic fixation techniques rely on rigid drilling tools, which
inherently constrain the placement of rigid medical screws in the complex
anatomy of pelvis. These constraints prevent medical screws from following
anatomically optimal pathways and force clinicians to fixate screws in linear
trajectories. This suboptimal approach, combined with the unnatural placement
of the excessively long screws, lead to complications such as screw
misplacement, extended surgery times, and increased radiation exposure due to
repeated X-ray images taken ensure to safety of procedure. To address these
challenges, in this paper, we present the design and development of a unique 4
degree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic
CT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling
trajectories that follow the natural curvatures of the pelvic anatomy. The
performance of the pelvic CT-SDR was thoroughly evaluated through several
S-shape drilling experiments in simulated bone phantoms.

### 9. [MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics](http://arxiv.org/pdf/2507.01843v1)

Authors: Dmytro Kuzmenko, Nadiya Shvai

Mixture-of-Experts (MoE) approaches have recently gained traction in robotics
applications due to their ability to dynamically allocate computational
resources and specialize sub-networks for distinct tasks or environmental
contexts, enabling more efficient decision-making. Such systems often comprise
sparsely activated experts combined under a single monolithic architecture and
require a well-configured internal routing mechanism, which does not allow for
selective low-level expert and router customization and requires additional
training. We propose MoIRA, an architecture-agnostic modular MoE framework
designed to coordinate existing experts with an external text-based router.
MoIRA incorporates two zero-shot routing options: embedding-based similarity
and prompt-driven language model inference. In our experiments, we choose large
Vision-Language-Action models, gr00t-N1 and $\pi_0$, as the underlying experts,
and train low-rank adapters for low-overhead inference. We evaluate MoIRA on
various GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it
consistently outperforms generalist models and competes with other MoE
pipelines. Additionally, we analyse the robustness of the proposed approach to
the variations of the instructions. While relying solely on textual
descriptions of tasks and experts, MoIRA demonstrates the practical viability
of modular deployment with precise, low-effort routing and provides an
alternative, scalable foundation for future multi-expert robotic systems.

### 10. [TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types](http://arxiv.org/pdf/2507.01857v1)

Authors: Yuhao Lin, Yi-Lin Wei, Haoran Liao, Mu Lin, Chengyi Xing, Hao Li, Dandan Zhang, Mark Cutkosky, Wei-Shi Zheng

Dexterous teleoperation plays a crucial role in robotic manipulation for
real-world data collection and remote robot control. Previous dexterous
teleoperation mostly relies on hand retargeting to closely mimic human hand
postures. However, these approaches may fail to fully leverage the inherent
dexterity of dexterous hands, which can execute unique actions through their
structural advantages compared to human hands. To address this limitation, we
propose TypeTele, a type-guided dexterous teleoperation system, which enables
dexterous hands to perform actions that are not constrained by human motion
patterns. This is achieved by introducing dexterous manipulation types into the
teleoperation system, allowing operators to employ appropriate types to
complete specific tasks. To support this system, we build an extensible
dexterous manipulation type library to cover comprehensive dexterous postures
used in manipulation tasks. During teleoperation, we employ a MLLM
(Multi-modality Large Language Model)-assisted type retrieval module to
identify the most suitable manipulation type based on the specific task and
operator commands. Extensive experiments of real-world teleoperation and
imitation learning demonstrate that the incorporation of manipulation types
significantly takes full advantage of the dexterous robot's ability to perform
diverse and complex tasks with higher success rates.

### Software Engineering

### 1. [Context-Aware Code Wiring Recommendation with LLM-based Agent](http://arxiv.org/pdf/2507.01315v1)

Authors: Taiming Wang, Yanjie Jiang, Chunhao Dong, Yuxia Zhang, Hui Liu

Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

### 2. [Combining Type Inference and Automated Unit Test Generation for Python](http://arxiv.org/pdf/2507.01477v1)

Authors: Lukas Krodinger, Stephan Lukasczyk, Gordon Fraser

Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

### 3. [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](http://arxiv.org/pdf/2507.01628v1)

Authors: Zilong He, Pengfei Chen, Hongyu Zhang, Xiaoyun Li, Guangba Yu, Hongyang Chen, Zibin Zheng

Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

### 4. [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](http://arxiv.org/pdf/2507.01827v1)

Authors: Haichuan Hu, Congqing He, Hao Zhang, Xiaochen Xie, Quanjun Zhang

Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

### 5. [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](http://arxiv.org/pdf/2507.01457v1)

Authors: Federico Nicolas Peccia, Frederik Haxel, Oliver Bringmann

RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

### Social and Information Networks

### 1. [Mapping the interaction between science and misinformation in COVID-19 tweets](http://arxiv.org/pdf/2507.01481v1)

Authors: Lucila G. Alvarez-Zuzek, Juan P. Bascur, Anna Bertani, Riccardo Gallotti, Vincent A. Traag

During the COVID-19 pandemic, scientific understanding related to the topic
evolved rapidly. Along with scientific information being discussed widely, a
large circulation of false information, labelled an infodemic by the WHO,
emerged. Here, we study the interaction between misinformation and science on
Twitter (now X) during the COVID-19 pandemic. We built a comprehensive database
of $\sim$407M COVID-19 related tweets and classified the reliability of URLs in
the tweets based on Media Bias/Fact Check. In addition, we use Altmetric data
to see whether a tweet refers to a scientific publication. We find that many
users find that many users share both scientific and unreliable content; out of
the $\sim$1.2M users who share science, $45\%$ also share unreliable content.
Publications that are more frequently shared by users who also share unreliable
content are more likely to be preprints, slightly more often retracted, have
fewer citations, and are published in lower-impact journals on average. Our
findings suggest that misinformation is not related to a ``deficit'' of
science. In addition, our findings raise some critical questions about certain
open science practices and their potential for misuse. Given the fundamental
opposition between science and misinformation, our findings highlight the
necessity for proactive scientific engagement on social media platforms to
counter false narratives during global crises.

### 2. [Modeling individual attention dynamics on online social media](http://arxiv.org/pdf/2507.01511v1)

Authors: Jaume Ojer, Filippo Radicchi, Santo Fortunato, Michele Starnini, Romualdo Pastor-Satorras

In the attention economy, understanding how individuals manage limited
attention is critical. We introduce a simple model describing the decay of a
user's engagement when facing multiple inputs. We analytically show that
individual attention decay is determined by the overall duration of
interactions, not their number or user activity. Our model is validated using
data from Reddit's Change My View subreddit, where the user's attention
dynamics is explicitly traceable. Despite its simplicity, our model offers a
crucial microscopic perspective complementing macroscopic studies.

### 3. [Reduced Efficiency in the Right Fronto-Parietal Attentional Network During Distractor Suppression in Mild Cognitive Impairment](http://arxiv.org/pdf/2507.01433v1)

Authors: Jatupong Oboun, Piyanon Charoenpoonpanich, Anna Raksapatcharawong, Chaipat Chunharas, Itthi Chatnuntawech, Chainarong Amornbunchornvej, Sirawaj Itthipuripat

Mild Cognitive Impairment (MCI) is a critical transitional stage between
normal cognitive aging and dementia, making its early detection essential. This
study investigates the neural mechanisms of distractor suppression in MCI
patients using EEG and behavioral data during an attention-cueing Eriksen
flanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks
with congruent and incongruent stimuli of varying saliency levels. During these
tasks, EEG data were analyzed for alpha band coherence's functional
connectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and
Hit Rate (HR) were also collected.
  Our findings reveal significant interactions between congruency, saliency,
and cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted
in higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster
RTs (p < 0.0001, MVT), and higher HRs (p < 0.0001, MVT) compared to incongruent
conditions. HCs also showed increased GE in salient conditions for incongruent
trials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions
with shorter RTs and higher HRs (both p < 0.0001, MVT) compared to incongruent
conditions but showed reduced adaptability in GE, with no significant GE
differences between conditions.
  These results highlight the potential of alpha band coherence and GE as early
markers for cognitive impairment. By integrating GE, RT, and HR, this study
provides insights into the interplay between neural efficiency, processing
speed, and task accuracy. This approach offers valuable insights into cognitive
load management and interference effects, indicating benefits for interventions
aimed at improving attentional control and processing speed in MCIs.

### Systems and Control

### 1. [Synchronising DER inverters to weak grid using Kalman filter and LQR current controller](http://arxiv.org/pdf/2507.01300v1)

Authors: Phuoc Sang Nguyen, Ghavameddin Nourbakhsh, Gerard Ledwich

Grid-following (GFL) inverters are commonly used for integrating renewable
energy sources into power grids. However, the dynamic performance of GFL models
can be significantly impacted by the Phase-Locked Loop (PLL) in a weak grid,
leading to instability due to inaccuracies in grid source phase angle
estimation. The proposed method in this manuscript replaces the PLL with an
Advanced Angle Estimation based Kalman Filter including a Linear Quadratic
Regulator (LQR) controller of the GFL. This method is robust in incorporating
grid impedance terms as part of state space models in the Kalman Filter
approach to estimate instantaneous phase angle using {\alpha}-\b{eta}
Synchronous Reference Frame equations. The stability performance of the
proposed approach is validated through eigenvalue analysis in a two-source
case. Additionally, an LQR controller is employed to regulate capacitor
voltage, inverter current, and the current at the Point of Common Coupling
(PCC). The proposed controller surpasses existing approaches in terms of
accuracy and distortion reduction under abrupt grid impedance increases.
Moreover, drop compensation is integrated into the Kalman Filter to enhance
robustness of the inverter against external oscillation disturbances from a
synchronous machine connected to the GFL via the PCC. The results in this paper
demonstrate substantial improvement in oscillation damping across a range of
frequencies compared with published research works.

### 2. [Robust Input Shaping Control for Flexible Structures Based on Unscented Kalman Filter](http://arxiv.org/pdf/2507.01460v1)

Authors: Weiyi Yang, Yu Yuan, Mingsheng Shang

With the rapid development of industrial automation and smart manufacturing,
the control of flexible structures and underactuated systems has become a
critical research focus. Residual vibrations in these systems not only degrade
operational efficiency but also pose risks to structural integrity and
longevity. Traditional input shaping techniques, while effective, often suffer
from performance degradation due to parameter inaccuracies and environmental
disturbances. To address these challenges, this paper introduces an innovative
unscented Kalman filter-based zero vibration derivative input shaping (UZS)
method. The proposed approach combines two key innovations: 1) a data-driven
Unscented Kalman Filterfor real-time system parameter identification, and 2) a
zero-vibration derivative (ZVD) input shaper for robust vibration suppression.
To validate the effectiveness of UZS, we conducted extensive experiments on a
vertical flexible beam platform, and the results demonstrate significant
improvements over state-of-the-art methods. Additionally, we have made the
experimental datasets publicly available to facilitate further research. The
findings highlight UZS's potential for practical applications in industrial
automation, robotics, and precision engineering.

### 3. [Frequency Domain Design of a Reset-Based Filter: An Add-On Nonlinear Filter for Industrial Motion Control](http://arxiv.org/pdf/2507.01491v1)

Authors: S. Ali Hosseini, Fabian R. Quinten, Luke F. van Eijk, Dragan Kostic, S. Hassan HosseinNia

This study introduces a modified version of the Constant-in-Gain,
Lead-in-Phase (CgLp) filter, which incorporates a feedthrough term in the
First-Order Reset Element (FORE) to reduce the undesirable nonlinearities and
achieve an almost constant gain across all frequencies. A backward calculation
approach is proposed to derive the additional parameter introduced by the
feedthrough term, enabling designers to easily tune the filter to generate the
required phase. The paper also presents an add-on filter structure that can
enhance the performance of an existing LTI controller without altering its
robustness margins. A sensitivity improvement indicator is proposed to guide
the tuning process, enabling designers to visualize the improvements in
closed-loop performance. The proposed methodology is demonstrated through a
case study of an industrial wire bonder machine, showcasing its effectiveness
in addressing low-frequency vibrations and improving overall control
performance.

### 4. [Vision-Aided ISAC in Low-Altitude Economy Networks via De-Diffused Visual Priors](http://arxiv.org/pdf/2507.01574v1)

Authors: Yulan Gao, Ziqiang Ye, Zhonghao Lyu, Ming Xiao, Yue Xiao, Ping Yang, Agata Manolova

Emerging low-altitude economy networks (LAENets) require agile and
privacy-preserving resource control under dynamic agent mobility and limited
infrastructure support. To meet these challenges, we propose a vision-aided
integrated sensing and communication (ISAC) framework for UAV-assisted access
systems, where onboard masked De-Diffusion models extract compact semantic
tokens, including agent type, activity class, and heading orientation, while
explicitly suppressing sensitive visual content. These tokens are fused with
mmWave radar measurements to construct a semantic risk heatmap reflecting
motion density, occlusion, and scene complexity, which guides access technology
selection and resource scheduling. We formulate a multi-objective optimization
problem to jointly maximize weighted energy and perception efficiency via radio
access technology (RAT) assignment, power control, and beamforming, subject to
agent-specific QoS constraints. To solve this, we develop De-Diffusion-driven
vision-aided risk-aware resource optimization algorithm DeDiff-VARARO, a novel
two-stage cross-modal control algorithm: the first stage reconstructs visual
scenes from tokens via De-Diffusion model for semantic parsing, while the
second stage employs a deep deterministic policy gradient (DDPG)-based policy
to adapt RAT selection, power control, and beam assignment based on fused
radar-visual states. Simulation results show that DeDiff-VARARO consistently
outperforms baselines in reward convergence, link robustness, and semantic
fidelity, achieving within $4\%$ of the performance of a raw-image upper bound
while preserving user privacy and scalability in dense environments.

### 5. [Auto-optimization of Energy Generation for Wave Energy Converters with Active Learning](http://arxiv.org/pdf/2507.01727v1)

Authors: Siyang Tang, Wen-Hua Chen, Cunjia Liu

This paper presents an auto-optimization control framework for wave energy
converters (WECs) to maximize energy generation under unknown and changing
ocean conditions. The proposed control framework consists of two levels. The
high-level controller operating at a longer time scale aims to maximize the
average energy generation over several wave periods. The generated Power
Take-Off (PTO) profile as the reference for the low-level physical system to
follow. The new auto-optimization process leverages the parameterization of the
non-stationary operation condition in WECs, establishing the relationship
between the average energy generation and the key design parameters of the PTO
force subject to the unknown wave parameters. The high-level controller is
designed based on the concept of Dual Control for Exploration and Exploitation
(DCEE) to quickly learn the unknown wave parameters by actively probing the
ocean condition, while generating the optimal PTO profile. During this process,
the uncertainty of the estimated wave condition is quantified and embedded in
the optimization cost function to enable active learning. Simulation results
under unknown regular and irregular waves demonstrate the effectiveness and
robustness of this novel auto-optimization WEC systems with active learning,
outperforming model predictive control, extremum seeking and classic Bang-Bang
control approaches.

### 6. [An Error Bound for Aggregation in Approximate Dynamic Programming](http://arxiv.org/pdf/2507.01324v1)

Authors: Yuchao Li, Dimitri Bertsekas

We consider a general aggregation framework for discounted finite-state
infinite horizon dynamic programming (DP) problems. It defines an aggregate
problem whose optimal cost function can be obtained off-line by exact DP and
then used as a terminal cost approximation for an on-line reinforcement
learning (RL) scheme. We derive a bound on the error between the optimal cost
functions of the aggregate problem and the original problem. This bound was
first derived by Tsitsiklis and van Roy [TvR96] for the special case of hard
aggregation. Our bound is similar but applies far more broadly, including to
soft aggregation and feature-based aggregation schemes.

### 7. [Cooperative Target Capture in 3D Engagements over Switched Dynamic Graphs](http://arxiv.org/pdf/2507.01350v1)

Authors: Abhinav Sinha, Shashi Ranjan Kumar

This paper presents a leaderless cooperative guidance strategy for
simultaneous time-constrained interception of a stationary target when the
interceptors exchange information over switched dynamic graphs. We specifically
focus on scenarios when the interceptors lack radial acceleration capabilities,
relying solely on their lateral acceleration components. This consideration
aligns with their inherent kinematic turn constraints. The proposed strategy
explicitly addresses the complexities of coupled 3D engagements, thereby
mitigating performance degradation that typically arises when the pitch and yaw
channels are decoupled into two separate, mutually orthogonal planar
engagements. Moreover, our formulation incorporates modeling uncertainties
associated with the time-to-go estimation into the derivation of cooperative
guidance commands to ensure robustness against inaccuracies in dynamic
engagement scenarios. To optimize control efficiency, we analytically derive
the lateral acceleration components in the orthogonal pitch and yaw channels by
solving an instantaneous optimization problem, subject to an affine constraint.
We show that the proposed cooperative guidance commands guarantee consensus in
time-to-go values within a predefined time, which can be prescribed as a design
parameter, regardless of the interceptors' initial configurations. We provide
simulations to attest to the efficacy of the proposed method.

### 8. [Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints](http://arxiv.org/pdf/2507.01426v1)

Authors: Ratnangshu Das, Pushpak Jagtap

In this paper, we present a novel funnel-based tracking control algorithm for
robotic systems with unknown dynamics and prescribed input constraints. The
Euler-Lagrange formulation, a common modeling approach for robotic systems, has
been adopted in this study to address the trade-off between performance and
actuator safety. We establish feasibility conditions that ensure tracking
errors evolve within predefined funnel bounds while maintaining bounded control
efforts, a crucial consideration for robots with limited actuation
capabilities. We propose two approximation-free control strategies for
scenarios where these conditions are violated: one actively corrects the error,
and the other stops further deviation. Finally, we demonstrate the robust
performance and safety of the approach through simulations and experimental
validations. This work represents a significant advancement in funnel-based
control, enhancing its applicability to real-world robotics systems with input
constraints.

### 9. [Multi-Revolution Low-Thrust Trajectory Optimization With Very Sparse Mesh Pseudospectral Method](http://arxiv.org/pdf/2507.01450v1)

Authors: Yilin Zou, Fanghua Jiang

Multi-revolution low-thrust trajectory optimization problems are important
and challenging in space mission design. In this paper, an efficient, accurate,
and widely applicable pseudospectral method is proposed to solve
multi-revolution low-thrust trajectory optimization problems with various
objective functions and perturbations. The method is based on the Sundman
transformation and pseudospectral method, together with a sparse mesh that is
monotonic, near-uniformly spaced, and uniformly scattered on the unit circle.
Two methods are proposed to construct the mesh: a deterministic method based on
rotation mapping; a stochastic method utilizing autocorrelated random
sequences. Core mechanisms ensuring the correctness of the method are analyzed,
including the dual roles of mesh points as both integration points in the
temporal domain and sampling points in the angular domain, the slow dynamics of
the system excluding the fast angle variable, and the nearly commutative vector
fields generated by applying different control inputs. The method is
demonstrated through a multi-revolution low-thrust orbital rendezvous problem.
Results show that the proposed method achieves high accuracy with only a few
seconds of computational time for challenging problems.

### 10. [Chargax: A JAX Accelerated EV Charging Simulator](http://arxiv.org/pdf/2507.01522v1)

Authors: Koen Ponse, Jan Felix Kleuker, Aske Plaat, Thomas Moerland

Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

### Machine Learning (Statistics Category)

### 1. [Semi-supervised learning for linear extremile regression](http://arxiv.org/pdf/2507.01314v1)

Authors: Rong Jiang, Keming Yu, Jiangfeng Wang

Extremile regression, as a least squares analog of quantile regression, is
potentially useful tool for modeling and understanding the extreme tails of a
distribution. However, existing extremile regression methods, as nonparametric
approaches, may face challenges in high-dimensional settings due to data
sparsity, computational inefficiency, and the risk of overfitting. While linear
regression serves as the foundation for many other statistical and machine
learning models due to its simplicity, interpretability, and relatively easy
implementation, particularly in high-dimensional settings, this paper
introduces a novel definition of linear extremile regression along with an
accompanying estimation methodology. The regression coefficient estimators of
this method achieve $\sqrt{n}$-consistency, which nonparametric extremile
regression may not provide. In particular, while semi-supervised learning can
leverage unlabeled data to make more accurate predictions and avoid overfitting
to small labeled datasets in high-dimensional spaces, we propose a
semi-supervised learning approach to enhance estimation efficiency, even when
the specified linear extremile regression model may be misspecified. Both
simulation studies and real data analyses demonstrate the finite-sample
performance of our proposed methods.

### 2. [Nonparametric learning of heterogeneous graphical model on network-linked data](http://arxiv.org/pdf/2507.01473v1)

Authors: Yuwen Wang, Changyu Liu, Xin He, Junhui Wang

Graphical models have been popularly used for capturing conditional
independence structure in multivariate data, which are often built upon
independent and identically distributed observations, limiting their
applicability to complex datasets such as network-linked data. This paper
proposes a nonparametric graphical model that addresses these limitations by
accommodating heterogeneous graph structures without imposing any specific
distributional assumptions. The proposed estimation method effectively
integrates network embedding with nonparametric graphical model estimation. It
further transforms the graph learning task into solving a finite-dimensional
linear equation system by leveraging the properties of vector-valued
reproducing kernel Hilbert space. Moreover, theoretical guarantees are
established for the proposed method in terms of the estimation consistency and
exact recovery of the heterogeneous graph structures. Its effectiveness is also
demonstrated through a variety of simulated examples and a real application to
the statistician coauthorship dataset.

### 3. [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](http://arxiv.org/pdf/2507.01613v1)

Authors: Shirong Xu, Jingnan Zhang, Junhui Wang

Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

### 4. [Out-of-Distribution Detection Methods Answer the Wrong Questions](http://arxiv.org/pdf/2507.01831v1)

Authors: Yucen Lily Li, Daohan Lu, Polina Kirichenko, Shikai Qiu, Tim G. J. Rudner, C. Bayan Bruss, Andrew Gordon Wilson

To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

### 5. [Targeted tuning of random forests for quantile estimation and prediction intervals](http://arxiv.org/pdf/2507.01430v1)

Authors: Matthew Berkowitz, Rachel MacKay Altman, Thomas M. Loughin

We present a novel tuning procedure for random forests (RFs) that improves
the accuracy of estimated quantiles and produces valid, relatively narrow
prediction intervals. While RFs are typically used to estimate mean responses
(conditional on covariates), they can also be used to estimate quantiles by
estimating the full distribution of the response. However, standard approaches
for building RFs often result in excessively biased quantile estimates. To
reduce this bias, our proposed tuning procedure minimizes "quantile coverage
loss" (QCL), which we define as the estimated bias of the marginal quantile
coverage probability estimate based on the out-of-bag sample. We adapt QCL
tuning to handle censored data and demonstrate its use with random survival
forests. We show that QCL tuning results in quantile estimates with more
accurate coverage probabilities than those achieved using default parameter
values or traditional tuning (using MSPE for uncensored data and C-index for
censored data), while also reducing the estimated MSE of these coverage
probabilities. We discuss how the superior performance of QCL tuning is linked
to its alignment with the estimation goal. Finally, we explore the validity and
width of prediction intervals created using this method.

### 6. [A generative modeling / Physics-Informed Neural Network approach to random differential equations](http://arxiv.org/pdf/2507.01687v1)

Authors: Georgios Arampatzis, Stylianos Katsarakis, Charalambos Makridakis

The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

### 7. [Entropic optimal transport beyond product reference couplings: the Gaussian case on Euclidean space](http://arxiv.org/pdf/2507.01709v1)

Authors: Paul Freulon, Nikitas Georgakis, Victor Panaretos

The optimal transport problem with squared Euclidean cost consists in finding
a coupling between two input measures that maximizes correlation. Consequently,
the optimal coupling is often singular with respect to Lebesgue measure.
Regularizing the optimal transport problem with an entropy term yields an
approximation called entropic optimal transport. Entropic penalties steer the
induced coupling toward a reference measure with desired properties. For
instance, when seeking a diffuse coupling, the most popular reference measures
are the Lebesgue measure and the product of the two input measures. In this
work, we study the case where the reference coupling is not necessarily assumed
to be a product. We focus on the Gaussian case as a motivating paradigm, and
provide a reduction of this more general optimal transport criterion to a
matrix optimization problem. This reduction enables us to provide a complete
description of the solution, both in terms of the primal variable and the dual
variables. We argue that flexibility in terms of the reference measure can be
important in statistical contexts, for instance when one has prior information,
when there is uncertainty regarding the measures to be coupled, or to reduce
bias when the entropic problem is used to estimate the un-regularized transport
problem. In particular, we show in numerical examples that choosing a suitable
reference plan allows to reduce the bias caused by the entropic penalty.

### 8. [Generative flow-based warm start of the variational quantum eigensolver](http://arxiv.org/pdf/2507.01726v1)

Authors: Hang Zou, Martin Rahm, Anton Frisk Kockum, Simon Olsson

Hybrid quantum-classical algorithms like the variational quantum eigensolver
(VQE) show promise for quantum simulations on near-term quantum devices, but
are often limited by complex objective functions and expensive optimization
procedures. Here, we propose Flow-VQE, a generative framework leveraging
conditional normalizing flows with parameterized quantum circuits to
efficiently generate high-quality variational parameters. By embedding a
generative model into the VQE optimization loop through preference-based
training, Flow-VQE enables quantum gradient-free optimization and offers a
systematic approach for parameter transfer, accelerating convergence across
related problems through warm-started optimization. We compare Flow-VQE to a
number of standard benchmarks through numerical simulations on molecular
systems, including hydrogen chains, water, ammonia, and benzene. We find that
Flow-VQE outperforms baseline optimization algorithms, achieving computational
accuracy with fewer circuit evaluations (improvements range from modest to more
than two orders of magnitude) and, when used to warm-start the optimization of
new systems, accelerates subsequent fine-tuning by up to 50-fold compared with
Hartree--Fock initialization. Therefore, we believe Flow-VQE can become a
pragmatic and versatile paradigm for leveraging generative modeling to reduce
the costs of variational quantum algorithms.

### 9. [Enhanced Generative Model Evaluation with Clipped Density and Coverage](http://arxiv.org/pdf/2507.01761v1)

Authors: Nicolas Salvy, Hugues Talbot, Bertrand Thirion

Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

### 10. [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](http://arxiv.org/pdf/2507.01806v1)

Authors: Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios

Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.



---

# Nature Computer Science Reports

Collection of today's Computer Science research papers pulled from Nature Open Access Reports.

---

Pulled on 2025-07-03 PST.

### 1. [Printed document layout analysis and optical character recognition system based on deep learning](https://www.nature.com/articles/s41598-025-07439-y)

Authors: Dong-Lin Li et al.

### 2. [Leveraging self attention driven gated recurrent unit with crocodile optimization algorithm for cyberattack detection using federated learning framework](https://www.nature.com/articles/s41598-025-99452-4)

Authors: Manal Abdullah Alohali et al.

### 3. [An improved lightweight method based on EfficientNet for birdsong recognition](https://www.nature.com/articles/s41598-025-07875-w)

Authors: Haolun He et al.

### 4. [Semantic ECG hash similarity graph](https://www.nature.com/articles/s41598-025-07838-1)

Authors: Yixian Fang et al.

### 5. [Positive–negative prototypes fusion framework for open set recognition](https://www.nature.com/articles/s41598-025-09625-4)

Authors: Xincheng Zhong et al.

### 6. [Transformer fault diagnosis method based on Gramian Angular Field and optimized parallel ShuffleNetV2](https://www.nature.com/articles/s41598-025-08590-2)

Authors: Qiang Guo et al.

### 7. [Robust dynamic event-triggered sliding mode control for lateral dynamics of intelligent electric vehicle](https://www.nature.com/articles/s41598-025-09816-z)

Authors: Zhi Liu et al.

### 8. [Influence of cognitive networks and task performance on fMRI-based state classification using DNN models](https://www.nature.com/articles/s41598-025-05690-x)

Authors: Murat Kucukosmanoglu et al.

### 9. [Artificial intelligence-driven cybersecurity: enhancing malicious domain detection using attention-based deep learning model with optimization algorithms](https://www.nature.com/articles/s41598-025-99420-y)

Authors: Fatimah Alhayan et al.

### 10. [MuTATE: an interpretable multi-endpoint machine learning framework for automated molecular subtyping in cancer](https://www.nature.com/articles/s44401-025-00025-4)

Authors: Sarah G. Ayton et al.

### 11. [An efficient privacy-preserving multilevel fusion-based feature engineering framework for UAV-enabled land cover classification using remote sensing images](https://www.nature.com/articles/s41598-025-08930-2)

Authors: S. Nagadevi et al.

### 12. [Privacy-preserving data reprogramming](https://www.nature.com/articles/s44387-025-00012-y)

Authors: Haoyue Bai et al.

